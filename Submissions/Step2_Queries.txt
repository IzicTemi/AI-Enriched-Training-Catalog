query 1 - search=labelling&$select=metadata_author,metadata_title,metadata_creation_date
result
{
  "@odata.context": "https://test-cogno.search.windows.net/indexes('papers-index')/$metadata#docs(*)",
  "value": [
    {
      "@search.score": 1.3460975,
      "content": "\nRESEARCH Open Access\n\nAugmented reality virtual glasses try-on\ntechnology based on iOS platform\nBoping Zhang\n\nAbstract\n\nWith the development of e-commerce, network virtual try-on, as a new online shopping mode, fills the gap that\nthe goods cannot be tried on in traditional online shopping. In the work, we discussed augmented reality virtual\nglasses try-on technology on iOS platform to achieve optimal purchase of online glasses, improving try-on speed of\nvirtual glasses, user senses of reality, and immersion. Face information was collected by the input device-monocular\ncamera. After face detection by SVM classifier, the local face features were extracted by robust SIFT. Combined with\nSDM, the feature points were iteratively solved to obtain more accurate feature point alignment model. Through\nthe head pose estimation, the virtual model was accurately superimposed on the human face, thus realizing the\ntry-on of virtual glasses. The above research was applied in iOS glasses try-on APP system to design the try-on system\nof augmented reality virtual glasses on iOS mobile platform. It is proved that the method can achieve accurate\nidentification of face features and quick try-on of virtual glasses.\n\nKeywords: Virtual try-on, Virtual glasses, Augmented reality, Computer vision, Pose estimation, iOS\n\n1 Introduction\nNetwork virtual try-on is a new way of online shopping.\nWith the development of e-commerce, it broadens the\nexternal propaganda channels of merchants to enhance\nthe interaction between consumers and merchants.\nVirtual try-on fills the gap that the goods cannot be\ntried on in traditional online shopping. As an important\npart of network virtual try-on, virtual glasses try-on\ntechnology has become a key research issue in this field\nrecently [1–4]. During virtual glasses try-on process,\nconsumers can select their favorite glasses by compar-\ning the actual wearing effects of different glasses in the\nonline shopping. The research key of virtual glasses\ntry-on system is the rapid achievement of experiential\nonline shopping.\nAR (augmented reality) calculates the position and\n\nangle of camera image in real time while adding corre-\nsponding images. The virtual world scene is superim-\nposed on a screen in real world for real-time\ninteraction [5]. Using computer technology, AR simu-\nlates physical information (vision, sound, taste, touch,\netc.) that is difficult to experience within certain time\n\nand space of real world. After superimposition of phys-\nical information, the virtual information is perceived by\nhuman senses in real world, thus achieving sensory ex-\nperience beyond reality [6].\nBased on AR principle, virtual glasses try-on technol-\n\nogy achieves optimal purchase of user online glasses and\nquick try-on of virtual glasses, improving the senses of\nreality and immersion. Monocular camera is used as the\ninput device to discuss try-on technology of AR glasses\non iOS platform. Face information is collected by mon-\nocular camera. After face detection by SVM (support\nvector machine) classifier, the local features of faces are\nextracted by robust SIFT (scale-invariant feature trans-\nform). Combined with SDM (supervised descent\nmethod), the feature points were iteratively solved to ob-\ntain more accurate feature point alignment model.\nThrough the head pose estimation, the virtual glasses\nmodel was accurately superimposed on the human face,\nthus realizing the try-on of virtual glasses. The above re-\nsearch is applied in iOS glasses try-on APP system to de-\nsign the try-on system of AR glasses on iOS mobile\nplatform. It is proved that the method can achieve ac-\ncurate identification of face features and quick try-on of\nvirtual glasses.Correspondence: bopingzhang@yeah.net\n\nSchool of Information Engineer, Xuchang University, Xuchang 461000,\nHenan, China\n\nEURASIP Journal on Image\nand Video Processing\n\n© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and\nreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to\nthe Creative Commons license, and indicate if changes were made.\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 \nhttps://doi.org/10.1186/s13640-018-0373-8\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13640-018-0373-8&domain=pdf\nhttp://orcid.org/0000-0001-7835-7622\nmailto:bopingzhang@yeah.net\nhttp://creativecommons.org/licenses/by/4.0/\n\n\n2 Research status of network virtual try-on\ntechnology\nGlasses try-on system was first applied in the USA.\nGlasses companies such as Camirror, Smart Look, Ipoint\nKisok, and Xview pioneered the online try-on function [7].\nUsers freely feel the wearing effect, enhancing the online\nshopping experience. Recently, online try-on function is\nexplored by domestic and foreign glasses sellers, such as\nMeijing [8], Kede [9] and Biyao [10].\nVirtual glasses try-on system involves computer vision,\n\naugmented reality, and image processing technology.\nRecently, research hotspots are speed, experience, and\nimmersion of try-on. At present, research results can be di-\nvided into four categories, namely 2D image superposition,\n3D glasses superimposed on 2D face images, 3D face mod-\neling, and AR technology based on video stream [11–14].\nHuang [15] introduced virtual optician system based on\n\nvision, which detects user’s face before locating user’s eyes.\nThree points are selected from face and glasses images.\nTwo corresponding isosceles triangles are formed for af-\nfine transformation, thus estimating the pose and scale of\nface in real time. This method realizes real-time head mo-\ntion tracking. However, the glasses model easily produces\nunrealistic deformation, affecting the realism of the\nglasses.\nAR technology is also applied in the virtual glasses\n\ntry-on system. Cheng et al. [16] selected a monocular\nCCD (charge-coupled device) camera as the input sensor\nto propose AR technology design based on the inter-\naction of marker and face features. Virtual glasses try-on\nsystem is established based on Android mobile platform,\nachieving good results. During virtual try-on process, we\nuse 2D image overlay or 3D modeling approach. There\nare still different defects although all kinds of virtual\nglasses try-on techniques have certain advantages. The\nsuperposition of 2D images is unsatisfactory in the sense\nof reality. Besides, the 3D modeling takes too long to\nmeet the real-time requirements of online shopping.\n\nIn-depth research is required to realize accurate tracking\nand matching. These problems can be solved by\nAR-based glasses try-on technology to a large extent,\nthus providing new ideas for virtual try-on technology.\n\n3 Methods of face recognition\nIt is necessary to integrate virtual objects into real envir-\nonment for the application of AR technology in virtual\nglasses try-on system, wherein face recognition is the\nprecondition for virtual glasses try-on system. During\ntry-on process, it is necessary to detect the face in each\nframe of the video. However, the problems of posture, il-\nlumination, and occlusion can increase the omission and\nfalse ratios of face detection. The real time of detection\nis an important indicator of system performance to en-\nhance people’s experience senses.\nGeneral face recognition process consists of face de-\n\ntection, tracking, feature extraction, dimension reduc-\ntion, and matching recognition (see Fig. 1) [17].\nIn Fig. 1, face detection is the first step to realize face\n\nrecognition. Its purpose is to automatically find face re-\ngion in an input image. If there is a face area, the spe-\ncific location and range of face needs to be located. Face\ndetection is divided into image-based and video-based\ndetection. If the input is a still image, each image is de-\ntected; if the input is a video, face detection is performed\nthroughout the video sequence.\nFeature extraction is based on face detection, and the\n\ninput is the detected face image. Common features are\nLBP (local binary patterns), HOG (histogram of oriented\ngradient), Gabor, etc. HOG [18] describes the edge fea-\ntures. Due to insensitiveness to illumination changes and\nsmall displacements, it describes the overall and local in-\nformation of human face. LBP [19] shows the local tex-\nture changes of an image, with brightness invariance.\nGabo feature [20] captures the local structural content\nof spatial position, direction selectivity, and spatial fre-\nquency. It is suitable for description of human faces.\n\nFig. 1 Face recognition process\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 2 of 19\n\n\n\nFeature dimension reduction is described as follows.\nFace feature is generally high-dimensional feature vector.\nFace recognition of high-dimensional feature vector\nincreases time and space complexity. Besides, it is difficult\nto effectively judge the description ability of high-dimen-\nsional face features. The high-dimensional face feature\nvector can be projected to the low-dimensional subspace.\nThe low-dimensional subspace information can complete\nface feature identification. After feature extraction, the ori-\nginal features are recombined to reduce vector dimension\nof face feature.\nAfter the previous links, we compare the existing tar-\n\ngets in face database and the faces to be identified based\non certain matching strategy, making final decision.\nMatching recognition can be represented by offline\nlearning and online matching models.\n\n3.1 SVM-based face detection\nFace detection is the premise of virtual glasses try-on\ntechnology. Recently, scholars proposed face detection\nmethods, such as neural network, SVM (support vector\nmachine), HMM (hidden Markov model), and AdaBoost.\nIn the work, the classic SVM algorithm is used for face\ndetection. SVM algorithm is a machine learning method\nbased on statistical theory. Figure 2 shows the network\nstructure of SVM [21]. SVM algorithm can be regarded\nas a three-layer feedforward neural network with a hid-\nden layer. Firstly, the input vector is mapped from\nlow-dimensional input space to the high-dimensional\nfeature space by nonlinear mapping. After that, the opti-\nmal hyperplane with the largest interval is constructed\nin the high-dimensional feature space.\nIt is denoted that the input vector of SVM x= (x1, x2,…, xn).\n\nEquation (1) shows the network output of output layer\nbased on x.\n\ny xð Þ ¼ sgn\nXN train\n\ni¼1\nyi∂\n\n \ni K xi; x\n\n   þ b \n   \n\nð1Þ\n\nwherein the inner product K(x(i), x) is a kernel function\nsatisfying the Mercer condition. Common kernel func-\ntions consist of polynomial, Gauss, and Sigmoid kernel\n\nfunctions. The Gaussian kernel function Kðx; zÞ ¼ e\njjx−zjj2\n2σ2 ,\n\nand σ is the width function.\nOptimization problem of quadratic function (Eq. (2)) is\n\nsolved to obtain the optimal parameter vector ∂ \n\n¼ ð∂ 1; ∂ 2;…; ∂ N train\nÞT in discriminant function.\n\nmin\n1\n2\nð\nXN train\n\ni¼1\n\nXN train\n\ni¼1\n∂i∂ jy\n\niy jK xi; x j\n   \n\n−\nXN train\n\ni¼1\n∂i ð2Þ\n\ns:t:\nXNtrain\n\ni¼1\n\n∂iyi i ¼ 1; 2;…;N train\n\n0≤∂i≤C\n\nThe training sample xi corresponding to ∂i > 0 is used\nas a support vector. The optimization parameter b∗ can\nbe calculated by Eq. (3).\n\nb  ¼ 1\nNsv\n\nX\ni∈SV\n\nyi−\nX\n\nj∈SV\n∂ jK xi; x j\n\n      \nð3Þ\n\nSVM classifier is used to determine whether the de-\ntected image is a human face. If it is not human face,\nthen the image is discarded. If it is, then the image is\nretained to output the detection result. Figure 3 shows\nthe detection process.\n\n3.2 Face recognition based on SIFT\nAfter face detection, face features are extracted for face\nrecognition, providing conditions for face alignment. In\nthe work, the robust SIFT algorithm is used for local fea-\nture extraction [22]. The algorithm finds feature points in\ndifferent scale spaces. It is irrelevant to rotation, scale, and\nbrightness changes. Besides, the algorithm has certain sta-\nbility to noise, affine transformation, and angle change.\n\n3.2.1 Basic principle of SIFT algorithm\nIn the process of feature construction by SIFT algorithm,\nit is necessary to deal with multiple details, achieving faster\noperation and higher positioning accuracy. Figure 4 shows\nflow block diagram of SIFT algorithm [21]. The generation\nprocess of local feature is described as follows [22]:\n\nFig. 2 SVM network structure\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 3 of 19\n\n\n\n① Detect extreme points\nGaussian differential functions are used for image search\n\non all scales, thus identifying potential fixed points.\n② Position key points\nThe scale on candidate position of model is confirmed.\n\nThe stability degree determines the selection of key points.\n③ Determine the direction of key points\nUsing the gradient direction histogram, each key point\n\nis assigned a direction with the highest gradient value to\ndetermine the main direction of key point.\n④ Describe the key points\nThe local gradients of image are calculated and repre-\n\nsented by a kind of symbol.\n\n3.2.2 Key point matching\n3.2.2.1 Scale space Scale space introduces a scale par-\nameter into image matching model. The continuously\nvariable scale parameter is used to obtain the scale space\nsequence. After that, the main contour of scale space is\n\ntaken as the feature vector to extract the edge features\n[23]. The larger scale leads to the more blurred image.\nTherefore, scale space can simulate the formation\nprocess of target on the retina of the human eye.\nScale space of image can be expressed as Eq. (4).\n\nL x; y; σð Þ ¼ G x; y; σð Þ   I x; yð Þ ð4Þ\nIn Eq. (4), G(x, y, σ) is the Gaussian function, I(x, y) the\n\noriginal image, and * the convolution operation.\n\n3.2.2.2 Establishing Gaussian pyramid\n\nG x; y; σð Þ ¼ 1\n2πσ2\n\ne− x−d=2ð Þ2þ y−b=2ð Þ2ð Þ=2σ2 ð5Þ\n\nIn Eq. (5), d and b are the dimensions of Gaussian\ntemplate, (x, y) is the pixel location, and σ the scale space\nfactor.\nGaussian pyramid is established according to Eq. (5),\n\nincluding Gaussian blur and down-sampling (see Fig. 5).\nIt is observed that the pyramids with different sizes con-\nstitute tower model from bottom to top. The original\nimage is used for the first layer, the new image obtained\nby down-sampling for the second layer. There are n\nlayers in each tower. The number of layers can be calcu-\nlated by Eq. (6).\n\nn ¼ log2 minf p; qð Þg−d dϵ 0; log2 minf p; qð Þg½  \nð6Þ\n\nIn Eq. (6), p and q are the sizes of the original image and d\nis the logarithm of minimum dimension of tower top image.\n\n3.2.2.3 Gaussian difference pyramid After scale\nnormalization of maxima and minima of the Gaussian La-\nplace function σ2∇2G, we obtain the most stable image fea-\ntures using other feature extraction functions. The\nGaussian difference function is approximated to the Gauss-\nian Laplace function σ2∇2G after scale normalization. The\nrelationship is described as follows:\n\n∂G\n∂σ\n\n¼ σ2∇ 2G ð7Þ\n\nDifferential is approximately replaced by the difference:\n\nσ2∇ 2G ¼ ∂G\n∂σ\n\n≈\nG x; y; kσð Þ−G x; y; σð Þ\n\nkσ−σ\nð8Þ\n\nTherefore,\n\nG x; y; kσð Þ−G x; y; σð Þ ≈ k−1ð Þσ2∇ 2G ð9Þ\nIn Eq. (9), k − 1 is a constant.\nIn Fig. 6, the red line is the DoG operator curve; the\n\nblue line the Gauss-Laplacian curve. In extreme detection\n\nFig. 3 The detection process of SVM classifier\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 4 of 19\n\n\n\nmethod, the Laplacian operator is replaced by the DoG\noperator [24] (see Eq. (10).\n\nD x; y; σð Þ ¼ G x; y; kσð Þ−G x; y; σð Þð Þ   I x; yð Þ\n¼ L x; y; kσð Þ−L x; y; σð Þ ð10Þ\n\n3.2.2.4 Spatial extreme detection In Gaussian differ-\nence space, local extreme points constitute the key\npoints. When searching for key points, we compare the\n\nimages between two adjacent layers in the same group.\nAfter that, each pixel point is compared with all the ad-\njacent points to judge whether it is large or small (see\nFig. 6). The red intermediate detection point is com-\npared with 26 points in the surrounding, upper, and\nlower scale spaces to detect extreme points.\nIn the calculation, the Gaussian difference image is the\n\ndifference between the adjacent upper and lower images\nin each group of the Gaussian pyramid (see Fig. 7).\n\n3.2.2.5 Spatial extreme detection In Gaussian differ-\nence space, local extreme points constitute the key\npoints. When searching for key points, we compare the\nimages between two adjacent layers in the same group.\nAfter that, each pixel point is compared with all the\nadjacent points to judge whether it is large or small\n(see Fig. 8). The red intermediate detection point is com-\npared with 26 points in the surrounding, upper, and lower\nscale spaces to detect extreme points.\nIf there are N extreme points in each group, then we\n\nneed N+ 2-layer DoG pyramid and N+ 3-layer Gaussian\npyramid (see Fig. 8). Due to edge response, the extreme\npoints generated in this case are not all stable.\n\n3.2.2.6 Key point matching At first, the key point is\ncharacterized by position, scale, and direction. To main-\ntain the invariance of perspective and illumination\nchanges, the key point should be described by a set of vec-\ntors. Then, the descriptor consists of key points and other\ncontributive pixels. Besides, the independent characteristic\n\nFig. 4 SIFT algorithm flow chart\n\nFig. 5 Gaussian pyramid\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 5 of 19\n\n\n\nof descriptor is guaranteed to improve the probability of\ncorrect matching of feature points.\nThe gradient value of key point is calculated. The gra-\n\ndient value and direction are determined by Eq. (11).\n\nm x; yð Þ ¼\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\nN xþ 1; yð Þ−N x−1; yð Þð Þ2 þ N x; yþ 1ð Þ−N x; y−1ð Þð Þ2\n\nq\n\nθ x; yð Þ ¼ α tan2\nN x; yþ 1ð Þ−N x; y−1ð Þ\nN xþ 1; yð Þ−N x−1; yð Þ\n\n   \nð11Þ\n\nIn Eq. (11), N represents the scale space value of key point.\nGradient histogram statistics. The gradient and direc-\n\ntion of pixels in the neighborhood are represented by\nhistogram. The direction ranges from 0 to 360°. There is a\n\nFig. 6 Comparison of Gauss-Laplacian and DoG\n\nFig. 7 Gaussian pyramid of each group\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 6 of 19\n\n\n\nsquare column for every 10°, forming 36 columns [25]\n(see Fig. 9). In feature point field, the peak represents the\ngradient direction. The histogram of maximum is the\nmain direction of key point. Meanwhile, the histogram\nwith peak value greater than 80% of main direction is se-\nlected for auxiliary direction to improve the matching\nrobustness.\nAfter successful matching of key points, the entire al-\n\ngorithm is not over yet. This is because substantial mis-\nmatched points appear in the matching process. These\nmismatched points are eliminated by Ransac method in\nSIFT matching algorithm [26].\n\n3.2.3 Face recognition experiment\nTo evaluate the algorithm, the experiment is conducted\nbased on face infrared database provided by Terravic Re-\nsearch Corporation. There are a total of 20 infrared\nimage sequences with head rotation, glasses, hats, and\nlight-illuminated pictures. Three pairs of images are se-\nlected from each face, with a total of 60 pairs. Figure 10\nshows the selected 120 images. In the work, the classic\n\nSIFT matching algorithm is used as the initial matching\nmethod to manually determine matching accuracy and\nmismatch rate of each group. In other words, the match-\ning performance is described by accuracy and error de-\ngrees. Accuracy is defined by the ratio of the number of\ncorrect matches in total number. Error degree is the ra-\ntio of the number difference (between key and matched\npoints) in the total number of key points.\nThese 120 samples are conducted with abstract match-\n\ning contrast according to the variables including head\nrotation angle, illumination transformation, glasses, and\nhat wearing. Meanwhile, other variables remain the\nsame. Figures 11, 12, 13, and 14 show the matching re-\nsults, respectively:\n\n1. Matching results when head rotation angle changes\n2. Matching results when wearing glasses\n3. Matching results when wearing a hat\n4. Matching results when light and shade change\n\nThe experimental data are shown in Table 1.The ex-\nperimental image and Table 1 show:\n① SIFT matching performance is more easily affected\n\nby wearing glasses than head rotation angle, light illu-\nmination, darkness, and wearing hat.\n② In the case of the same number of matches, the\n\nsuccess rate of SIFT matching is higher than that of the\nHarris matching method [27].\nThe overall trend of results can be well presented al-\n\nthough there are inevitable errors due to the finiteness\nof experimental samples.\n\n3.3 Face alignment\nFace alignment is the positioning of face feature points.\nAfter face image detection, the SIFT algorithm automat-\nically positions the contour points of the eyebrows, eyes,\nnose, and mouth. In the try-on process of AR glasses,\nthe eyes are positioned to estimate the head posture.\nThe pose estimation is applied to the tracking registra-\ntion subsystem of glasses, thus producing perspective\n\nFig. 8 The detection of DoG space extreme point\n\nFig. 9 The histogram of the main direction\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 7 of 19\n\n\n\ntransformation. However, the pose estimation is easily af-\nfected by the positioning of face feature points, resulting\nin estimation error. The feature points are accurately posi-\ntioned to achieve good effect of head pose estimation.\nAt present, there are many face alignment algorithms.\n\nSDM is a method of finding function approximation\nproposed by Zhu et al [28] by calculating the average\nface, and local features around each feature point are ex-\ntracted to form feature vector descriptor. The offset be-\ntween average and real face is calculated to obtain the\nstep size and motion direction for iteration. The current\nface feature points are converged to the optimal position\nby repeated iterations.\nFigure 15 shows the SDM-based face alignment process.\n\nThe face alignment process is described as follows.\n\n3.3.1 Image normalization\nThe image is normalized to achieve face alignment, thus\nimproving the efficiency of training. The face image to be\ntrained is manually labeled with feature points. After rea-\nsonable translation, rotation, and scaling transformation,\nthe image is aligned to the first sample. The sample size is\nunified to arrange the original data information with con-\nfused, reducing interference other than shape factors. Fi-\nnally, the calculated average face is placed on the sample\nas the estimated face. The average is aligned with the ori-\nginal face image in the center.\n\nIt is denoted that x∗ is the optimal solution in face fea-\nture point location, x0 the initialization feature point,\nd(x) ∈ Rn × 1 the coordinates of n feature points in the\nimage, and h the nonlinear feature extraction function\nnear each feature point. If the SIFT features of 128 di-\nmensions are extracted from each feature point, then\nh(d(x)) ∈ R128n × 1. The SIFT feature extracted at x∗ can\nbe expressed as ∅∗ = h(d(x∗)). Then, the face feature\npoint alignment is converted into the operation of solv-\ning Δx, which minimizes Eq. (12).\n\nf x0 þ Δxð Þ ¼ hðd x0 þ Δxð Þk k22 ð12Þ\nThe step size Δx is calculated based on the SDM\n\nalgorithm.\n\nxk ¼ xk þ Δxk ð13Þ\nIf Rk and bk are the paths of each iteration, then\n\nEq. (11) can converge the feature point from the initial\nvalue x0 to x∗.\n\nxkþ1 ¼ xk−1 þ Rk−1∅k−1 þ bk−1 ð14Þ\nDuring training process, {di} is the set of face images,\n\n{di} the set of manually labeled feature points, and x0 the\nfeature point of each image. Face feature point location\nis transformed into a linear regression problem. For the\nproblem, the input feature is the SIFT feature ∅i\n\n0 at x0;\n\nFig. 10 Sample sequence set\n\nFig. 11 Matching results when head rotation angle changes\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 8 of 19\n\n\n\nthe result the iteration step size Δxi  ¼ xi  þ Δxi0 from x0\nto x∗; and the objective function Eq. (15).\n\nargminR0b0\n\nX\ndi\n\nX\nxi\n\nΔxi −R0∅i\n −b0\n\n\t\t \t\t2\n2 ð15Þ\n\nIn this way, R0 and b0 from the training set are iterated\nto obtain Rk and Rk. The two parameters are used for\nthe test phase to achieve the alignment of test images.\n\n3.3.2 Local feature extraction of SIFT algorithm\nIn the work, the principal component analysis is used to\nreduce the dimension of image [29], the impact of\nnon-critical dimensions, and the amount of data, thus\nimproving the efficiency. After the dimension reduction,\nthe local feature points are extracted from the face\nimage. To improve the alignment accuracy of feature\npoints, the robust SIFT algorithm is applied for local fea-\nture extraction. Section 3.2.2 introduces the extraction\nprocess in detail.\n\n3.3.3 SDM algorithm alignment result\nTraining samples are selected from IBUG and LFW face\ndatabases. The former contains 132 face images. Each\nimage is labeled with 71 face feature points, which are\nsaved in pts file. The latter consists of the sets of test and\ntraining samples, wherein, the set of test sample contains\n206 face images. Each image is labeled with 71 face feature\npoints, which are saved in pts file. The set of training\n\nsample contains 803 face images. Each image is labeled\nwith 68 face feature points. Figures 16 and 17 show frontal\nand lateral face alignment results, respectively.\n\n3.4 Face pose estimation\nBased on computer vision, the pose of object refers to\nits orientation and position relative to the camera. The\npose can be changed by moving the camera or object.\nGeometric model of camera imaging determines the re-\nlationship between 3D geometric position of certain\npoint on head surface and corresponding point of image.\nThese geometric model parameters are camera parame-\nters. In most cases, these parameters are obtained by ex-\nperiments. This process is called labeling [27, 29].\nCamera labeling determines the geometric and optical\nproperties, 3D position, and direction of camera relative\nto certain world coordinate system.\nThe idea of face pose estimation is described as fol-\n\nlows. Firstly, we find the projection relationship between\n2D coordinates on face image and 3D coordinates of\ncorresponding points on 3D face model. Then, the mo-\ntion coordinates of camera are calculated to estimate\nhead posture.\nA 3D rigid object has two movements relative to the camera:\n① Translation movement\nThe camera is moved from current spatial position\n\n(X,Y, Z) to new spatial position (X′,Y′, Z′), which is called\n\nFig. 12 Matching results when wearing glasses\n\nFig. 13 Matching results when wearing a hat\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 9 of 19\n\n\n\ntranslation. Translation vector is expressed as τ = (X′ −X,\nY′ −Y, Z′ −Z).\n② Rotary movement\nIf the camera is rotated around the XYZ axis, the rota-\n\ntion has six degrees of freedom. Therefore, pose estima-\ntion of 3D object means finding six numbers (three for\ntranslation and three for rotation).\n\n3.4.1 Feature point labelling\nThe 2D coordinates of N points are determined to calcu-\nlate 3D coordinates of points, thus obtaining 3D pose of\nobject in an image.\nTo determine the 2D coordinates of N points, we se-\n\nlect the points with rigid body invariance, such as the\nnose tip, corners of eyes, and mouth. In the work, there\nare six points including the nose tip, chin, left, and right\ncorners of eyes and mouth.\nSFM (Surrey Face Model) is used as general 3D face\n\nmodel to obtain 3D coordinates corresponding to se-\nlected 2D coordinates [30]. By manual labeling, we ob-\ntain the 3D coordinates (x, y, z) of six points for pose\nestimation. These points are called world coordinates in\nsome arbitrary reference/coordinate system.\n\n3.4.2 Camera labeling\nAfter determining world coordinates, the camera is reg-\nistered to obtain the camera matrix, namely focal length\nof camera, optical center, and radial distortion parame-\nters of image. Therefore, camera labeling is required. In\nthe work, the camera is labeled by Yang and Patras [31]\nto obtain the camera matrix.\n\n3.4.3 Feature point mapping\nFigure 18 shows the world, camera, and image coordin-\nate systems. In Fig. 18, O is the center of camera, c the\noptical center of 2D image plane, P the point in world\ncoordinate system, and P′ the projection of P on image\nplane. P′ can be determined according to the projection\nof the P point.\nIt is denoted that the world coordinate of P is (U,V, W).\n\nBesides, the known parameters are the rotation matrix R\n\nFig. 14 Matching results when light and shade change\n\nTable 1 Match result analysis table\n\nVariate Number of\nmatches\n\nTotal number\nof key points\n\nMatch\nratio\n\nFalse\nmatch rate\n\nHead rotation 18 158 0.129 0.871\n\nWearing glasses 15 167 0.099 0.901\n\nWearing a hat 21 106 0.247 0.753\n\nLight and shade\nchange\n\n45 281 0.191 0.809\nFig. 15 The face alignment process based on SDM\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 10 of 19\n\n\n\n(matrix 3 × 3) and translation vector τ (vector 3 × 1) from\ncamera to world coordinate. It is possible to determine\nposition O(X, Y, Z) of P in camera coordinate system.\n\nx\ny\nz\n\n2\n4\n\n3\n5 ¼ R\n\nu\nv\nw\n\n2\n4\n\n3\n5þ τ⇒\n\nx\ny\nz\n\n2\n4\n\n3\n5 ¼ Rjτ½  \n\nu\nv\nw\n\n2\n4\n\n3\n5 ð16Þ\n\nEquation (16) is expanded as follows:\n\nx\ny\nz\n\n2\n4\n\n3\n5 ¼\n\nr00 r01 r02 τx\nr10 r11 r12 τy\nr20 r21 r22 τz\n\n2\n4\n\n3\n5\n\nu\nv\nw\nl\n\n2\n664\n\n3\n775 ð17Þ\n\nIf plenty of points are mapped to (X, Y, Z) and (U,V, W),\nthe above problem is transformed into a system of linear\nequations with unknown (τx, τy, τz) . Then, the system of\nlinear equations can be solved.\nFirstly, the six points on 3D model are manually la-\n\nbeled to derive their world coordinates (U, V, W). Equa-\ntion (18) is used to determine 2D coordinates (X, Y) of\nsix points in image coordinate system.\n\nx\ny\n1\n\n2\n4\n\n3\n5 ¼ S\n\nf x 0 0\n0 f y 0\n0 0 1\n\n2\n4\n\n3\n5 x\n\ny\nZ\n\n2\n4\n\n3\n5 ð18Þ\n\nwhere fx and fy are the focal lengths in the x and y direc-\ntions, (cx, cy) is the optical center, and S the unknown scaling\nfactor. If P in 3D is connected to O, then P′ where light in-\ntersects image plane is the same image connecting all points\nin the center of the camera produced by P along the ray.\nEquation (18) is converted to the following form:\n\nS\nX\nY\nZ\n\n2\n4\n\n3\n5 ¼\n\nr00 r01 r02 τx\nr10 r11 r12 τy\nr20 r21 r22 τz\n\n2\n4\n\n3\n5\n\nu\nv\nw\nl\n\n2\n664\n\n3\n775 ð19Þ\n\nThe image and world coordinates are known in the\nwork. Therefore, Eqs. (18) and (19) are transformed into\nthe following form:\n\nx\ny\n1\n\n2\n4\n\n3\n5 ¼ S\n\nf x 0 0\n0 f y 0\n0 0 1\n\n2\n4\n\n3\n5 r00 r01 r02 τx\n\nr10 r11 r12 τy\nr20 r21 r22 τz\n\n2\n4\n\n3\n5\n\nu\nv\nw\nl\n\n2\n664\n\n3\n775 ð20Þ\n\nIf the correct poses R and τ are known, then the 2D\nposition of 3D facial point on image can be predicted by\nprojecting the 3D point onto the image (see Eq. (20)).\nThe 2D facial feature points are known. Pose estimation\ncan be performed by calculating the distance between\nthe projected 3D point and 2D facial feature. If the pose\nis correctly estimated, the 3D points projected onto\nimage plane will almost coincide with the 2D facial fea-\ntures. Otherwise, the re-projection error can be mea-\nsured. The least square method is used to calculate the\nsum of squares of the distance between the projected 3D\nand 2D facial feature points.\n\n3.5 Tracking registration system\nTracking registration technology is the process of align-\ning computer-generated virtual objects with scenes in\nthe real world. At present, there are two tracking regis-\ntration techniques. The first superimposes certain point\nof face feature with a point of virtual glasses based on\nthe face feature point tracking method [32]. The second\nis based on the geometric transformation relation track-\ning method. Face geometry and virtual glasses model are\nconducted with affine transformation. Virtual glasses\nmodel moves with the movement of human head, mak-\ning corresponding perspective changes and realizing 3D\ntry-on effect [33]. For the first technique, the virtual\nglasses cannot be changed with the movement of user\nhead, causing poor user experience. The second tech-\nnique has good tacking effect. The virtual glasses will be\ndistorted with overlarge head corner. Combined with the\ntwo methods, the glasses model is conducted with per-\nspective transformation using six degrees of freedom ob-\ntained by pose estimation in Section 3.3. After face\nsuperposition, accurate tracking is realized through bet-\nter stereoscopic changes.\n\nFig. 16 The picture of front face alignment\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 11 of 19\n\n\n\n3.5.1 Affine transformation method of glasses try-on\nIn Fig. 19, the center between two corners of the eye is\ncalculated according to the distance between them. An\nisosceles right triangle ABC is defined [34]. The coordi-\nnates of the triangle are A(a1, a2), B(b1, b2), and C(c1, c2).\nIf the threshold is determined by experiment ahead of\ntime, the coordinates of C are as follows.\n\nC c1; c2ð Þ ¼ b1−b2 þ a2; b1 þ b2−a2ð Þ ð21Þ\n\nDuring try-on process, the glasses model is matched\nto the eye of user using the affine transformation\nEq. (22).\n\nx0 ¼ axþ byþ c y0 ¼ dxþ eyþ f ð22Þ\n\nIn the glasses model, the vertices of isosceles right\ntriangle are priori, with the coordinates of (x1, y1),\n(x2, y2), and (x3, y3). The vertices of isosceles right\ntriangle on user face (x01; y\n\n0\n1 ), (x\n\n0\n2; y\n\n0\n2 ), and (x03; y\n\n0\n3 ) can be\n\ndetected in motion. The affine transformation parameter\nh = (a, b, c, d, e, f )T.\n\nx01\ny01\nx02\ny02\nx03\ny03\n\n2\n6666664\n\n3\n7777775\n¼\n\nx1 y1 1 0 0 0\n0 0 0 x1 y1 1\nx2 y2 1 0 0 0\n0 0 0 x2 y2 1\nx3 y3 1 0 0 0\n0 0 0 x3 y3 1\n\n2\n6666664\n\n3\n7777775\n\na\nb\nc\nd\ne\nf\n\n2\n6666664\n\n3\n7777775\n\nð23Þ\n\nEquation (23) is abbreviated as P = Ah. Finally, the af-\nfine transformation parameter h (h = (ATA)−1) is calcu-\nlated by least square method. If h is applied to the\nisosceles right triangle, then the image of glasses will be\nprojected onto the right position of the face.\n\n3.5.2 Perspective transformation method of glasses try-on\nAffine transformation can realize the tracking of 3D\nmodel. The tracked glasses are prone to deformation be-\ncause the affine transformation has the characteristics of\nflatness and parallelism based on three-point transform-\nation [35]. The six degrees of freedom are obtained from\nhead pose estimation. After perspective transformation,\nthe glasses are superimposed with eye feature points to\nachieve real-time tracking effect. When the head moves,\nthe space model of glasses should conform to human\nvisual law, with certain deformation. It is realized by per-\nspective transformation [36] (Fig. 20).\n\nFig. 17 The picture of side face alignment\n\nFig. 18 Three coordinate systems\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 12 of 19\n\n\n\n3.6 Virtual model generation system\nIn the work, the 3D glasses model is built in 3ds max\nand exported to 3DS format file. The 3DS data file can-\nnot display the 3D model in OpenGL in real time.\nFirstly, the 3DS model is conducted with analytic oper-\nation. Only by transferring the operational data to the\nOpenGL function can we draw the virtual glasses model\nin this case [37].\n\n3.7 Virtual and real synthesis system\nTo achieve the perfect combination of virtual glasses\nand realistic scenes, virtual glasses must be positioned to\nthe exact position in the real world at first. This process\nis achieved by integrating markers with natural features.\nFigure 21 shows the overall structure of fused glasses\ntry-on subsystem [38].\n\n4 iOS system application\nTo verify the effectiveness of proposed system method, we\ndevelop a mobile “AR Glasses Try-on Sales System” for\niOS platform. This system comprehensively uses the\n\ncommon controls of iOS. Besides, the controls are recon-\nstructed and optimized to improve the operating efficiency\nof the system. Meanwhile, most function blocks are modi-\nfied to minimize the application, dependencies, and main-\ntenance procedure of third-party framework. The system\nrealizes the basic functions including browsing of glasses\nproducts, user registration, login, goods collection, adding\nto carts, user address adding, modifying, deleting, goods\npurchasing, integrated Alipay payment, and order man-\nagement [39, 40]. In addition, it is embedded with glasses\ntry-on, photographing, recording video, uploading, and\nsharing WeChat and Weibo. Meanwhile, the system has\nits social platform for users to browse try-on effects of\nothers. The same glasses are quickly tried on, thus meet-\ning the needs of vast users (Fig. 22).\nCommodity browsing module is the core of the system,\n\ncovering commodity browsing, screening, try-on, photo-\ngraphing, uploading, and sharing. The try-on and quick\ntry-on subsystems need to call face recognition method in\nPart 3.\n\n4.1 Menu module\nMenu module is the framework module of the whole ap-\nplication. All the sub-modules are switched by a Menu-\nViewController controller. This control contains the\nviews of menu, home page, favorites, shopping cart,\norder, coupon, photo wall, setting modules, initialization,\nand the switching method of controllers. The menu page\nis not displayed in the default startup page. User calls up\nthe menu page by clicking the upper-left icon of the de-\nfault page or sliding to the right in the home page.\nMenu page adopts the traditional frosted glass method.\n\nFirstly, the UIImage object is obtained by taking a screen\nshot and then processed by the frosted glass tool. The\nfrosted glass UImage is used as the background of the\nmenu to realize the translucent frosted glass effect.\nMenu view is the leftmost view of MenuViewControl-\n\nler. The view is located at the top of the entire applica-\ntion. The menu can be switched out in all modules. It is\nachieved mainly by the table. The rows are selected by\nthe table to trigger the effect of selected rows.\nThe module title in the menu is clicked to trigger the\n\nproxy event of table, thus calling the method of selecting\ncurrent module for module switching. The switchable\nmodules mainly include user, commodity, collection,\nshopping cart, order, coupon, photo wall, and setting.\n\n4.2 User registration module\nUser registration module is used for the management of\nregistered users. Registered users enjoy the VIP promo-\ntion activities and prices. Unregistered users enter the\nregistration page by clicking the registration button.\n\nFig. 19 The isosceles triangle made on face and eyeglass images\n\nFig. 20 The quadrilateral made on face and eyeglass images\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 13 of 19\n\n\n\n4.3 Commodity module\nCommodity module is the key of “AR glasses sales sys-\ntem,” including commodity browsing, selection, and add-\ning to cart.\n\n4.3.1 Commodity browsing\nAll the products are browsed in the login or non-login\nstatus. In commodity browsing, the big images of glasses\nare slid to browse the front of product image and leg\nstyle. The detail page can be slid to view more commod-\nity information. More commodity information is loaded\nby sliding up.\nThe pull-up and pull-down are proxy methods based\n\non the table and its parent class (UIScrollView).\n(void)scrollViewDidScroll:(UIScrollView *)scrollView\nWhen the height of parent container offset is greater\n\nthan 20% of table height, the pull-down refresh is called.\nThe pull-up refresh is called when the height difference\n\nbetween the height of parent container and the sum of\ntable height and offset exceeds 10% of the screen height.\n\n4.3.2 Commodity screening\nIn commodity browsing, users quickly find products\nmeeting their needs and click to select multiple items. If\none item is selected, the system will feed back the num-\nber of eligible products in time. The display result but-\nton is clicked to display the screening result. Screening\nresults can be cleared by clicking on the cross on the\nright of blue subtitle.\nList screening is realized by modified and nested tables.\n\nThe segment head of custom table is used as first-level\nscreening title. Second-level screening catalog is achieved\nby nested table and custom table cells. By nesting\nsecond-level screening catalog, we obtain third-level\nscreening catalog. The subtitle of first-level catalog is\nrefreshed by recording the selected filter item in real time.\n\nFig. 21 General structure of system\n\nFig. 22 The structure of “AR Glasses Sales System”\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 14 of 19\n\n\n\nSimultaneously, the server is synchronized to get the\nremaining product information.\nIn the screening tool class, the record of third-level\n\nmenu is complicated. In implementation, the third-level\noptions are recorded for local summary, updating selected\nor unselected state. Full summary is performed by reusing\nlocal summary, indicating in the first-level menu.\n\n4.3.3 Glasses try-on\nIn the system, the face data are captured by the camera\nfor further processing. Firstly, user’s face is located at 30–\n50 cm right ahead the front camera of mobile phone. The\nface is slightly rotated, without getting out of capture area\nof camera. In addition, user should not keep his/her back\nto the light during the try-on process because the light af-\nfects the capture effect of the camera. When the camera\ndoes not capture face data, face position will be adjusted\nby a prompt. As user wiggles in front of camera, the en-\ngine re-recognizes the face information. The quick try-on\nbutton is clicked to enter the quick try-on page. User can\nput the phone 30–50 cm in front of him. Then, his/her\nface appears on the screen of the mobile phone. Mean-\nwhile, the system automatically recognizes user face data\nto put on glasses. The product details are viewed by\n\nsliding the glasses or clicking on the left detail button.\nFigure 23 shows the try-on process in detail.\nIn the try-on process, the third-party oepnframeworks is\n\nused to modify the class library according to the require-\nments. Section 3 introduces face recognition, alignment,\ntracking registration and head pose estimation, and virtual\nmodel generation methods. Combined with these\nmethods, the interface is packaged to increase the stability\nof system, reducing the dependence on third-party con-\ntrols. The part embedded in openframeworks starts with\nthe main function. Using openframeworks, the window is\ninitialized to call the Appdeleagte class of openframe-\nworks. This class is compatible with UIKit library in iOS.\nThe ofApp will be initialized to call engine loading model.\n\n4.3.3.1 AFNetworking In iOS development, the NSURL-\nConnection of XCode is competent to submit a request to\na simple page of Web site, thus obtaining the response\nfrom server. However, most web pages to be visited are\nprotected by authority. The pages cannot be visited by a\nsimple URL. This involves the processing of Session and\nCookie. Here, NSURLConnection can be used to realize\naccess, with larger complexity and difficulty.\nAFNetworking is more suitable to process requests to\n\nWeb sites, including detailed Sessions and Cookies\n\nFig. 23 The detailed flow chart of eyeglass try-on\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 15 of 19\n\n\n\nproblems. It can be used to send HTTP requests and re-\nceive HTTP responses. However, it does not cache ser-\nver responses or execute the JAvascript code in the\nHTML page. Meanwhile, AFNetworking has built-in\nJSON, plist, and XML file parsing for convenient\napplication.\nSome interfaces of the library are packaged to facilitate\n\nthe use of AFNetworking. The packaged AFNetworking\ncan record the operation due to disconnection request\nfailure. After networking, the request is re-initiated.\nWhen data needs to be requested, if get request is\n\ncalled, the following methods will be called:\n-(void)GET:(NSString *)URLString parameters:(id)para\n\nmeters WithBlock:(resultBlock)result;\nIf a image necessary upload pictures the following methods\n\nwill be call.\n// Upload pictures\n-(void)POST:(NSString *)URLString parameters:(id)para\n\nmeters WithData:(NSData *)data WithKey:(NSString *)key\nWithTypeO:(NSString*)pngOrMp4\nWithBlock:(resultBlock)result;\nIf a video necessary upload pictures the following\n\nmethods will be call.\n// Upload video\n-(void)POST:(NSString *)URLString parameters:(id)pa\n\nrameters WithDic:(NSDictionary *)dic WithTypeO:(NS\nString*)pngOrMp4 WithBlock:(resultBlock)result;\nIf post the following interface will be call.\n// post\n- (void)POST:(NSString *)URLString parameters:(id)\n\nparameters WithBlock:(resultBlock)result;\n\n4.3.3.2 SDWebImage SDWebImage is a framework for\nthird-party applications. It is used to implement asyn-\nchronous loading and caching of images. In this system,\nall network images are loaded using this framework. By\ndefining interface classes, we can easily implement asyn-\nchronous loading and caching of images.\n#import <Foundation/Foundation.h>\n@interface TGImageTool : NSObject\n+ (void)downloadImage:(NSString *)url placeholder:\n\n(UIImage *)place imageView:(UIImageView *)imageView;\n+ (void)clear;\n@end\n#import “TGImageTool.h”\n#import “UIImageView+WebCache.h”\n@implementation TGImageTool\n+ (void)downloadImage:(NSString *)url placeholder:\n\n(UIImage *)place imageView:(UIImageView *)imageView\n{[imageView setImageWithURL:[NSURL URLWithStrin\n\ng:url] placeholderImage:place options:SDWebImageLowPri\nority | SDWebImageRetryFailed];}\n+ (void)clear\n{\n\n// 1. Clear the cached images in memory\n[[SDImageCache sharedImageCache] clearMemory];\n[[SDImageCache sharedImageCache] clearDisk];\n// 2. Cancel all download requests\n[[SDWebImageManager sharedManager] cancelAll];\n}\n@end\nThe image is loaded by the above tool class method.\n\nWhen the cache is implemented, the image will be auto-\nmatically added to the cache.\nThe clear method of tool class is called to clear the cache.\n\n4.3.3.3 JSONKit JSONKit is used in this system only\nwhen the order information is submitted. It transcodes\nthe complicated parameter information to JSON strings\nfor server application. The conversion method is de-\nscribed as follows.\nNSString *new=[dic JSONString]\n\n4.3.4 Adding to cart\nThe satisfied glasses are added to shopping cart by clicking\non the “Add to Cart” button. The animation of “Add to\nCart” is realized by path and combined animation in the\nQuartzCore library.\n\n4.3.5 Buy a glasses immediately\nUser directly jumps to the page for purchasing the\nglasses without adding to cart. The function is realized\nby directly jumping to order information improvement\npage after summarizing commodity information.\n\n4.3.6 Taking photos or recording videos\nUsers with glasses can take off their glasses after logging\nin. VR glasses are tried on to take photos or record videos.\nThe try-on effects can also be watched after wearing\nglasses. The system provides functions of taking photos\nand recording videos. The photo/video button is used to\nswitch between taking pictures and recording videos. In\nthe work, this function is realized by modifying the engine\nin oepnframeworks. This system only involves the call.\nThe photos and videos are placed in the four preview\nareas below, where user can click to view the details.\n\n4.3.7 Uploading and sharing\nThe system provides uploading and sharing functions of\nphotos or videos to share satisfactory try-on results and\nwonderful moments with friends. “Share” button is\nclicked to upload videos to photo wall, friend circle,\nWeibo, or WeChat in the server. The photos or videos\nare deleted by clicking the “Delete” button.\nThe third-party AFNetworking method is used to up-\n\nload files. The files can be shared to Sina Weibo,\nWeChat circle, friends, and photo wall.\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 16 of 19\n\n\n\nThe sharing principle is to obtain the information of\nthe photo or video on server side. Then, the html5 page\nis generated, including image, video, like, and comment.\nThe URL is returned to the client and shared to WeChat\nand Sina Weibo.\nUsers can choose whether to share to photo wall at\n\nthe same time. Sharing to photo wall is to send a request\nto the server. The photo or video is backed up in the\ntable corresponding to database photo wall information.\nWhen being requested, the shared information can be\nobtained in the photo wall.\n\n4.4 Collection module\nIn the implementation of collection module, the cells of\ntable are reused in the home page. The data are replaced\nwith the data of favorite list. After logging in, the favorite\nitem is added to the collection list of personal information\nby clicking the gray heart button, which is convenient for\nnext viewing. “Collect” button is clicked to cancel the col-\nlected item, removing it from the collection list.\n\n4.5 Shopping cart module\nAfter logging in, the satisfactory item is added to shop-\nping cart in the try-on interface. In implementation, the\ncustom tool class is used to record the selected state.\nWhen clicking “Select All” button, all data in the table\nare selected. The selected state of “Select All” button is\nremoved to cancel certain item. Meanwhile, the sums of\nselected item quantities and unit prices are calculated.\nThe head position shows the number of items. “Settle”\nbutton at the bottom of table shows the total number of\nitems. Users can modify orders and postal addresses,\nwhile submitting orders and paying online.\n\n4.6 Order module\nAfter logging in, users can see their historical orders in\n“My Order.” There are two states in the order, including\npending (immediate payment) and successful payment.\n“Pay now” button is clicked to jump to payment interface.\nDuring order payment, it will jump to the immediate pay-\nment page of shopping module and then to Alipay.\n\n4.7 Coupon module\nThe coupon module is a channel through which mer-\nchants can distribute benefits to users. After logging in,\nusers check the coupons matching their own eligibilities.\nThere are three types of coupons received: available,\nused, and expired coupons. After reading coupon usage\nrules, users can select whether to use the coupon in the\ninterface of order information completion.\n\n4.8 Photo wall module\nThe photo wall is a display platform provided by the sys-\ntem to user. It is convenient for user to browse the\n\ntry-on results of others. Based on dynamic prompt func-\ntion, user quickly finds the favorite style of glasses.\nUser dynamic prompt function is implemented by de-\n\ntecting new messages. Once menu page pops up, a request\nis sent to the server, requesting a new unread message. If\nthere is a new message, it will show user avatars of last dy-\nnamic message and the number of new messages; other-\nwise, the prompt box is not displayed.\nWhile seeing the favorite try-on results, users can like,\n\ncomment, forward, or view the same item and try it on\nquickly.\n“View commodity” button is clicked to view the de-\n\ntailed information of glasses try-on results. The product\ninformation is uniquely determined according to the\nproduct ID. It is the same as quick try-on principle.\nUser can directly try on the same glasses worn by\n\nother users by clicking the quick try-on button. The\nphoto wall and product data are bound in the database\nat the beginning. Therefore, the product can be directly\nfound and tried on according to the product ID.\n\n4.9 Setting module\nThe setting module contains “check updates,” “clean up\npicture cache,” “about us,” “rating,” “feedback,” and “exit\ncurrent user.” Relatively, it is the application of native\ntable control, which is not described here.\n\n5 Results and discussion\nExperimental environment is described as follows.\nOperating system: iOS 9\nDevelopment tools: Xcode 6\nRelated libraries: OpenCV, MFC\nProgramming language: C language, Objective-C, C++\nFigure 24 shows the partial operation interface of the\n\nsystem.\nAlthough a lot of jobs are done, there are still some\n\nshortcomings in the system:\n\n1. Equipped with a try-on engine, the system has\ncertain requirements on the performance of iPhone.\nThe higher configuration of iPhone leads to the\nmore accurate identification. At present, the models\nrunning smoothly are iPhone 5s and iPhone 6 and\niPhone 6 Plus.\n\n2. It is difficult for user to perform subsequent\noperations in the case of unstable network\nenvironment, especially for failed login.\n\n3. In the system, the face data are captured by the\ncamera for further processing. Firstly, user face is\nlocated at 30–50 cm right ahead the front camera\nof the mobile phone. The face is slightly rotated,\nwithout getting out of the capture area of the camera.\n\n4. User should not keep his/her back to the light\nduring the try-on process because the light affects\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 17 of 19\n\n\n\ncapture effect of the camera. When the camera does\nnot capture face data, face position will be adjusted\nby a prompt. As user wiggles in front of camera, the\nengine re-recognizes user face information.\n\n5. At present, only the Chinese version of “AR Glasses\nSales System” has been developed. There is no\ncorresponding English version.\n\n6 Conclusions\nIn the work, we discussed augmented reality virtual\nglasses try-on technology. Face information was collected\nby monocular camera. After face detection by SVM classi-\nfier, the local face features were extracted by robust SIFT.\nCombined with SDM, the feature points were iteratively\nsolved to obtain more accurate feature point alignment\nmodel. Through the head pose estimation, the virtual\nglasses model was accurately superimposed on the human\nface, thus realizing the try-on of virtual glasses. This the-\noretical research was applied in iOS platform for the\ntry-on of virtual glasses, thus providing best services for\nuser selection. Experiments showed that the virtual glasses\nhad realistic effect and high try-on speed and user satisfac-\ntion. Consequently, AR-based glasses try-on technology\nprovided new idea for virtual try-on technology. Camera\ncapture under complex light conditions will be further\nstudied. App running test on iPhone 7 and above will be\ncarried out. Multilingual versions will be developed.\n\nAbbreviations\nAPP: Application; AR: Augmented reality; CCD: Charge-coupled device;\nHMM: Hidden Markov model; HOG: Histogram of oriented gradient;\nIBUG: Intelligent Behaviour Understanding Group; iOS: iPhone OS; LBP: Local\nbinary patterns; LFW: Labeled Faces in the Wild; SDM: Supervised descent\nmethod; SFM: Surrey Face Model; SIFT: Scale-invariant feature transform;\nSVM: Support vector machines; VR: Virtual reality\n\nAcknowledgements\nThis work is partially supported by Shanxi Province Universities Science and\nTechnology Innovation Project (2017107) and Shanxi Province Science\nFoundation for Youths (201701D12111421).\nThanks to the editor and reviewers.\n\nFunding\nThe paper is subsidized by science and technology key project of Henan\nProvince, China. NO.172102210462\n\nAvailability of data and materials\nData will not be shared; reason for not sharing the data and materials is that\nthe work submitted for review is not completed. The research is still ongoing, and\nthose data and materials are still required by my team for further investigations.\n\nAuthor’s contributions\nBZ designed the research, analyzed the data, and wrote and edited the\nmanuscript. The author read and approved the final manuscript.\n\nAuthor’s information\nBoping Zhang, female, is currently an Associate Professor at the School of\nInformation Engineering, Xuchang University, China. She received master’s\ndegree from Zhengzhou University, China, in 2006. Her current research\ninterests include computer vision, image processing, virtual reality, and\npattern recognition.\n\nEthics approval and consent to participate\nNot applicable.\n\nConsent for publication\nNot applicable.\n\nCompeting interests\nThe author declares that she has no competing interests. The author confirms\nthat the content of the manuscript has not been published or submitted for\npublication elsewhere.\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published\nmaps and institutional affiliations.\n\nReceived: 15 August 2018 Accepted: 5 November 2018\n\nReferences\n1. DITTO. http://www.ditto.com/\n2. O. Deniz, M. Castrillon, J. Lorenzo, et al., Computer vision based eyewear\n\nselector. Journal of Zhejiang University-SCIENCE C (Computers & Electronics)\n11(2), 79–91 (2010)\n\n3. Gongxin Xie. A transformation road for the glasses industry[J]. China Glasses,\n2014,03:112–113\n\n4. Liu Cheng, Wang Feng, QI Changhong, et al. A method of virtual glasses\ntry-on based on augmented reality[J]. Industrial Control Computer, 2014,\n27(12):66–69\n\n5. Boping Zhang. Design of mobile augmented reality game based on image\nrecognition[J]. EURASIP Journal on Image and Video Processing, 2017, 20:2–20\n\na b c d\nFig. 24 The part of interface for system operation\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 18 of 19\n\nhttp://www.ditto.com/\n\n\n6. Yan Lei, Yang Xiaogang, et al. Mobile augmented reality system design and\napplication based on image recognition[J], Journal of Image and Graphics,\n2016, 21(2):184–191\n\n7. Niswar A, Khan I R, Farbiz F. Virtual try-on of eyeglasses using 3D model of\nthe head[C]. International Conference on Virtual Reality Continuum and ITS\nApplications in Industry. New York: ACM; 2011:435–438.\n\n8. Meijing[OL]. http://www.meijing.com/tryinon.html\n9. Kede [OL]. http://www.kede.com/frame\n10. Biyao [OL]. http://www.biyao.com/home/index.html\n11. Li Juan, Yang Jie. Eyeglasses try-on based on improved Poisson equations.\n\n2011 Conference on Multimedia Technology. New York: ICMT 2011. 2011;\n3058–3061.\n\n12. DU Yao,WANG Zhao-Zhong. Real-like virtual fitting for single image[J].\nComputer Systems Application, 2015, 24(4):19–20\n\n13. Y. Lu, W. Shi-Gang, et al., Technology of virtual eyeglasses try-on system\nbased on face pose estimation[J]. Chinese Optics 8(4), 582–588 (2015)\n\n14. Yuan M, Khan I R, Farbiz F, et al. A mixed reality virtual clothes try-on\nsystem[J]. IEEE Transactions on Multimedia. 2013;15(8):1958-968.\n\n15. Huang W Y, et al. Vision-based virtual eyeglasses fitting system[C]. IEEE,\nInternational Symposium on Consumer Electronics. New York: IEEE. 2013;45–46\n\n16. Wang Feng, Qi Changhong, Liu Cheng, Jiang Wei, Ni Zhou, Zou Ya.\nReconstruction of 3D head model based on orthogonal images [J]. Journal\nof Southeast University (Natural Science Edition). 2015;45(1):36-40.\n\n17. Zhang B. Cluster Comput. 2017. https://doi.org/10.1007/s10586-017-1330-5.\n18. Maatta J, Hadid A, Pietikainen M. Face spoofing detection from single images\n\nusing texture and local shape analysis[J]. IET Biometrics, 2012, 1(1):3–10\n19. Lin Y, Lv F, Zhu S, et al. Large-scale image classification: fast feature\n\nextraction and svm training[C]. Computer Vision and Pattern Recognition\n(CVPR), 2011 IEEE Conference on. New York: IEEE; 2011:1689–1696\n\n20. Lowe D G, Lowe D G. Distinctive image features from scale-invariant\nkeypoints[J]. Int. J. Comput. Vis., 2004, 60(2):91–110\n\n21. Zhang Boping. Research on automatic recognition of color multi\ndimensional face images under variable illumination[J]. Microelectronics &\nComputer, 2017,34(5) :128–132\n\n22. MING An-Long MA Hua-dong. Region-SIFT descriptor based\ncorrespondence between multiple cameras[J]. CHIN ESE JOURNA L OF\nCOMPUTERS, 2008, 12(4):650–662\n\n23. He Kai, Wang Xiaowen, Ge Yunfeng. Adaptive support-weight stereo\nmatching algorithm based on SIFT descriptors[J]. Journal of Tiajin University,\n2016, Vol.49(9):978–984\n\n24. D.G. Lowe, Distinctive image features from scale-invariant key points. Int. J.\nComput. Vis. 60(2), 91–110 (2004)\n\n25. Chen Guangxi, Gong Zhenting, et al. Fast image recognition method Bsded\non locality-constrained linear coding[J]. Computer Science, 2016, vol. 43(5),\n308–314\n\n26. Bai Tingzhu, Hou Xibao. An improved image matching algorithm base on\nSIFT[J]. Transaction of Beijing Institute of Technology, 2013, 33(6):622–627\n\n27. Xiong X, Tome F D L. Supervised descent method and its applications to face\nalignment[C].Computer Vision and Pattern Recognition. New York: IEEE; 2013:\n532–539\n\n28. Zhu JE, et al. Real-Time Non-rigid Shape Recovery Via Active Appearance\nModels for Augmented Reality (Proc. Of 9th European Conference on\nComputer Vision, Graz, 2006), pp. 186–197\n\n29. Huber P, Hu G, Tena R, et al. A multiresolution 3D morphable face model\nand fitting framework[C]. Visapp. 2015\n\n30. Zhang Z. A flexible new technique for camera calibration[J]. IEEE Transactions\non Pattern Analysis&Machine Intelligence, 2000, 22(11):1330–1334\n\n31. Yang H, Patras I. Sieving Regression Forest Votes for Facial Feature\nDetection in the Wild[C]. New York: ICCV; 2013:1936–1943\n\n32. Dantone M, Gall J, Fanelli G, et al. Real-time facial feature detection using\nconditional regression forests[C]. Computer Vision and Pattern Recognition.\nNew York: IEEE; 2012:2578–2585\n\n33. Google Release online AR mobile games Ingress[OL]. http://www.csdn.net/\narticle/2012-11-16/2811943-google-launches-ingress\n\n34. D. Shreiner, G. Sellers, J.M. Kessenich, B.M. Licea-Kane, OpenGL Programming\nGuide: The Official Guide to Learning OpenGL, 8th edn. (Addison-Wesley\nProfessional, United States, 2013)\n\n35. J. Kim, S. Forsythe, Adoption of virtual try-on technology for online apparel\nshopping. J. Interact. Mark. 22, 45–59 (2008)\n\n36. A. Merle, S. Senecal, A. St-Onge, Whether and how virtual try-on influences\nconsumer responses to an apparel web site. Int. J. Electron. Commer. 16,\n41–64 (2012)\n\n37. Niswar, A.; Khan, I.R.; Farbiz, F. In Virtual try-on of eyeglasses using 3d model\nof the head, Proceedings of the 10th International Conference on Virtual\nReality Continuum and Its Applications in Industry. New York: ACM; 2011.\npp 435–438\n\n38. Koestinger M, Wohlhart P, Roth PM, Bischof H. Annotated facial landmarks\nin the wild: A large-scale, real-world database for facial landmark\nlocalization. First IEEE International Workshop on Benchmarking Facial\nImage Analysis Technologies, 2011\n\n39. Q. Zhou, Multi-layer affective computing model based on emotional\npsychology. Electron. Commer. Res. 18(1), 109–124 (2018). https://doi.org/\n10.1007/s10660-017-9265-8\n\n40. Q. Zhou, Z. Xu, N.Y. Yen, User sentiment analysis based on social network\ninformation and its application in consumer reconstruction intention.\nComput. Hum. Behav. (2018) https://doi.org/10.1016/j.chb.2018.07.006\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 19 of 19\n\nhttp://www.meijing.com/tryinon.html\nhttp://www.kede.com/frame\nhttp://www.biyao.com/home/index.html\nhttps://doi.org/10.1007/s10586-017-1330-5\nhttp://www.csdn.net/article/2012-11-16/2811943-google-launches-ingress\nhttp://www.csdn.net/article/2012-11-16/2811943-google-launches-ingress\nhttps://doi.org/10.1007/s10660-017-9265-8\nhttps://doi.org/10.1007/s10660-017-9265-8\nhttps://doi.org/10.1016/j.chb.2018.07.006\n\n\tAbstract\n\tIntroduction\n\tResearch status of network virtual try-on technology\n\tMethods of face recognition\n\tSVM-based face detection\n\tFace recognition based on SIFT\n\tBasic principle of SIFT algorithm\n\tKey point matching\n\tFace recognition experiment\n\n\tFace alignment\n\tImage normalization\n\tLocal feature extraction of SIFT algorithm\n\tSDM algorithm alignment result\n\n\tFace pose estimation\n\tFeature point labelling\n\tCamera labeling\n\tFeature point mapping\n\n\tTracking registration system\n\tAffine transformation method of glasses try-on\n\tPerspective transformation method of glasses try-on\n\n\tVirtual model generation system\n\tVirtual and real synthesis system\n\n\tiOS system application\n\tMenu module\n\tUser registration module\n\tCommodity module\n\tCommodity browsing\n\tCommodity screening\n\tGlasses try-on\n\tAdding to cart\n\tBuy a glasses immediately\n\tTaking photos or recording videos\n\tUploading and sharing\n\n\tCollection module\n\tShopping cart module\n\tOrder module\n\tCoupon module\n\tPhoto wall module\n\tSetting module\n\n\tResults and discussion\n\tConclusions\n\tAbbreviations\n\tAcknowledgements\n\tFunding\n\tAvailability of data and materials\n\tAuthor’s contributions\n\tAuthor’s information\n\tEthics approval and consent to participate\n\tConsent for publication\n\tCompeting interests\n\tPublisher’s Note\n\tReferences\n\n",
      "metadata_author": "Boping Zhang",
      "metadata_title": "Augmented reality virtual glasses try-on technology based on iOS platform",
      "metadata_creation_date": "2018-11-23T11:51:48Z"
    },
    {
      "@search.score": 0.35695967,
      "content": "\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 \nDOI 10.1186/s40493-015-0019-z\n\nRESEARCH Open Access\n\nToward a testbed for evaluating\ncomputational trust models: experiments\nand analysis\nPartheeban Chandrasekaran and Babak Esfandiari*\n\n*Correspondence:\nbabak@sce.carleton.ca\nDepartment of Systems and\nComputer Engineering, Carleton\nUniversity, 1125 Colonel By Drive,\nOttawa, Ontario K1s5B6, Canada\n\nAbstract\nWe propose a generic testbed for evaluating social trust models and we show how\nexisting models can fit our tesbed. To showcase the flexibility of our design, we\nimplemented a prototype and evaluated three trust algorithms, namely EigenTrust,\nPeerTrust and Appleseed, for their vulnerabilites to attacks and compliance to various\ntrust properties. For example, we were able to exhibit discrepancies between\nEigenTrust and PeerTrust, as well as trade-offs between resistance to slandering attacks\nversus self-promotion.\n\nKeywords: Trust testbed; Reputation; Multi-agent systems\n\nIntroduction\nMotivation\n\nWith the growth of online community-based systems such as peer-to-peer file-sharing\napplications, e-commerce and social networking websites, there is an increasing need to\nprovide computational trust mechanisms to determine which users or agents are honest\nand which ones are malicious. Many models calculate trust by relying on analyzing a\nhistory of interactions. The calculations can range from the simple averaging of ratings\non eBay to flow-based scores in the Advogato website. Thus for a researcher to evaluate\nand compare his or her latest model against existing ones, a comprehensive test tool is\nneeded. However, our research shows that the tools that exist to assist researchers are not\nflexible enough to include different trust models and their evaluations. Moreover, these\ntools use their own set of application-dependent metrics to evaluate a reputation system.\nThis means that a number of trust models cannot be evaluated for vulnerabilities against\ncertain types of attacks. Thus, there is still a need for a generic testbed to evaluate and\ncompare computational trust models.\n\nOverview of our solution and contributions\n\nIn this paper, we present a model and a testbed for evaluating a family of trust algo-\nrithms that rely on past transactions between agents. Trust assessment is viewed as a\nprocess consisting of a succession of graph transformations, where the agents form the\nvertices of the graph. The meaning of the edges depends on the transformation stage,\n\n© 2015 Chandrasekaran and Esfandiari. Open Access This article is distributed under the terms of the Creative Commons\nAttribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution,\nand reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to\nthe Creative Commons license, and indicate if changes were made.\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40493-015-0019-z-x&domain=pdf\nmailto: babak@sce.carleton.ca\nhttp://creativecommons.org/licenses/by/4.0/\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 2 of 27\n\nand can refer to the presence of transactions between the two agents or the existence\nof a trust relationship between them. Our first contribution is to show that with this\nview, existing reputation systems can be adopted under a single model, but they work at\ndifferent stages of the trust assessment workflow. This allows us to present a new classi-\nfication scheme for a number of trust models based on where they fit in the assessment\nworkflow. The second contribution of our work is that this workflow can be described\nformally, and by doing this, we show that it is possible to model a variety of attacks\nand evaluation schemes. Finally, out of the larger number of systems we classified, we\nselected three reputation systems, namely EigenTrust [1], PeerTrust [2] and Appleseed\n[3], to exemplify the range and variety of reputation systems that our testbed can accom-\nmodate. We evaluated these three systems in our testbed against simple attacks and\nwe validated their compliance to basic trust properties. In particular, we were able to\nexhibit differences in the way EigenTrust and PeerTrust rank the agents, we observed\nthe subtle interplay between slandering and self-promoting attacks (higher sensitivity\nto one attack can lead to lower sensitivity to the other), and we verified that trust\nweakens along a friend-of-a-friend chain and that it is more easily lost than gained\n(as it should be).\n\nOrganization\n\nThis article is organized as follows: section ‘Background and literature review’ provides\nbackground and state of the art on trust models, attacks against them, and existing\ntestbeds for evaluation. Section ‘Problem description and model’ formulates the research\nproblem of this article and proposes our model for a testbed. Section ‘Classifying and\nchaining algorithms’ shows how some of existing trust algorithms can fit our model, and\nhow one can combine or compare them using our model and testbed. Section ‘Results and\ndiscussion’ describes the implementation details of our testbed prototype and presents\nevaluation results of three different trust algorithms, namely EigenTrust, PeerTrust, and\nAppleseed. Section ‘Conclusions’ concludes this article and summarizes the contributions\nand limitations of our work.\n\nBackground and literature review\nSocial trust models\n\nTrust management systems aid agents in establishing and assessing mutual trust. How-\never, the actual mechanisms used in these systems vary. For example, public key infras-\ntructures [4] rely on certificates whereas reputation-based trust management systems are\nbased on experiences of earlier direct and indirect interactions [5].\nIn this paper we will focus on social trust models based on reputation. The trust model\n\nshould provide a means to compare the trustworthiness of agents in order to choose a\nparticular agent to perform an action. For instance, on an e-commerce website like eBay,\nwe need to be able to compare the trustworthiness of sellers in order to pick the most\ntrustworthy one to buy a product from.\nSocial trust models rely on past experiences of agents to produce trust assertions. That\n\nis, the agents in the system interact with each other and record their experiences, which\nare then used to determine whether a particular agent is trustworthy. This model is self-\nsufficient because it does not rely on a third party to propagate trust, like it would in\ncertificate authority-based PKI trust models. However, there are drawbacks to having no\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 3 of 27\n\nroot of trust. For instance, agents evaluating the trustworthiness of agents with whom\nthere has been no interaction must use recommendations from others and, in turn,\nevaluate the trustworthiness of the recommenders. Social trust models must address this\nproblem.\n\nNature of input\n\nVarious inputs are used by social trust algorithms to measure the trustworthiness of\nagents. In EigenTrust [1], PeerTrust [2], TRAVOS [6] and Beta Reputation System (BRS)\n[7], agents rate their satisfaction after a transaction (e.g., downloading a file in a P2P\nfile-sharing network). These ratings are used to obtain a trust score that represents the\ntrustworthiness of the agent. In Aberer and Despotovic’s system [5]1, agents may file com-\nplaints (can be seen as dissatisfaction) about each other after a transaction. In Advogato\n[8], whose goal is to discourage spam on its blogging website, users explicitly certify\neach other as belonging to a particular level in the community. Trust algorithms may\nalso directly use trust scores among agents to compute an aggregated trustworthiness\nof agents, as in TidalTrust [9] and Appleseed [3]. In the specific context of P2P file-\nsharing, Credence [10] uses the votes on file authenticity to calculate a similarity score\nbetween agents and uses it to measure trust. The trust score is then used to recommend\nfiles.\n\nDirect vs. indirect trust\n\nThe truster may use some or all of its own and other agents’ past experiences with the\ntrustee to obtain a trust score. Trust algorithms often use gossiping to poll agents with\nwhom the truster has had interactions in the past.\nThe trust score calculated using only the experiences from direct interactions is\n\ncalled the direct trust score, while the trust score calculated using the recommenda-\ntions from other agents is called the indirect trust score [11]. As mentioned earlier,\nreputation systems use different inputs (satisfaction ratings, votes, certificates, etc.) to\ncalculate direct trust scores and indirect trust scores. PeerTrust uses satisfaction ratings\nto calculate both direct and indirect trust scores, whereas EigenTrust and TRAVOS\nuse satisfaction ratings to calculate direct trust scores, which they then use to calcu-\nlate indirect trust scores. Therefore, we can categorize the trust algorithms based on\nthe input required. But how do trust algorithms calculate the trust scores of agents\nusing the above information? It again varies from algorithm to algorithm. For instance,\nPeerTrust, EigenTrust, and Aberer use simple averaging of ratings, TRAVOS and BRS\nuse the beta probability density function, and Appleseed uses the Spreading Activation\nmodel.\n\nGlobal vs. local trust\n\nThe trust algorithm may output a global trust score or a local trust score [3, 12]. A global\ntrust score is one that represents the general trust that all agents have on a particular\nagent, whereas local trust scores represents the trust from the perspective of the truster\nand thus each truster may trust an agent differently. In our survey, we found PeerTrust,\nEigenTrust, and Aberer to be global trust algorithms whereas TRAVOS, BRS, Credence,\nAdvogato, TidalTrust, Appleseed, Marsh [13] and Abdul-Rahman [14] are local trust\nalgorithms.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 4 of 27\n\nTo trust or not to trust\n\nOnce the trust score is calculated, it can be used to decide whether to trust the agent. It\ncan be as simple as comparing the trust score against a threshold: if the trust score is above\na certain threshold, then the agent is trusted. Marsh [13], and Aberer [5] use thresholding\ntechniques. If the trust algorithm outputs normalized trust scores of agents as in Eigen-\nTrust, then the trust scores of agents are ranked. In this case, one may consider a certain\npercentage of the top ranked agents as trustworthy. In Appleseed, a graph is first obtained\nwith trust scores of agents as edge weights, and then, the truster agent is “injected” with\na value called the activation energy. This energy is spread to agents with a spreading fac-\ntor along the edges in the graph and the algorithm ranks the agents according to their\ntrust scores. Trust decisions can also be flow-based such as in Advogato, which calculates\na maximum “flow of trust” in the trust graph to determine which agents are trustworthy\nand which are not.\nIn short, social trust models focus on the following:\n\n1. What is the input to calculate the trust score of an agent?\n2. Does the trust algorithm use only direct experience or does it also rely on third\n\nparty recommendations?\n3. Is the trust score of an agent global or local?\n4. How does one decide whether to trust an agent?\n\nGiven the above discussion, and to assess the scope of our testbed, we propose tomodel,\nevaluate and compare three algorithms from fairly different families. The next sections\nprovide detailed descriptions of the trust models we selected and that we implemented in\nour testbed. The details are given to help understand the output of our experiments, but\nreaders familiar with EigenTrust, PeerTrust and/or AppleSeed may skip those respective\nsections.\n\nPeerTrust\n\nIn PeerTrust, agents rate each other in terms of the satisfaction received. These ratings\nare weighted by trust scores of the raters, and a global trust score is computed recursively\nusing Eq. 2.1, where:\n\n• T(u) is the trust score of agent u\n• I(u) is the set of transactions that agent u had with all the agents in the system\n• S(u, i) is the satisfaction rating on u for transaction i\n• p(u, i) is the agent that provided the rating.\n\nT(u) =\nI(u)∑\ni=1\n\nS(u, i) × T(p(u, i))∑I(u)\nj=1 T(p(u, j))\n\n(2.1)\n\nPeerTrust also provides a method for calculating local trust scores. In both local and\nglobal trust score computations, the trust score is compared against a threshold to decide\nwhether to trust or not.\n\nEigenTrust\n\nAgents in EigenTrust rate transactions as satisfactory or unsatisfactory [1]. These trans-\naction ratings are used as input, to calculate a local direct trust score, from which a global\ntrust score is then calculated.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 5 of 27\n\nAn agent i calculates the normalized local trust score of agent j, as shown in Eq. 2.2,\nwhere tij ∈ {+1,−1} is the transaction rating, and sij is the sum of ratings.\n\nsij =\n∑\nTij\n\ntrij\n\ncij = max(sij, 0)∑\nk max(sik , 0)\n\n(2.2)\n\nNote that we cannot use sij as the local trust score without normalizing, because mali-\ncious agents can arbitrarily assign high local trust values to fellow malicious agents and\nlow local trust values to honest agents.\nTo calculate the global trust score of an agent, the truster queries his friends for their\n\ntrust scores on the trustee. These local trust scores are aggregated, as shown in Eq. 2.3.\n\ntik =\n∑\nj\ncijcjk (2.3)\n\nIf we let C be the matrix containing cij elements,  ci be the local trust vector for i (each\nelement corresponds to the trust that i has in j), and  ti the vector containing tik , then,\n\n ti = CT  ci (2.4)\n\nBy asking a friend’s friend’s opinion, Eq. 2.4 becomes  ti = (CT )2  ci. If an agent keeps\nasking the opinions of its friends of friends, the whole trust graph can be explored, and\nEq. 2.4 becomes Eq. 2.5, where n is the number of hops from i.\n\n t = (CT )n  ci (2.5)\n\nThe trust scores of the agents converge to a global value irrespective of the trustee.\nBecause EigenTrust outputs global trust scores (normalized over the sum of all agents),\n\nagents are ranked according to their trust scores (unlike PeerTrust). Therefore, an agent\nis considered trustworthy if it is within a certain rank.\n\nAppleseed\n\nAppleseed is a flow-based algorithm [3]. Assuming that we are given a directed weighted\ngraph with agents as nodes, edges as trust relationships, and the weight of an edge as\ntrustworthiness of the sink, we can determine the amount of trust that flows in the graph.\nThat is, given a trust seed, an energy in ∈ R\n\n+\n0 , spreading factor decay ∈[ 0, 1], and conver-\n\ngence threshold Tc, Appleseed returns a trust score of agents from the perspective of the\ntrust seed.\nThe trust propagation from agent a to agent b is determined using Eq. 2.6, where the\n\nweight of edge (a, b) represents the amount of trust a places in b, and in(a) and in(b)\nrepresent the flow of trust into a and b, respectively.\n\nin(b) = decay ×\n∑\n\n(a,b)∈E\nin(a) × weight(a, b)∑\n\n(a,c)∈E weight(a, c)\n(2.6)\n\nThe trust of an agent b (trust(b)) is then updated using Eq. 2.7, where the decay factor\nensures that trust in an agent decreases as the path length from the seed increases.\n\ntrust(b) := trust(b) + (1 − decay) × in(b) (2.7)\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 6 of 27\n\nGenerally, trust graphs have loops, which makes Eq. 2.7 recursive. Thus a termination\ncondition like the one below is required, where Ai ⊆ A is the set of nodes that were\ndiscovered until step i and trusti(x) is the current trust scores for all x ∈ Ai:\n\n∀x ∈ Ai : trusti(x) − trusti−1(x) ≤ Tc (2.8)\n\nAfter Eq. 2.7 terminates, the trust scores of agents are ranked. Since this set is ranked\nfrom the perspective of the seed, Appleseed is a local trust algorithm.\nAs our brief survey shows, the trust models vary in terms of their input, output, and\n\nthe methods they use. To evaluate and compare them, testbeds are needed. In the next\nsection we take a look at existing testbeds.\n\nTestbeds\n\nWe investigated two testbed models, namely Guha’s [15] andMacau [16], and two testbed\nimplementations, namely ART [17] and TREET [18], which are used to evaluate trust\nalgorithms. This section provides details of our investigation.\n\nGuha\n\nGuha [15] proposes a model to capture document recommendation systems, where trust\nand reputation play an important role. The model relies on a graph of agents where the\nedges can be weighted based on their mutual ratings, and a rating function for documents\nby agents. Guha then discusses how trust can be calculated based on those ratings, and\nevaluates a few case studies of real systems that can be accommodated by the model.\nGuha’s model can capture trust systems that take a set of documents and their ratings\n\nas input (such as Credence [10]), but it cannot accommodate systems where the only\ninput consists of direct feedbacks between agents, such as in PeerTrust (global) [2] or\nEigenTrust [1]. Also, the rating of documents is itself an output of Guha’s model, and that\nis often not the purpose or output of many more general-purpose trust models.\nIn short, document recommendation systems can be viewed as a specialization or\n\nsubclass of more general trust systems, and Guha’s model is suitable for that subclass.\n\nMacau\n\nHazard and Singh’s Macau [16] is a model for evaluating reputation systems. The authors\ndistinguish two roles for any agent: a rater that evaluates a target. Transactions are viewed\nas a favor provided by the target to the rater. The target’s reputation, local to each rater-\ntarget pairing, is updated after each transaction and depends on the previous reputation\nvalue. The target’s payoff in giving a favor is also dependent on its current reputation but\nalso on its belief of the likelihood that the rater will in turn return the favor in the future.\nBased on the above definitions, the authors define a set of desirable properties for a\n\nreputation system:\n\n• Monotonicity: given two different targets a and b, the computed reputation of a\nshould be higher than that of b if the predicted payoff of a transaction with a is\nhigher than with b.\n\n• Unambiguity and convergence: the reputation should converge over time to a single\nfixpoint, regardless of its initial value.\n\n• Accuracy: this convergence should happen quickly, thus minimizing the total\nreputation estimation errors in the meantime.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 7 of 27\n\nMacau thus captures an important stage in trust assessment, i.e. the update of one-to-\none trustworthiness based on past transactions. It has been used to evaluate, in terms of\ntheir compliance to the properties defined above, algorithms such as TRAVOS [6] and the\nBeta Reputation System (BRS) [7] that model positive and negative experiences as ran-\ndom variables following a beta probability distribution. The comparison of trust models\nrelying on the beta distribution and their resilience to various attacks has also recently\nbeen explored in [19].\n\nART\n\nThe Agent Reputation and Trust testbed (ART) [17] provides an open-source message-\ndriven simulation engine for implementing and comparing the performance of reputation\nsystems. ART uses art painting sales as the domain.\nEach client has to sell paintings belonging to a particular era. To determine their\n\nmarket values, clients refer to agents for appraisals for a fee. Because each agent\nis an expert only in a specific era, it may not be able to provide appraisals for\npaintings from other eras and therefore refers to other agents for a fee. After such\ninteractions, agents record their experiences, calculate their reputation scores, and\nuse them to choose the most trustworthy agents for future interactions. The goal\nof each agent is to finish the simulation with the highest bank balance, and, intu-\nitively, the winning agent’s trust mechanism knows the right agents to trust for\nrecommendations.\nThe ART testbed provides a protocol that each agent must implement. The protocol\n\nspecifies the possible messages that agents can send to each other. Themessages are deliv-\nered by the simulation engine, which loops over each agent at every time interval. The\nengine is also responsible for keeping track of the bank balance of the agents, and assign-\ning new clients to agents. All results are collected and stored in a database and displayed\non a graphical user interface (GUI) at runtime.\nART is best suited for evaluating trust calculation schemes from a first person point\n\nof view. It is not meant as a platform for testing trust management as a service provided\nby the system. For example, to evaluate EigenTrust in ART, one would either need to\nconsiderably modify ART itself (for the centralized version of EigenTrust) or to require\ncooperation from the participating agents and an additional dedicated distributed infras-\ntructure (for the distributed version). Furthermore, as also pointed out in [16] and [20],\nthe comparison of the performance of different agents is not necessarily based on their\ncorrect ability to assess the reputation of other agents, but rather based on how well they\nmodel and exploit the problem domain.\n\nTREET\n\nThe Trust and Reputation Experimentation and Evaluation Testbed (TREET) [18] mod-\nels a general marketplace scenario where there are buyers, sellers, and 1,000 different\nproducts with varying prices, such that there are more inexpensive items than expensive\nones. The sale price of the products is fixed, to avoid the influence of market competition.\nThe cost of producing an item is 75% of the selling price, and the seller incurs this cost.\nTo lower this cost and increase profit, a seller can cheat by not shipping the item. Each\nproduct also has a utility value of 110% of the selling price, which encourages buyers to\npurchase.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 8 of 27\n\nAgents join or exit after 100 simulation days or after a day with a probability of 0.05,\nbut to keep the number of buyers and sellers constant, an agent is introduced for each\ndeparting agent. At initialization, each seller is assigned a random number of products\nto sell. Buyers evaluate the offers from each seller and pick a seller. Sellers are informed\nof the accepted offers and are paid. Fourteen days after a sale, the buyer knows whether\nhe has been cheated or not, depending on whether he receives the purchased item. The\nbuyer then provides feedback based on his experience of the transaction. The feedback is\nin turn used to choose sellers for future transactions.\nTREET evaluates the performance of various reputation systems under Reputation Lag\n\nattack, Proliferation attack, and Value Imbalance attack using the following metrics:\n\n1. cheater sales over honest sales ratio\n2. cheater profit over honest profit ratio\n\nMultiple seller accounts are needed to orchestrate a Proliferation Attack, but TREET\ndoes not consider attacks such as White-Washing and Self-Promoting, which require\ncreating multiple buyer accounts.\nTREET addresses many of ART’s limitation in a marketplace scenario. To name a\n\nfew [21], TREET supports both centralized and decentralized trust algorithms, allows\ncollusion attacks to be implemented, and does not put a restriction on trust score rep-\nresentation. However, like ART, the evaluation metrics in TREET are tightly coupled to\nthe marketplace domain. It is unclear how ART or TREET can be used to evaluate trust\nmodels used in other systems, such as P2P file-sharing networks, online product review\nwebsites and others that use trust. To our knowledge, there is no testbed that provides\ngeneric evaluation metrics and that is independent of the application domain.\n\nSummary\n\nTrust is a tool used in the decision-making process and it can be computed. There are\nmanymodels based on social trust that attempt to aid agents in making rational decisions.\nHowever, these models vary in terms of their input and output requirements. This makes\nevaluations against a common set of attacks difficult.\n\nProblem description andmodel\nOur goal is to have a testbed that is generic enough to accommodate as many trust\nmanagement systems and models as possible. Our requirements are:\n\n1. A model that provides an abstraction layer for developers to incorporate existing\nand new systems that match the input and output of the model.\n\n2. An evaluation framework to measure and compare the performance of trust models\nagainst trust properties and attacks independently of the application domain.\n\nIn this section, we introduce an abstract model for trust management systems. This\nmodel will be the foundation of our testbed. Our model is essentially based on the\nfollowing stages:\n\n1. In stage 1 of the trust assessment process, the feedback provided by agents on other\nagents is represented as a feedback history graph.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 9 of 27\n\n2. In stage 2, a reputation graph is produced, where the weight of an arc denotes the\nreputation of the target agent. “Reputation” here follows [14], as “an expectation\nabout an individual’s behavior based on information about or observations of its\npast behavior”. It is viewed as an estimation of trustworthiness based on a\ncombination of direct and indirect feedback.\n\n3. In the final stage, a trust graph is produced, where the existence of an arc implies\ntrust in the target agent. We take “trust” here to mean the “belief by agent A that\nagent B is trustworthy” [2, 22], and so it is boolean and subjective in our model.\n\nIn the rest of this section, we define the aforementioned graphs in stages.\n\nStage 1—obtain feedback history graph\n\nWe first define a feedback, f (a, b) ∈ R as an assessment made by agent a of an action or\ngroup of actions performed by agent b, where a and b belong to the set A of all the agents\nin the system. The list of n feedbacks by a on b, FHG(a, b), is called a feedback history,\nrepresented as follows:\n\nFHG(a, b)  → (f1(a, b), f2(a, b), . . . , fn(a, b)) (3.1)\n\nThe feedback fi(a, b) indicates the ith satisfaction received by a from b’s action. For\nexample, in a file-sharing network, the feedback by a downloader may indicate the sat-\nisfaction received from downloading a file from an uploader in terms of a value in R.\nExisting trust models use different ranges of values for feedback, and letting the feedback\nvalue be in R allows us to include these reputation systems in our testbed.\nIf A is the set of agents, E is the set of labelled arcs (a, b), and the label is FHG(a, b)\n\nwhen FHG(a, b) \t= ∅, then the feedback histories for all agents in A are represented in a\ndirected and labelled graph called Feedback History Graph (FHG)2, FHG = (A,E):\n\nFHG : A × A → R\nN\n\n∗\n(3.2)\n\nNote that we have not included timestamps associated with each feedback (which would\nbe useful for, among other things, running our testbed as a discrete event simulator), but\nour model can be expanded to accommodate it.\nOnce the feedback history graph is obtained, the next step is to produce a reputation\n\ngraph.\n\nStage 2—obtain reputation graph\n\nA Reputation Graph (RG), RG = (A,E′\n), is a directed and weighted graph, where the\n\nweight on an arc, RG(a, b), is the trustworthiness of b from a’s perspective:\n\nRG : A × A → R (3.3)\n\nThe edges are added by computing second and nth-hand trust via transitive closure of\nedges in E. That is: if (a, b) ∈ E and (b, c) ∈ E ⇒ (a, b), (b, c), and (a, c) ∈ E′ (the value of\nthe weight of the edges, however, depends on the particular trust algorithm).\nReputation algorithms may also exhibit the reflexive property by adding looping arcs to\n\nindicate that the truster trusts itself to a certain degree for a particular task [1–3].\nThe existing literature categorizes reputation algorithms into two groups: local and\n\nglobal (Figs. 1(a) and (b), respectively) [3, 5]. Global algorithms assign a single reputa-\ntion score to each agent. Therefore, if a global algorithm is used, then the weights of the\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 10 of 27\n\nFig. 1 Examples of reputation graphs output respectively by a local and global algorithm\n\nincoming arcs of an agent should be the same, as shown in Fig. 1(b) (although for clar-\nity’s sake we will often present the graph simply as a ranking of agents in the rest of this\narticle). There is no such property for local algorithms.\nReputation algorithms may also differ in how the graphs is produced. One method is\n\nto first calculate one-to-one scores of agents using direct feedbacks and then use them\nto calculate the trustworthiness of agents previously unknown to the truster (e.g., Eigen-\nTrust). This is shown as 1a and 1b in Fig. 2. The other method (#2 in Fig. 2) skips the\nintermediate graph in the aforementioned method and produces a reputation graph (e.g.,\nPeerTrust).\n\nStage 3—obtain trust graph\n\nThe graph obtained in stage 2 contains information about the trustworthiness of agents.\nBut to use this information to make a decision about a transaction in the future, agents\nmust convert trustworthiness to boolean trust (see [23] for an example), which can also\nbe expressed as a graph. We refer to this directed graph as the Trust Graph (TG) TG =\n(A, F), where a directed edge ab ∈ F represents agent a trusting agent b.\nTo summarize ourmodel, we can represent the stages as part of a workflow as illustrated\n\nin Fig. 3.\n\nFig. 2 Two methods to obtain a reputation graph\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 11 of 27\n\nFig. 3 Overview of the stages in our model\n\nIn the next section, we see at what stages in our model do various algorithms fit, and\ndescribe criteria for chaining different algorithms.\n\nClassifying and chaining algorithms\nBy refactoring the trust models according to the stages presented in the above sections,\nwe start to see a new classification scheme. Let us take EigenTrust, PeerTrust, and Apple-\nseed as examples and describe them using our model. EigenTrust takes an FHG with\nedge labels in {0, 1}∗ as input and outputs an RG with edge labels in [ 0, 1]. PeerTrust,\non the other hand, takes an FHG with edge labels in [ 0, 1]∗ as input and outputs an\nRG with edge labels in [ 0, 1]. Meanwhile, Appleseed requires an RG with edge labels in\n[ 0, 1] as input and outputs another RG′ in the same codomain. It is also possible for an\nalgorithm to skip some stages. For example, according to our model, Aberer [5] skips\nstage 2 and does not output a reputation graph. One can also represent simple mecha-\nnisms to generate a trust graph by applying a threshold on reputation values (as output\nfor example by EigenTrust), or by selecting the top k agents. This stage transitions of\nalgorithms are depicted3 in Fig. 4. In addition to the existing classification criteria in the\nstate of the art, trust algorithms can now be classified according to their stage transi-\ntions (i.e., from one stage to another as well as transitioning within a stage) as shown in\nTable 1.\nIt is important to note that although these three algorithms output a reputation\n\ngraph with continuous reputation values between 0 and 1, the semantics of these val-\nues are different. EigenTrust outputs relative (among agents) global reputation scores,\nPeerTrust outputs an absolute global reputation score, and Appleseed produces relative\nlocal reputation scores. In other words, EigenTrust and Appleseed are ranking algorithms\n(global and local, respectively), whereas PeerTrust is not.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 12 of 27\n\nFig. 4 Stage transitions of Trust algorithms\n\nAs we can see, each step of the trust assessment process can be viewed as a\ngraph transformation function, and we can use this functional view to easily describe\nevaluation mechanisms as well. Suppose an experimenter wants to compare PeerTrust\nand EigenTrust. The inputs and outputs of these algorithms are semantically different.\nTo match the input, we can use a function that discretizes continuous feedback values\n(f (a, b)) in [0, 1] to {-1, 1}, using some threshold t:\n\nTable 1 A classification for trust models\n\nStage Global or\nAbsolute or\n\nTrust Algorithm\nTransitions\n\nInput\nLocal\n\nRelative\nReputation Scores\n\nEigenTrust 0 → 2\nsatisfaction\n\nglobal relativeratings\n\nPeerTrust 0 → 2\nsatisfaction\n\nglobal absoluteratings\n\nAppleSeed 2 → 2\nreputation\n\nlocal absolutescores\n\nAberer & Despotovic 0 → 3 complaints global N/A\n\nAdvogato 3 → 3 certificates local N/A\n\nTRAVOS 0 → 2\nsatisfaction\n\nlocal absoluteratings\n\nRanking 2 → 3\nreputation\n\nN/A relativescores\n\nThresholding 2 → 3\nreputation\n\nN/A absolutescores\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 13 of 27\n\ndiscretizet : [ 0, 1] → {−1, 1}\n\nx  → discretizet(x) =\n{\n\n−1 if x ≤ t\n1 if x > t\n\n(4.1)\n\nTo lighten the notations, in what follows we will adopt a default threshold of 0.5 and drop\nt. We will also, in an abuse of notation, actually use discretize : FHG→ FHG, which applies\nthe function defined in 4.1 to every feedback of every edge of the graph.\nLet us turn to the output now. Recall that the output of EigenTrust is a relative global\n\nscore while PeerTrust’s is also global but absolute. To make the outputs more directly\ncomparable, we can use a normalization function on PeerTrust’s output, ensuring that the\nsum of outgoing weights for each agent is 1. It takes a reputation graph RG2 with edge\nlabels in [ 0, 1] as input, and outputs another reputation graph RG3 with edge labels in\n[ 0, 1], but where the reputation scores are relative to one another. This is achieved using\nEq. 4.2, where N(a) is the set of agents adjacent to a via outgoing edges.\n\nnormalize : A × A →[ 0, 1]\n\n(a, b)  → normalize(a, b) = RG(a, b)∑\nc∈N(a) RG(a, c)\n\n(4.2)\n\nAgain, in an abuse of notation we will also refer in what follows to normalize as the func-\ntion in RG→ RG that applies 4.2 to every edge of the graph. Note again that normalization\nisn’t strictly speaking a necessary step if all one is concerned about is the ranking of the\nagent, and not the values attached to each agent, especially since the semantics of these\nvalues are still ultimately attached to the algorithm that is computing them. Normaliza-\ntion does not affect the ranking. Either way, Spearman’s rank correlation coefficient [24]\ncan finally be used to compare the rankings output by global trust functions such as Eigen-\nTrust and PeerTrust. This metric ∈[−1, 1] specifies the degree to which the ranking order\nof the outputs match, where 1 means a perfect match and -1 means no match: spearman:\nRG × RG → [−1, 1].\nThe choice of discretization and normalization methods are important and they may\n\nintroduce a bias when comparing two algorithms. For the purpose of demonstrating our\nmodel, we will assume that our choices are good enough.\nBy combining these algorithms, we can represent the workflow as shown in Fig. 5.\nTo more clearly capture trust management algorithms as a process consisting of graph\n\ntransformation operations, and to be able to express them in our experiments as such,\nwe introduce named functions that make these transformations explicit. Here is an\nillustration of the convention we will follow: the function that captures EigenTrust’s trans-\nformation from an FHG to an RGwill be named f 2reigentrust , where “f ” stands for FHG and\n“r” for RG. Hence, in functional programming terms, the experiment comparing Eigen-\nTrust and PeerTrust using a given feedback history graph FHG0 can be specified very\nsimply as follows:\n\nspearman(f 2reigentrust(f 2fdiscretize(FHG0)), r2rnormalize(f 2rpeertrust(FHG0)))\n\nIn the current implementation of our testbed, we assume that the pre-condition checks\nas per Table 1 are enforced by the algorithms rather than the testbed. However, this can\nbe modified easily in the future.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 14 of 27\n\nFig. 5 Comparing EigenTrust and PeerTrust\n\nIn the next section, we present simple experiments that we ran to analyze trust\nalgorithms using the model described in this section.\n\nResults and discussion\nUsing the prototype testbed that we implemented following the model presented in\nthe previous section, we conducted several experiments and analyzed their results on\nthree trust algorithms, namely EigenTrust, PeerTrust, and Appleseed. The experiments\nthat we present in this section are grouped into two categories: vulnerability assess-\nments and trust properties assessments. But first we give a general description of the\nimplementation.\n\nImplementation\n\nA prototype was designed and built in Java to test the model described in Section ‘Prob-\nlem description and model’. The two main components of the testbed are graphs and\nalgorithms. A Graph can be a feedback history graph, a reputation graph or a trust graph.\nThese graphs follow our model as described in the previous section.\nThe graphs can be populated either programatically or by providing a file following\n\nthe Attribute-Relation File Format (ARFF) used in the Weka machine learning toolkit\n(ARFF was chosen for its simplicity, but other formats could obviously be accommodated\nas well in the future if needed). For instance, a feedback history graph is populated with\na file containing a list of relations, each containing 3 attributes: source agent identifier,\nsink agent identifier and the feedback value. An example of a ARFF file for populating a\nfeedback history graph is given in Listing 1:\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 15 of 27\n\nListing 1 An example ARFF file for populating a feedback history graph\n\n@relation feedback\n@attribute assessorID string\n@attribute assesseeID string\n@attribute feedbackValue numeric\n@data\n0,1,0.6\n0,1,0.7\n0,1,0.2\n\nIn our prototype, even though the algorithms are implemented using object-orientation,\nthey are ultimately presented to the experimenter as functions that take a graph as input\nand return a graph as the output, to fully conform to the model we have presented, and\nto make the experiments simple and intuitive to express. Other utility functions, such as\nevaluation, discretization and comparison, are also provided. See Listing 2 for an example.\n\nListing 2 How to create a simple experiment in our testbed\n\nimport static trust.Algorithms.*;\nFHG fhg = new FHG(\"ex.arff\");\neigentrust(discretize(fhg)); //the experiment!\n\nThe testbed’s source code is open and is available on Google Code4. To ensure that our\nalgorithm implementationsmatch the authors’ intentions and our own understanding, we\nalso provide a set of unit tests taken from examples found in the literature as well as our\nown additional scenarios.\n\nVulnerability assessments\n\nThe security of trust algorithms is measured by their resistance to attacks. In this section,\nwe subject them to attacks and assess their vulnerabilities. We will purposely try to come\nup with the simplest attack scenarios that can exhibit the properties we are looking for.\nThirunarayan et al. [19] follows a similar approach but restricted to trust algorithms\nrelying on the beta probability distribution.\n\nNormalization-based attack\n\nSetup In this first experiment, we want to exhibit whether two different trust algorithms\n(in this case, EigenTrust vs PeerTrust) may output different rankings given the same input.\nFor this we investigate how reputation is affected by the number of good feedbacks versus\nthe bad feedbacks. Suppose we were given FHG0 in Fig. 6(a) and FHG1 in Fig. 6(b). Since\nthere are more good feedbacks on agent 2 in FHG0 than FHG1, it is reasonable to expect\nagent 2’s reputation in RG1 to be lower than in RG2.\nWe normalize the output of PeerTrust simply to make the scale of the reputation num-\n\nbers similar to EigenTrust (we do not claim that the semantics are the same). And as\nEigenTrust requires feedbacks to be either positive (+1) or negative (-1), we first need to\nconvert the 1.0 values in the FHGs to +1 and the 0.0 values to -1, hence the use of the\ndiscretizer function in the experiment specifications below:\n\nspearman\n(\nf 2reigentrust\n\n(\nf 2fdiscretize(FHG0)\n\n)\n, f 2reigentrust\n\n(\nf 2fdiscretize(FHG1)\n\n))\nspearman\n\n(\nf 2rpeertrust(FHG0), f 2rpeertrust(FHG1)\n\n)\nspearman\n\n(\nf 2reigentrust\n\n(\nf 2fdiscretize(FHG0)\n\n)\n, f 2rpeertrust(FHG0)\n\n)\nspearman\n\n(\nf 2reigentrust\n\n(\nf 2fdiscretize(FHG1)\n\n)\n, f 2rpeertrust(FHG1)\n\n)\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 16 of 27\n\nFig. 6 Normalization-based Discrepancy\n\nResults Comparing the rankings in Table 2, we note that EigenTrust reports no change\nin agent 2’s rank, whereas in PeerTrust its rank has changed from 1 to 2 (Spearman’s\ncoefficient = 0.83).\nWhy did agent 2’s rank not change in the EigenTrust experiment? During the initial cal-\n\nculation of its normalized local trust values (used for the calculation of the reputation\ngraph) for a given agent, Eigentrust essentially calculates the difference between positive\nand negative feedbacks for each neighbour and then it normalizes it over all its neigh-\nbours. In our case, agent 1 only has agent 2 as its neighbour and so the normalization\nprocess leads to a loss of information, namely the number of positive feedbacks.\nThus, if an agent has interacted with a malicious agent only, then the malicious agent\n\ncan get the victim to trust him fully, as long as the number of positive feedbacks that the\nmalicious agent received is greater than the number of negative feedbacks. This prob-\nlem is acknowledged by EigenTrust’s authors [1]. But PeerTrust does not suffer from this\nproblem, because it does not attempt to perform a sum of positive and negative feedbacks\nin this fashion.\n\nSelf-promoting attack\n\nSetup Suppose the least trustworthy agent rates a newcomer (i.e., one that had received\nno feedback yet) highly. If the newcomer becomes more trustworthy than before the\n\nTable 2 Rankings before and after slandering in retort (reputations in brackets)\n\nAgent 0 Agent 1 Agent 2 Agent 3\n\nEigenTrust(FHG0) 3 (0.16) 2 (0.29) 1 (0.39) 3 (0.16)\n\nEigenTrust(FHG1) 3 (0.16) 2 (0.29) 1 (0.39) 3 (0.16)\n\nPeerTrust(FHG0) 3 (0.11) 1 (0.44) 1 (0.44) 4 (0.00)\n\nPeerTrust(FHG1) 3 (0.13) 1 (0.52) 2 (0.35) 4 (0.00)\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 17 of 27\n\nFig. 7 Self-promoting attack – EigenTrust\n\nrating, then this can be exploited maliciously, paving the way to collusive self-promoting\nattacks [12]. Conversely, if the newcomer becomes less trustworthy than before, then the\nnewcomer is vulnerable to a slandering attack. Consider FHG1 in Fig. 7(b) which is based\non FHG0 in Fig. 7(a), after agent 3 rates agent 0 highly. The experiment set-up is therefore\nas follows:\n\nspearman\n(\nf 2reigentrust\n\n(\nf 2fdiscretize(FHG0)\n\n)\n, f 2reigentrust\n\n(\nf 2fdiscretize(FHG1)\n\n))\nand the output can be seen in Fig. 7(c).\n\nResults Looking at Table 3, we see that agent 0’s reputation relative to agent 3 increased.\nNote that it is possible that agent 3 genuinely rated agent 0 highly, in which case Eigen-\nTrust’s output is a good assessment, otherwise it might have missed a case of collusive\nself-promoting attack. It is also worth noting that this experiment breaks PeerTrust, due\nto a division-by-0 problem in an equation. Indeed, when calculating agent 0’s reputa-\ntion score, the feedbacks provided by agent 3 and its reputation score must be taken into\naccount. In this case, agent 3’s reputation score is 0, due to the fact that it received only\n0-valued feedbacks. The division by agent 3’s reputation is the problem here.\n\nSlandering attack, scenario 1\n\nSetup When the least trustworthy agent (agent 3) gives a newcomer (agent 0) a very\nlow rating, it can either be a genuine rating or a slandering attempt [12]. The reputation\nalgorithm may resist slandering by not decreasing agent 0’s reputation, but the risk or\ntradeoff would be the possibility of ignoring a genuine assessment. The inputs for this\nexperiment are FHG0 and FHG1 as shown in Fig. 8, and the experiment is again therefore\nspecified as follows:\n\nspearman\n(\nf 2reigentrust\n\n(\nf 2fdiscretize(FHG0)\n\n)\n, f 2reigentrust\n\n(\nf 2fdiscretize(FHG1)\n\n))\n\nTable 3 Ranking before and after low feedback to newcomer (reputations in brackets)\n\nAgent 0 Agent 1 Agent 2 Agent 3\n\nEigenTrust(FHG0) 3 (0.16) 2 (0.29) 1 (0.39) 3 (0.16)\n\nEigenTrust(FHG1) 3 (0.22) 2 (0.3) 1 (0.36) 4 (0.12)\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 18 of 27\n\nFig. 8 Slandering attack scenario 1 – EigenTrust\n\nResults Comparing the rankings in Table 4, we observe that agent 0’s reputation and\nglobal rank has not changed. If agent 3 has indeed provided a dishonest negative feedback,\nthen we can say that EigenTrust output is a correct assessment. However, it is also possible\nthat agent 3 is the one being slandered (by agent 2) and has provided a genuine negative\nfeedback, in which case, we would expect agent 0’s rank to decrease. Thus, we can only\nsay that EigenTrust is insensitive to bad feedbacks and therefore resists the slandering of\na newcomer. Note that again PeerTrust would produce invalid results for the same reason\nas previously.\n\nSlandering attack, scenario 2\n\nSetup Now going back to the initial situation (Fig. 9(a)), what happens if, as shown\nin FHG1 in Fig. 9(b), agent 3 decides to direct its bad feedbacks to agent 2\ninstead of agent0? Looking purely at the feedbacks, one can make the following\nguesses:\n\n• Agent 3 rated agent 2 negatively to cover its malicious acts (a slandering attack).\n• Agent 2 cheated agent 3 and thus obtained negative feedbacks but acted honestly\n\nwith agent 1 to keep its reputation high (a white-washing attack).\n\nHaving received more positive feedbacks than agent 3, if agent 2’s reputation is not\naffected, then the algorithm is said to be resistant to a slandering attack but it is vulnerable\n\nTable 4 Ranking before and after high feedback to newcomer (reputations in brackets)\n\nAgent 0 Agent 1 Agent 2 Agent 3\n\nEigenTrust(FHG0) 3 (0.16) 2 (0.29) 1 (0.39) 3 (0.16)\n\nEigenTrust(FHG1) 3 (0.16) 2 (0.29) 1 (0.39) 3 (0.16)\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 19 of 27\n\nFig. 9 Slandering attack scenario 2 – EigenTrust\n\nto a white-washing attack. Both EigenTrust and PeerTrust can be used to run this\nexperiment, which is specified as follows:\n\nspearman\n(\nf 2reigentrust\n\n(\nf 2fdiscretize(FHG0)\n\n)\n, f 2reigentrust\n\n(\nf 2fdiscretize(FHG1)\n\n))\nspearman\n\n(\nf 2rpeertrust(FHG0), f 2rpeertrust(FHG1)\n\n)\nspearman\n\n(\nf 2reigentrust\n\n(\nf 2fdiscretize(FHG1)\n\n)\n, r2rnormalize\n\n(\nf 2rpeertrust(FHG1)\n\n))\n\nResults The rankings in Table 5 show that both EigenTrust and PeerTrust are resis-\ntant to this kind of slandering attack, but, as mentioned earlier, they are vulnerable to\nwhite-washing attacks (because the negative feedbacks by agent 3 did not lower agent 2’s\nranking).\n\nSlandering + Sybil attack for local trust algorithms\n\nSetup We now turn to finding out how many malicious agents it takes to slander\neffectively. In a Sybil attack, a malicious agent introduces a number of Sybils (i.e, accom-\nplices or pseudonyms), whose purpose is to slander a victim in the system [12]. Suppose\nwe are given the reputation graph shown in Fig. 10. Let agent 0 be a malicious agent that\nslanders agent 1. Assuming this agent has unlimited resources (e.g., an unlimited and\n\nTable 5 Rankings before and after slandering in retort (reputations in brackets)\n\nAgent 0 Agent 1 Agent 2 Agent 3\n\nEigentrust(FHG0) 3 (0.16) 2 (0.29) 1 (0.39) 3 (0.16)\n\nEigentrust(FHG1) 3 (0.16) 2 (0.29) 1 (0.39) 3 (0.16)\n\nPeerTrust(FHG0) 3 (0.25) 1 (1.00) 1 (1.00) 4 (0.00)\n\nPeerTrust(FHG1) 3 (0.11) 1 (0.44) 1 (0.44) 4 (0.00)\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 20 of 27\n\nFig. 10 Slandering attack using a feedback history graph\n\nlow-cost ability to create pseudonyms), it may introduce an unlimited number of Sybils\nto collectively slander agent 1. In such a scenario, it is useful to study how r(0, 1), i.e. the\nreputation of Agent 1 from the point of view of Agent 0, changes with respect to other\nagents. If r(0, 1) changes such that it is less than r(0, 2), then we can conclude that the slan-\ndering attack was successful and we would measure the effectiveness of the attack based\non the number of Sybils required. However, as explained in Section ‘Slandering attack,\nscenario 2’, one can also interpret this scenario as a white-washing attack. Thus, if an\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 21 of 27\n\nFig. 11 FHG after adding 10 sybils and slander edges\n\nalgorithm is resistant to a slandering attack, then it may be vulnerable to white-washing\nattacks.\nTo measure Appleseed’s attack resistance with the graph in Fig. 10, we first verify\n\nwhether the victim’s reputation is less than that of an agent in the attacker’s possession\n(in this case, we wish to check if r(0, 1) < r(0, 2)). If it is false, the result is updated and a\nSybil agent x is created. Edges (2, x, 1.0) and (x, 1, 0) are also added to the FHG and Apple-\nseed is run again (AppleSeed normally needs a reputation graph as input, but in this case\nsince each FHG edge has a singleton feedback, the value of the feedback can be directly\nused as reputation). This process continues and Sybils are added for a certain number of\niterations or until the attack succeeds (Fig. 11).\n\nResults Appleseed resists this type of attack well. Table 6 summarizes the values of\nr(0, 1) and r(0, 2) in RG after adding 1, 10, 50 and 100 Sybils and slander edges to\nthe FHG. We observe that r(0, 1) > r(0, 2). This confirms the attack resistance prop-\nerty of Appleseed described in [3]. This is not the case of global trust algorithms\nsuch as EigenTrust, since the addition of sybils simply dilutes the reputation values of\nagents.\n\nTrust properties assessments\n\nIn this section, trust algorithms are evaluted for their adherence to trust proper-\nties, namely: (a) weak transitivity and (b) the fact that trust is more easily lost than\ngained.\n\nWeak transitivity\n\nSetup Trustworthiness obtained via the transitive closure of trust paths should decrease\nas the length of the trust path increases [9, 25]. We subjected Appleseed and EigenTrust\nto simple tests to verify this basic rule. Even though the input and output graphs are of\ndifferent types for these two algorithms, the same assertions can be applied to test the\n\nTable 6 Reputation scores by Appleseed\n\nNo. of slandering edges added r(0, 1) r(0, 2)\n\n1 0.36 0.15\n\n10 0.34 0.15\n\n50 0.34 0.14\n\n100 0.34 0.14\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 22 of 27\n\ntransitive rules, as they do not depend on the semantics of the weight of edges or whether\nthe reputation scores are local or global.\n\nResults Table 7 presents the test cases (the \"Input FHG/RG\" column), expected results\n(the \"Output RG\" column represents the transitive closure resulting from the graph trans-\nformation, along with question marks on the edges where the weak transitivity property\nis tested) and actual results. Both Appleseed and EigenTrust passed our transitive tests.\n\nDynamic evolution of global trust\n\nAccording to Marsh [13], “When considering a simple trusting agent (i.e., one who does\nuse rules of reciprocation), the trust he has in a trustee will ordinarily increase if coop-\neration occurs, and decrease otherwise”. That is, if trust exists between two parties, then\ncooperation between them reinforces trust by increasing some trust score. If there was\nno trust, then cooperation builds trust. Furthermore, “a sudden defection from a trusted\nfriend can result in a drastic reduction of trust, to the extent that a lot of work is neces-\nsary to build that trust up again” [13]. That is, if cooperation does not ensue, then trust is\nlost at a rate higher that it was gained. Adherence to this property prevents white-washing\natttacks where attackers cheat periodically while maintaining a high reputation.\nWe propose the following method to evaluate this property.\nLet fi(∗, b) be the i’th feedback on b in feedback history FHG(∗, b), which is the list of\n\nfeedbacks on b by all agents in the system and ri(b) be b’s global reputation score following\ni’th feedback.\n\nTable 7 Transitivity tests and results\n\nExp Input FHG/RG Output RG Expectation Results\n\nEigenTrust Appleseed\n\n1 r(0, 1) >= r(0, 2)\n\nr(0, 2) == r(0, 3) true true\n\n2 r(0, 5) >= r(0, 2)\n\nr(0, 5) >= r(0, 3) true true\n\n3 r(0, 3) >= r(0, 2)\n\ntrue true\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 23 of 27\n\nAssuming both reputation scores and feedback values are on the same scale and r(b)\nis a function of the feedback history FHG(∗, b), then trust must evolve according to con-\nditions 5.1 and 5.2, where we define, for agent b and for any consecutive “instants” i − 1\nand i:\n\n• δf := fi(∗, b) − ri−1(b) (the ith feedback is compared to the reputation at instant i-1)\n• δr := ri(b) − ri−1(b) where ri(b) is obtained after feedback fi(∗, b)\n• δr\n\nδf\n+ := δr\n\nδf if\nδr\nδf ≥ 0, and δr\n\nδf\n− otherwise\n\n0 ≤ δr\nδf\n\n+\n≤ 1 (5.1)\n\n| δr\nδf\n\n+\n| ≤ | δr\n\nδf\n\n−\n| (5.2)\n\nThat is:\n\n1. Condition 5.1 verifies that if there is a positive change in feedback, then the change\nin reputation is also positive, but the change in reputation is less than the change in\nfeedback.\n\n2. Condition 5.2 verifies that the magnitude of the rate of change in reputation due to\npositive feedbacks is always less than the magnitude of the rate of change in\nreputation due to negative feedbacks.\n\nSetup and results We set up two scenarios to investigate the behaviour of PeerTrust. In\nthe first scenario, there are only two agents a and b, and the feedback history FHG(a, b) is\nhandcrafted such that a is typically satisfied with b, except once, where f (a, b) is set to 0.\nIn this scenario, we determine whether trust loss is greater than trust gain. In the second\nscenario, we introduce a third agent c and we investigate the impact of r(c, a) on r(a, b)\nand whether the above conditions are still held in this scenario.\n\nScenario 1: evolution due to direct interactions only Consider the feedback histories\nbetween two agents a and b in Table 8, which shows agent a was dissatisfied with b only\nonce. One could argue that b mounted a white-washing attack in which it behaved hon-\nestly in order to cheat once in a while, or it is also possible that a is concealing a slandering\nattack, where it unfairly rated b. Even though information such as the intention of an agent\nthus cannot be extracted from the feedback history alone, we can use the above condi-\ntions to characterize a trust algorithm’s behaviour as to whether reputation evolves as it\nshould.\nAssuming that all agents are pre-trusted equally (i.e., r0(∗, b) = 0.5), Table 8 and\n\nFig. 12 show the evolution of r0(∗, b). When f3(a, b) (a negative feedback) occurred, r(∗, b)\n\nTable 8 Evolution due to direct interactions only—Scenario 1\n\ni Assessor Assessee f r δf δr\n\n1 a b 1.0 1.0 0.5 0.5\n\n2 a b 1.0 1.0 0 0\n\n3 a b 0 0.66 −1.0 −0.33\n\n4 a b 1.0 0.75 0.33 0.08\n\n5 a b 1.0 0.8 0.25 0.05\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 24 of 27\n\nFig. 12 Evolution of reputation w.r.t feedback – Scenario 1\n\ndecreased but it increased at a slower rate when f4(a, b) (a positive feedback) occured.\nThus, PeerTrust has respected conditions 5.1 and 5.2.\nIn a separate experiment, we also noted that if there were two consecutive negative\n\nfeedbacks on b, then r(∗, b) decreased to 0.5. This suggests that an attacker can know\nexactly how many positive feedbacks are required in order to restore his lost reputation\nafter misbehaving.\n\nScenario 2: evolution due to direct and indirect interactions In this experiment, we use\nthe feedback history provided in Table 9. Initially, agent c provided a positive feedback to\na (f1), but later provides a negative feedback (f4). Because b’s reputation score is dependent\non a’s reputation score, a negative change in a’s reputation should affect b’s reputation.\nHowever, results show that it is unaffected. We explain the reason in what follows.\nIf we compare the reputation values shown in Table 10, we note that r(a) changed from\n\n1 to 0.5. However, this change did not affect r(b) after adding f5. This is because of the\nnormalization technique used by PeerTrust to calculate b’s reputation.\nThus, PeerTrust did not propagate trust as one would have expected (i.e., if c does not\n\ntrust a, it should not trust b either) and this can be easily exploited by attackers. For\nexample, if agents a and b both are part of a collusive group, then agent c cannot recognize\nthis and therefore cannot break away from a collusive group of attackers.\n\nTable 9 Evolution due to direct and indirect interactions\n\ni Assessor Assessee f r\n\n1 c a 1.0 0\n\n2 a b 0.9 0.9\n\n3 a b 0.9 0.9\n\n4 c a 0 0.9\n\n5 a b 1.0 0.93\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 25 of 27\n\nTable 10 Reputation output by PeerTrust\n\ni Assessor Assessee f r(∗, b)\n1 c a 1.0 0\n\n2 a b 0.9 0.9\n\n3 a b 0.9 0.9\n\n4 c a 0 0.9\n\n5 a b 1.0 0.93\n\nLimitations\n\n1. By definition, the global trust of an agent reflects the impact of feedbacks by all\nother agents in the system. To verify the conditions presented at the beginning of\nthis section, we only need to know the order of feedbacks for an agent, regardless of\nthe assessor. However, verifying the evolution of local trust scores according to the\nabove conditions is subjective. Suppose we obtained Table 11 from a local trust\nalgorithm. If we only consider direct experience between a and b, then we observe\nthat r(a, b) increased from 0.1 to 0.3 despite a negative feedback (f6), but this\nincrease may have been contributed by f5 and the fact that r(a, c) = 0.9. That is,\nr(a, b) is an aggregated score obtained through direct and indirect experiences and,\ndepending on the algorithm, different weights may be given to direct versus\nindirect experience [26].\n\nConclusions\nWe provided details of our prototype and evaluated trust algorithms for vulnerabili-\nties to attacks and adherence to trust properties. We were able to show that our model\ncould indeed accommodate various algorithms, and that the specification of experiments\nin terms of the model was straightforward and easy to express programmatically. The\nexperiments that we ran allowed us to make the following observations:\n\n• We were able to exhibit discrepancies between the outputs of EigenTrust and\nPeerTrust due to the way the initial normalized local trust values are calculated in\nEigenTrust; EigenTrust ignores the excess positive feedbacks once they outnumber\nnegative feedbacks;\n\n• There are tradeoffs between sensitivity to self-promotion and sensitivity to\nslandering. For example, an algorithm that is overly sensitive to slandering might take\ntoo long to incorporate bad feedback, and this allows bad behavior to go unnoticed\nfor a while. In general, the difficulty is that one cannot always detect an attack just by\nlooking at feedback history, since multiple interpretations are always possible.\n\nTable 11 Local reputation example\n\ni Assessor Assessee f r(a, b) r(a, c) r(c, b)\n\n1 a c 0.9 - 0.9 -\n\n2 a b 0.6 0.6 0.9 -\n\n3 a b 0.7 0.65 0.9 -\n\n4 a b 0.2 0.1 0.9 -\n\n5 c b 0.9 - 0.9 0.4\n\n6 a b 0 0.3 0.9 0.4\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 26 of 27\n\nClarification may come with the accumulation of feedback data, but multi-agent\nsystems might not be able to survive to see that accumulation, unless a certain\nnumber of pre-trusted nodes can be provided to bootstrap the system.\n\n• We were able to express and verify basic trust properties, such as weak transitivity\nand the fact that trust is more easily lost than gained. These should be building blocks\nupon which more complex properties could be expressed.\n\nWe have identified the following limitations and future work:\n\n• Since the feedback history graph limits itself to agent-to-agent transaction ratings,\nrecommender systems such as Credence [10] that use agent-to-object ratings cannot\nbe included in the testbed.\n\n• Trust systems that rely on different agent types (advisors, trusters, trustees, etc.) such\nas in [17] cannot be accommodated in our model.\n\n• Our framework cannot accommodate trust algorithms, such as REGRET [27] or FIRE\n[28], that use inputs other than feedback history.\n\n• Different agents might have different reputations in different contexts. Our testbed\ndoes not explicitly represent this context. One would have to create separate graphs\nfor each context (thus making the assumption that the contexts are independent\nfrom one another), and this might be an oversimplification in cases where trust in\none context partially influences trust in another context. We note however that in\nmost of the trust algorithms we have classified the notion of context is not explicitly\ntaken into account either.\n\n• Agent behavior simulation: it would be desirable to use agent simulations to\nautomatically generate feedback history graphs (stage 1 of the workflow). Such a\nfeature would be useful for designing large-scale experiments with each agent acting\nin a different manner.\n\n• Support for distrust: distrust indicates how much an agent is not trusted (opposite of\ntrustworthiness) and algorithms such as the distrust-aware version of Appleseed\ncompute both trust and distrust propagation. Because our reputation graphs do not\ninclude distrust information, such algorithms cannot be evaluated using our testbed.\n\nIn addition to addressing the above limitations in our model, future work involves\nimplementing more trust algorithms, building a user interface for our testbed and\nperforming large-scale experiments. In particular, experiments using large datasets from\nwebsites such as Epinions, Advogato, and Facebook can yield interesting results.\n\nEndnotes\n1In the remainder of the paper we will simply refer to the trust systems or models that\n\nhave no specific name by the name of the first author of the paper we are citing, so [5]\nwill be referred to as “Aberer”.\n\n2in what follows we will use the graph name and the labelling functions\ninterchangeably to reduce extraneous notations\n\n3Note that in this figure, we have included “0” in A × A  → {0, 1} for a trust graph for\nclarity but in reality, it indicates that there is no edge.\n\n4http://code.google.com/p/repsystestbed/\n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nhttp://code.google.com/p/repsystestbed/\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 27 of 27\n\nAuthors’ contributions\nPC did the implementation and ran the experiments, and participated in the formalization, the state of the art work and\nthe write-up. BE participated in the formalization, the state of the art work and the write-up. Both authors read and\napproved the final manuscript.\n\nReceived: 24 November 2014 Accepted: 3 July 2015\n\nReferences\n1. Kamvar SD, Schlosser MT, Garcia-Molina H (2003) The eigentrust algorithm for reputation management in p2p\n\nnetworks. In: WWW ’03 Proceedings of the 12th International Conference on World Wide Web. ACM, Budapest,\nHungary. pp 640–651. http://doi.acm.org/10.1145/775152.775242\n\n2. Xiong L, Liu L (2004) Peertrust: Supporting reputation-based trust for peer-to-peer electronic communities. IEEE\nTrans Knowl Data Eng 16(7):843–857\n\n3. Ziegler CN (2005) Chap. On propagating interpersonal trust in social networks. In: Computing with Social Trust.\nSpringer, London. pp 133–166\n\n4. Zimmermann PR (1995) The Official PGP User’s Guide. MIT Press, Cambridge, MA, USA\n5. Aberer K, Despotovic Z (2001) Managing trust in a peer-2-peer information system. In: CIKM ’01 Proceedings of the\n\nTenth International Conference on Information and Knowledge Management. ACM, Atlanta, GA. pp 310–317\n6. Teacy WTL, Patel J, Jennings N, Luck M (2006) Travos: Trust and reputation in the context of inaccurate information\n\nsources. Autonomous Agents Multi-Agent Syst 12(2):183–198. doi:10.1007/s10458-006-5952-x\n7. Jøsang A, Ismail R (2002) The beta reputation system. In: Proceedings of the 15th Bled Electronic Commerce\n\nConference. pp 41–55\n8. Levien R (2009) Chap. Attack-resistant Trust Metrics. In: Computing with Social Trust. Springer, London. pp 121–132\n9. Golbeck JA (2005) Computing and applying trust in web-based social networks. PhD thesis, University of Maryland\n\nat College Park\n10. Walsh K, Sirer EG (2006) Experience with an object reputation system for peer-to-peer filesharing. In: NSDI’06\n\nProceedings of the 3rd Conference on Networked Systems Design and Implementation. Usenix, Berkeley, USA\n11. Jøsang A, Pope S (2005) Semantic constraints for trust transitivity. In: Proceedings of the 2nd Asia-Pacific Conference\n\non Conceptual Modelling - Volume 43. APCCM ’05, Australian Computer Society, Inc., Darlinghurst, Australia,\nAustralia. pp 59–68. http://dl.acm.org/citation.cfm?id=1082276.1082284\n\n12. Hoffman K, Zage D, Nita-Rotaru C (2009) A survey of attack and defense techniques for reputation systems. ACM\nComput Surv 42:1–1131\n\n13. Marsh SP (1994) Formalising trust as a computational concept. PhD thesis, University of Stirling\n14. Abdul-Rahman A, Hailes S (2000) Supporting trust in virtual communities. In: HICSS ’00: Proceedings of the 33rd\n\nHawaii International Conference on System Sciences-Volume 6. IEEE Computer Society, Washington, DC, USA. p 6007\n15. Guha R (2004) Open rating systems. In: Proceedings of the 1st Workshop on Friends of a Friend, Social Networking\n\nand the Semantic Web. FOAF Galway, Galway, Ireland. http://www.w3.org/2001/sw/Europe/events/foaf-galway/\npapers/fp/open_rating_systems/wot.pdf. Accessed 31-Aug-2012\n\n16. Hazard CJ, Singh MP (September August 3) Macau: A basis for evaluating reputation systems. In: Rossi F (ed). IJCAI\n2013, Proceedings of the 23rd International Joint Conference on Artificial Intelligence, Beijing, China. AAAI\nPublications, Palo Alto, CA, USA. IJCAI/AAAI. http://www.aaai.org/ocs/index.php/IJCAI/IJCAI13/paper/view/6854\n\n17. Fullam K, Klos TB, Muller G, Sabater J, Schlosser A, Topol Z, Barber KS, Rosenschein JS, Vercouter L, Voss M (Unknown\nMonth July 25) A specification of the agent reputation and trust (ART) testbed: experimentation and competition for\ntrust in agent societies. In: Dignum F, Dignum V, Koenig S, Kraus S, Singh MP, Wooldridge M (eds). 4th International\nJoint Conference on Autonomous Agents and Multiagent Systems (AAMAS 2005). ACM, Utrecht, The Netherlands.\npp 512–518. doi:10.1145/1082473.1082551, http://doi.acm.org/10.1145/1082473.1082551\n\n18. Kerr R, Cohen R (2009) Smart cheaters do prosper: defeating trust and reputation systems. In: Proceedings of The 8th\nInternational Conference on Autonomous Agents and Multiagent Systems - Volume 2. AAMAS ’09. International\nFoundation for Autonomous Agents and Multiagent Systems, Richland, SC. pp 993–1000. http://portal.acm.org/\ncitation.cfm?id=1558109.1558151\n\n19. Thirunarayan K, Anantharam P, Henson C, Sheth A (2014) Comparative trust management with applications:\nBayesian approaches emphasis. Future Generation Comput Syst 31:182–199\n\n20. Yann Krupa JFH, Vercouter L (2009) Extending the Comparison Efficiency of the ART Testbed. In: Paolucci M (ed).\nProceedings of the First International Conference on Reputation: Theory and Technology - ICORE 09, Gargonza, Italy\n\n21. Kerr R, Cohen R (2010) Treet: the trust and reputation experimentation and evaluation testbed. Electron Commer\nRes 10:271–290\n\n22. O’Hara K (2012) A general definition of trust. http://eprints.soton.ac.uk/273193/\n23. Ceolin D, Nottamkandath A, Fokkink W (2014) Efficient semi-automated assessment of annotations trustworthiness.\n\nJ Trust Manag 1(1):1–31\n24. Myers JL, Well AD (2003) Research Design and Statistical Analysis. Lawrence Erlbaum Associates, New Jersey\n25. Jøsang A Trust and reputation systems. In: Aldini A, Gorrieri R (eds). Foundations of Security Analysis and Design IV,\n\nFOSAD 2006/2007 Tutorial Lectures. Lecture Notes in Computer Science. Springer, Berlin Vol. 4677. pp 209–245.\nhttp://dx.doi.org/10.1007/978-3-540-74810-6_8\n\n26. Guha R, Kumar R, Raghavan P, Tomkins A (2004) Propagation of trust and distrust. In: Proceedings of the 13th\nInternational Conference on World Wide Web, WWW ’04. ACM, New York, NY, USA. pp 403–412.\ndoi:10.1145/988672.988727, http://doi.acm.org/10.1145/988672.988727\n\n27. Sabater J, Sierra C (2001) Regret: A reputation model for gregarious societies. In: Fourth Workshop on Deception\nFraud and Trust in Agent Societies Vol. 70\n\n28. Dong-Huynha T, Jennings N, Shadbolt N (2004) Fire: An integrated trust and reputation model for open multi-agent\nsystems. In: 16th European Conference on Artificial Intelligence, Valencia, Spain. IOS Press, Amsterdam. pp 18–22\n\nhttp://doi.acm.org/10.1145/775152.775242\nhttp://dx.doi.org/10.1007/s10458-006-5952-x\nhttp://dl.acm.org/citation.cfm?id=1082276.1082284\nhttp://www.w3.org/2001/sw/Europe/events/foaf-galway/papers/fp/open_rating_systems/wot.pdf\nhttp://www.w3.org/2001/sw/Europe/events/foaf-galway/papers/fp/open_rating_systems/wot.pdf\nhttp://www.aaai.org/ocs/index.php/IJCAI/IJCAI13/paper/view/6854\nhttp://dx.doi.org/10.1145/1082473.1082551\nhttp://doi.acm.org/10.1145/1082473.1082551\nhttp://portal.acm.org/citation.cfm?id=1558109.1558151\nhttp://portal.acm.org/citation.cfm?id=1558109.1558151\nhttp://eprints.soton.ac.uk/273193/\nhttp://dx.doi.org/10.1007/978-3-540-74810-6_8\nhttp://dx.doi.org/10.1145/988672.988727\nhttp://doi.acm.org/10.1145/988672.988727\n\n\tAbstract\n\tKeywords\n\n\tIntroduction\n\tMotivation\n\tOverview of our solution and contributions\n\tOrganization\n\n\tBackground and literature review\n\tSocial trust models\n\tNature of input\n\tDirect vs. indirect trust\n\tGlobal vs. local trust\n\tTo trust or not to trust\n\tPeerTrust\n\tEigenTrust\n\tAppleseed\n\n\tTestbeds\n\tGuha\n\tMacau\n\tART\n\tTREET\n\n\tSummary\n\n\tProblem description and model\n\tStage 1—obtain feedback history graph\n\tStage 2—obtain reputation graph\n\tStage 3—obtain trust graph\n\n\tClassifying and chaining algorithms\n\tResults and discussion\n\tImplementation\n\tVulnerability assessments\n\tNormalization-based attack\n\tSetup\n\tResults\n\n\tSelf-promoting attack\n\tSetup\n\tResults\n\n\tSlandering attack, scenario 1\n\tSetup\n\tResults\n\n\tSlandering attack, scenario 2\n\tSetup\n\tResults\n\n\tSlandering + Sybil attack for local trust algorithms\n\tSetup\n\tResults\n\n\n\tTrust properties assessments\n\tWeak transitivity\n\tSetup\n\tResults\n\n\tDynamic evolution of global trust\n\tSetup and results\n\tScenario 1: evolution due to direct interactions only\n\tScenario 2: evolution due to direct and indirect interactions\n\n\tLimitations\n\n\n\n\tConclusions\n\tEndnotes\n\tCompeting interests\n\tAuthors' contributions\n\tReferences\n\n\n\n",
      "metadata_author": "Partheeban Chandrasekaran",
      "metadata_title": "Toward a testbed for evaluating computational trust models: experiments and analysis",
      "metadata_creation_date": "2015-09-04T09:59:41Z"
    },
    {
      "@search.score": 0.2665392,
      "content": "\nBig data stream analysis: a systematic \nliterature review\nTaiwo Kolajo1,2* , Olawande Daramola3  and Ayodele Adebiyi1,4 \n\nIntroduction\nAdvances in information technology have facilitated large volume, high-velocity of data, \nand the ability to store data continuously leading to several computational challenges. \nDue to the nature of big data in terms of volume, velocity, variety, variability, veracity, \nvolatility, and value [1] that are being generated recently, big data computing is a new \ntrend for future computing.\n\nBig data computing can be generally categorized into two types based on the process-\ning requirements, which are big data batch computing and big data stream computing \n\nAbstract \n\nRecently, big data streams have become ubiquitous due to the fact that a number of \napplications generate a huge amount of data at a great velocity. This made it difficult \nfor existing data mining tools, technologies, methods, and techniques to be applied \ndirectly on big data streams due to the inherent dynamic characteristics of big data. In \nthis paper, a systematic review of big data streams analysis which employed a rigorous \nand methodical approach to look at the trends of big data stream tools and technolo-\ngies as well as methods and techniques employed in analysing big data streams. It \nprovides a global view of big data stream tools and technologies and its comparisons. \nThree major databases, Scopus, ScienceDirect and EBSCO, which indexes journals and \nconferences that are promoted by entities such as IEEE, ACM, SpringerLink, and Elsevier \nwere explored as data sources. Out of the initial 2295 papers that resulted from the \nfirst search string, 47 papers were found to be relevant to our research questions after \nimplementing the inclusion and exclusion criteria. The study found that scalability, \nprivacy and load balancing issues as well as empirical analysis of big data streams and \ntechnologies are still open for further research efforts. We also found that although, sig-\nnificant research efforts have been directed to real-time analysis of big data stream not \nmuch attention has been given to the preprocessing stage of big data streams. Only a \nfew big data streaming tools and technologies can do all of the batch, streaming, and \niterative jobs; there seems to be no big data tool and technology that offers all the key \nfeatures required for now and standard benchmark dataset for big data streaming ana-\nlytics has not been widely adopted. In conclusion, it was recommended that research \nefforts should be geared towards developing scalable frameworks and algorithms that \nwill accommodate data stream computing mode, effective resource allocation strategy \nand parallelization issues to cope with the ever-growing size and complexity of data.\n\nKeywords: Big data stream analysis, Stream computing, Big data streaming tools and \ntechnologies\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\nKolajo et al. J Big Data            (2019) 6:47  \nhttps://doi.org/10.1186/s40537-019-0210-7\n\n*Correspondence:   \ntaiwo.kolajo@stu.cu.edu.ng; \ntaiwo.kolajo@fulokoja.edu.ng \n1 Department of Computer \nand Information Sciences, \nCovenant University, Ota, \nNigeria\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0001-6780-2495\nhttp://orcid.org/0000-0001-6340-078X\nhttp://orcid.org/0000-0002-3114-6315\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0210-7&domain=pdf\n\n\nPage 2 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\n[2]. Big data batch processing is not sufficient when it comes to analysing real-time \napplication scenarios. Most of the data generated in a real-time data stream need real-\ntime data analysis. In addition, the output must be generated with low-latency and any \nincoming data must be reflected in the newly generated output within seconds. This \nnecessitates big data stream analysis [3].\n\nThe demand for stream processing is increasing. The reason being not only that huge \nvolume of data need to be processed but that data must be speedily processed so that \norganisations or businesses can react to changing conditions in real-time.\n\nThis paper presents a systematic review of big data stream analysis. The purpose is to \npresent an overview of research works, findings, as well as implications for research and \npractice. This is necessary to (1) provide an update about the state of research, (2) iden-\ntify areas that are well researched, (3) showcase areas that are lacking and need further \nresearch, and (4) build a common understanding of the challenges that exist for the ben-\nefit of the scientific community.\n\nThe rest of the paper is organized as follows: “Background and related work” section \nprovides information on stream computing and big data stream analysis and the key \nissues involved in it and presents a review on big data streaming analytics. In “Research \nmethod” section, the adopted research methodology is discussed, while “Result” section \npresents the findings of the study. “Discussion” section presents a detailed evaluation \nperformed on big data stream analysis, “Limitation of the review” section highlights the \nlimitations of the study, while “Conclusion and further work” concludes the paper.\n\nBackground and related work\nStream computing\n\nStream computing refers to the processing of massive amount of data generated at high-\nvelocity from multiple sources with low latency in real-time. It is a new paradigm neces-\nsitated because of new sources of data generating scenarios which include ubiquity of \nlocation services, mobile devices, and sensor pervasiveness [4]. It can be applied to the \nhigh-velocity flow of data from real-time sources such as the Internet of Things, Sensors, \nmarket data, mobile, and clickstream.\n\nThe fundamental assumption of this paradigm is that the potential value of data lies in \nits freshness. As a result, data are analysed as soon as they arrive in a stream to produce \nresult as opposed to what obtains in batch computing where data are first stored before \nthey are analysed. There is a crucial need for parallel architectures and scalable com-\nputing platforms [5]. With stream computing, organisations can analyse and respond in \nreal-time to rapidly changing data. Streaming processing frameworks include Storm, S4, \nKafka, and Spark [6–8]. The real contrasts between the batch processing and the stream \nprocessing paradigms are outlined in Table 1.\n\nIncorporating streaming data into decision-making process necessitates a program-\nming paradigm called stream computing. With stream computing, fairly static questions \ncan be evaluated on data in motion (i.e. real-time data) continuously [9].\n\nBig data stream analysis\n\nThe essence of big data streaming analytics is the need to analyse and respond to real-\ntime streaming data using continuous queries so that it is possible to continuously \n\n\n\nPage 3 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nperform analysis on the fly within the stream. Stream processing solutions must be \nable to handle a real-time, high volume of data from diverse sources putting into con-\nsideration availability, scalability and fault tolerance. Big data stream analysis involves \nassimilation of data as an infinite tuple, analysis and production of actionable results \nusually in a form of stream [10].\n\nIn a stream processor, applications are represented as data flow graph made up of \noperations and interconnected streams as depicted in Fig. 1. In a streaming analytics \nsystem, application comes in a form of continuous queries, data are ingested continu-\nously, analysed and correlated, and stream of results are generated. Streaming analytic \napplications is usually a set of operators connected by streams. Streaming analytics \nsystems must be able to identify new information, incrementally build models and \naccess whether the new incoming data deviate from model predictions [9].\n\nThe idea of streaming analytics is that each of the received data tuples is processed \nin the data processing node. Such processing includes removing duplicates, filling \nmissing data, data normalization, parsing, feature extraction, which are typically done \nin a single pass due to the high data rates of external feeds. When a new tuple arrives, \nthis node is triggered, and it expels tuples older than the time specified in the sliding \nwindow (sliding window is a typical example of windows used in stream computing \nwhich keeps only the latest tuples up to the time specified in the windows). A window \n\nTable 1 Comparison between batch processing and streaming processing [82]\n\nDimension Batch processing Streaming processing\n\nInput Data chunks Stream of new data or updates\n\nData size Known and finite Infinite or unknown in advance\n\nHardware Multiple CPUs Typical single limited amount of memory\n\nStorage Store Not store or store non-trivial portion in memory\n\nProcessing Processed in multiple rounds A single or few passes over data\n\nTime Much longer A few seconds or even milliseconds\n\nApplications Widely adopted in almost every domain Web mining, traffic monitoring, sensor networks\n\nFig. 1 Data flow graph of a stream processor. The figure shows how applications (made up of operations and \ninterconnected streams) are represented as data flow graph in a stream processor [10]\n\n\n\nPage 4 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nis referred to as a logical container for data tuples received. It defines how frequently \ndata is refreshed in the container as well as when data processing is triggered [4].\n\nKey issues in big data stream analysis\n\nBig data stream analysis is relevant when there is a need to obtain useful knowledge \nfrom current happenings in an efficient and speedy manner in order to enable organisa-\ntions to quickly react to problems, or detect new trends which can help improve their \nperformance. However, there are some challenges such as scalability, integration, fault-\ntolerance, timeliness, consistency, heterogeneity and incompleteness, load balancing, \nprivacy issues, and accuracy [3, 11–18] which arises from the nature of big data streams \nthat must be dealt with.\n\nScalability\n\nOne of the main challenges in big data streaming analysis is the issue of scalability. The \nbig data stream is experiencing exponential growth in a way much faster than computer \nresources. The processors follow Moore’s law, but the size of data is exploding. There-\nfore, research efforts should be geared towards developing scalable frameworks and \nalgorithms that will accommodate data stream computing mode, effective resource allo-\ncation strategy and parallelization issues to cope with the ever-growing size and com-\nplexity of data.\n\nIntegration\n\nBuilding a distributed system where each node has a view of the data flow, that is, every \nnode performing analysis with a small number of sources, then aggregating these views \nto build a global view is non-trivial. An integration technique should be designed to ena-\nble efficient operations across different datasets.\n\nFault‑tolerance\n\nHigh fault-tolerance is required in life-critical systems. As data is real-time and infinite \nin big data stream computing environments, a good scalable high fault-tolerance strat-\negy is required that allows an application to continue working despite component failure \nwithout interruption.\n\nTimeliness\n\nTime is of the essence for time-sensitive processes such as mitigating security threats, \nthwarting fraud, or responding to a natural disaster. There is a need for scalable architec-\ntures or platforms that will enable continuous processing of data streams which can be \nused to maximize the timeliness of data. The main challenge is implementing a distrib-\nuted architecture that will aggregate local views of data into global view with minimal \nlatency between communicating nodes.\n\nConsistency\n\nAchieving high consistency (i.e. stability) in big data stream computing environments is \nnon-trivial as it is difficult to determine which data are needed and which nodes should \nbe consistent. Hence a good system structure is required.\n\n\n\nPage 5 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nHeterogeneity and incompleteness\n\nBig data streams are heterogeneous in structure, organisations, semantics, accessi-\nbility and granularity. The challenge here is how to handle an always ever-increas-\ning data, extract meaningful content out of it, aggregate and correlate streaming \ndata from multiple sources in real-time. A competent data presentation should be \ndesigned to reflect the structure, diversity and hierarchy of the streaming data.\n\nLoad balancing\n\nA big data stream computing system is expected to be self-adaptive to data streams \nchanges and avoid load shedding. This is challenging as dedicating resources to cover \npeak loads 24/7 is impossible and load shedding is not feasible when the variance \nbetween the average load and the peak load is high. As a result, a distributing envi-\nronment that automatically streams partial data streams to a global centre when local \nresources become insufficient is required.\n\nHigh throughput\n\nDecision with respect to identifying the sub-graph that needs replication, how many \nreplicas are needed and the portion of the data stream to assign to each replica is an \nissue in big data stream computing environment. There is a need for good multiple \ninstances replication if high throughput is to be achieved.\n\nPrivacy\n\nBig data stream analytics created opportunities for analyzing a huge amount of data \nin real-time but also created a big threat to individual privacy. According to the Inter-\nnational Data Cooperation (IDC), not more than half of the entire information that \nneeds protection is effectively protected. The main challenge is proposing techniques \nfor protecting a big data stream dataset before its analysis.\n\nAccuracy\n\nOne of the main objectives of big data stream analysis is to develop effective tech-\nniques that can accurately predict future observations. However, as a result of inher-\nent characteristics of big data such as volume, velocity, variety, variability, veracity, \nvolatility, and value, big data analysis strongly constrain processing algorithms spatio-\ntemporally and hence stream-specific requirements must be taken into consideration \nto ensure high accuracy.\n\nRelated work\n\nThis section discusses some of the previous research efforts that relate to big data \nstreaming analytics.\n\nThe work of [13] presented a review of various tools, technologies and methods \nfor big data analytics by categorizing big data analytics literature according to their \nresearch focus. This paper is different in that it presents a systematic literature review \nthat focused on big data “streaming” analytics.\n\n\n\nPage 6 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nAuthors in [19] presented a systematic review of big data analytics in e-commerce. The \nstudy explored characteristics, definitions, business values, types and challenges of big \ndata analytics in the e-commerce landscape. Likewise, [20] conducted a study that is cen-\ntred on big data analytics in technology and organisational resource management specifi-\ncally focusing on reviews that present big data challenges and big data analytics methods. \nAlthough they are systematic reviews, the focus is not, particularly on big data streaming.\n\nAuthors in [21] presented the status of empirical research and application areas in big \ndata by employing a systematic mapping method. In the same vein, authors in [22] also \nconducted a survey on big data technologies and machine learning algorithms with a \nparticular focus on anomaly detection. A systematic review of literature which aims to \ndetermine the scope, application, and challenges of big data analytics in healthcare was \npresented by [23]. The work of [2] presented a review of four big data streaming tools \nand technologies. While the study conducted in this paper provided a comprehensive \nreview of not only big data streaming tools and technologies but also methods and tech-\nniques employed in analyzing big data streams. In addition, authors [2] did not provide a \nclear explanation of the methodical approach for selecting the reviewed papers.\n\nResearch method\nThe study was grounded in a systematic literature review of tools and technologies \nwith methods and techniques used in analysing big data streams by adopting [24, 25] as \nmodels.\n\nResearch question\n\nThe study tries to answer the following research questions:\n\nResearch Question 1: What are the tools and technologies employed for big data \nstream analysis?\nResearch Question 2: What methods and techniques are used in analysing big data \nstreams?\nResearch Question 3: What do these tools and technologies have in common and \ntheir differences in terms of concept, purpose and capabilities?\nResearch Question 4: What are the limitations and strengths of these tools and tech-\nnologies?\nResearch Question 5: What are the evaluation techniques or benchmarks used for \nevaluating big data streaming tools and technology?\n\nSearch string\n\nCreating a good search string requires structuring in terms of population, compari-\nson, intervention and outcome [24]. Relevant publications were identified by forming \na search string that combined keywords driven by the research questions earlier stated. \nThe searches were conducted by employing three standard database indexes, which are \nScopus, Science Direct and EBSCOhost. The search string is “big data stream analysis” \nOR “big data stream technologies” OR “big data stream framework” OR “big data stream \nalgorithms” OR “big data stream analysis tools” OR “big data stream processing” OR “big \n\n\n\nPage 7 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\ndata stream analysis reviews” OR “big data stream literature review” OR “big data stream \nanalytics”.\n\nData sources\n\nAs research becomes increasingly interdisciplinary, global and collaborative, it is expedi-\nent to select from rich and standard databases. The databases consulted are as follows:\n\n i. Scopus1: Scopus is a bibliographic database containing abstracts and citations for \nacademic journal articles launched in 2004. It covers nearly 36,377 titles from over \n11,678 publishers of which 34,346 are peer-reviewed journals, delivering a compre-\nhensive overview of the world’s research output in the scientific, technical, medi-\ncal, and social sciences (including arts and humanities). It is the largest abstract \nand citation database of peer-reviewed literature.\n\n ii. ScienceDirect2: ScienceDirect is Elsevier’s leading information solution for \nresearchers, students, teachers, information professionals and healthcare profes-\nsionals. It provides both subscription-based and open access-based to a large data-\nbase combining authoritative, full-text scientific, technical and health publications \nwith smart intuitive functionality. It covers over 14 million publications from over \n3800 journals and more than 35,000 books. The journals are grouped into four \ncategories: Life Sciences, Physical Sciences and Engineering, Health Sciences, and \nSocial Sciences and Humanities.\n\n iii. EBSCOhost3: EBSCOhost covers a wide range of bibliographic and full-text data-\nbases for researchers, providing electronic journal service available to both cor-\nporate and academic researchers. It has a total of 16,711 journals and magazine \nindexed and abstracted of which 14,914 are peer-reviewed; more than 900,000 \nhigh-quality e-books and titles and over 60,000 audiobooks from more than 1500 \nmajor academic publishers.\n\n iv. ResearchGate4: A free online professional network for scientists and researchers to \nask and answer questions, share papers and find collaborators. It covers over 100 \nmillion publications from over 11 million researchers. ResearchGate was used as \na secondary source where the authors could not access some papers due to lack of \nsubscription.\n\nData retrieval\n\nThe search was conducted in Scopus, ScienceDirect and EBSCOhost since most of \nthe high impact journals and conferences are indexed in these set of rich databases. \nBoolean ‘OR’ was used in combining the nine (9) search strings. A total of 2295 arti-\ncles from the three databases were retrieved as shown in Table 2.\n\n1 http://www.scopu s.com.\n2 http://www.scien cedir ect.com.\n3 https ://www.ebsco host.com.\n4 https ://www.resea archg ate.net.\n\nhttp://www.scopus.com\nhttp://www.sciencedirect.com\nhttps://www.ebscohost.com\nhttps://www.reseaarchgate.net\n\n\nPage 8 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nFurther refinement was performed by (i) limiting the search to journals and confer-\nence papers; (ii) selecting computer science and IT related as the subject domain; (iii) \nselecting ACM, IEEE, SpringerLink, Elsevier as sources; and year of publication to \nbetween 2004 and 2018. The year range was selected due to the fact that interest in \nbig data stream analysis actually started in 2004. At this stage, a total of 1989 papers \nwere excluded leaving a total of 315 papers (see Table  3). The result of the search \nstring was exported to PDF.\n\nBy going through the title of the papers, 111 seemingly relevant papers were extracted \nexcluding a total number of 213 that were not relevant at this stage (see Table 4).\n\nThe abstracts of 111 papers and introduction (for papers that the abstracts were not \nclear enough) were then read to have a quick overview of the paper and to ascertain \nwhether they are suitable or at variance with the research questions. The citations of \nthe papers were exported to Microsoft Excel for easy analysis. The papers were grouped \ninto three categories; “relevant”, “may be relevant” and “irrelevant”. The “relevant” papers \nwere marked with black colour, “may be relevant” and “irrelevant” with green and red \ncolours respectively. At the end of this stage, 45 papers were classified as “relevant”, 9 \npapers as “may be relevant” and 11 as “irrelevant”. Looking critically at the abstract again, \n18 papers were excluded by using the exclusion criteria leaving a total of 47 papers (see \nTable 5) which were manually reviewed in line with the research questions.\n\nInclusion criteria\n\nPapers published in journals, peer-reviewed conferences, workshops, technical and \nsymposium from 2004 and 2018 were included. In addition, the most recent papers \nwere selected in case of papers with similar investigations and results.\n\nTable 2 First search string result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 2097 65 133 2295\n\nTable 3 Second search string result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 196 27 92 315\n\nTable 4 Third Search string refinement result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 64 23 24 111\n\nTable 5 Final Selection\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 25 10 12 47\n\n\n\nPage 9 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nExclusion criteria\n\nPapers that belong to the following categories were excluded from selection as part of \nthe primary study: (i) papers written in source language other than English; (ii) papers \nwith an abstract and or introduction that does not clearly define the contributions of the \nwork; (iii) papers whose abstract do not relate to big data stream analysis.\n\nResult\nThe findings of the study are now presented with respect to the research questions that \nguided the execution of the systematic literature review.\n\nResearch Question 1: What are the tools and technologies employed for big data stream \n\nanalysis?\n\nBig data stream platforms provide functionalities and features that enable big data \nstream applications to develop, operate, deploy, and manage big data streams. Such \nplatforms must be able to pull in streams of data, process the data and stream it back \nas a single flow. Several tools and technologies have been employed to analyse big data \nstreams. In response to the growing demand for big data streaming analytics, a large \nnumber of alternative big data streaming solutions have been developed both by the \nopen source community and enterprise technology vendors. According to [26], there are \nsome factors to consider when selecting big data streaming tools and technologies in \norder to make effective data management decisions. These are briefly described below.\n\nShape of the data\n\nStreaming data sources require serialization technologies for capturing, storing and rep-\nresenting such high-velocity data. For instance, some tools and technologies allow pro-\njection of different structures across data stores, giving room for flexibility for storage \nand access of data in different ways. However, the performance of such platforms may \nnot be suitable for high-velocity data.\n\nData access\n\nThere is a need to put into consideration how the data will be accessed by users and \napplications. For instance, many NoSQL databases require specific application interfaces \nfor data access. Hence there is a need to consider the integration of some other neces-\nsary tools for data access.\n\nAvailability and consistency requirement\n\nIf a distributed system is needed, then CAP theorem states that consistency and avail-\nability cannot be both guaranteed in the presence of network partition (i.e. when there is \na break in the network). In such a scenario, consistency is often traded off for availability \nto ensure that requests can always be processed.\n\nWorkload profile required\n\nPlatform as a service deployment may be appropriate for a spike load profile platform. \nIf platform distribution can be deployed on Infrastructure as a service cloud, then this \noption may be preferred as users will need to pay only when processing. On-premise \n\n\n\nPage 10 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\ndeployment may be considered for predictable or consistent loads. But if workloads are \nmixed (i.e. consistent flows or spikes), a combination of cloud and on-premise approach \nmay be considered so as to give room for easy integration of web-based services or soft-\nware and access to critical functions on the go.\n\nLatency requirement\n\nIf a minimal delay or low latency is required, key-value stores may be considered or bet-\nter still, an in-memory solution which allows the process of large datasets in real-time is \nrequired in order to optimize the data loading procedure.\n\nThe tools and technologies for big data stream analysis can be broadly categorized into \ntwo, which are open source and proprietary solutions. These are listed in Tables 6 and 7.\n\nThe selection of big data streaming tools and technologies should be based on the impor-\ntance of each factor earlier mentioned in this section. Proprietary solutions may not be eas-\nily available because of pricing and licensing issues. While open source supports innovation \nand development at a large scale, careful selection must be made especially when choosing \na recent technology still in production due to limited maturity and lack of support from \nacademic researchers or developer communities. In addition, open source solutions may \nlead to outdating and modification challenges [27]. Moreover, the selection of whether pro-\nprietary or open source or combination of both should depend on the problem to address, \nthe understanding of the true costs, and benefits of both open and proprietary solutions.\n\nTable 6 Open source tools and technologies for big data stream analysis\n\nTools and technology Article\n\nBlockMon [83]\n\nNoSQL [4, 84–86]\n\nSpark streaming [67, 87–91]\n\nApache storm [68, 85, 86, 92–97]\n\nKafka [85, 91, 95, 96, 98]\n\nYahoo! S4 [6, 45, 87, 99]\n\nApache Samza [46, 67, 100]\n\nPhoton [67, 101]\n\nApache Aurora [67, 102]\n\nMavEStream [103]\n\nEsperTech [104, 105]\n\nRedis [106]\n\nC-SPARQL [107, 108]\n\nSAMOA [56, 78, 109]\n\nCQELS [108, 110, 111]\n\nETALIS [112]\n\nXSEQ [73]\n\nApache Kylin [113]\n\nSplunk stream [114]\n\n\n\nPage 11 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nResearch Question 2: What methods and techniques are used in analysing big data \n\nstreams?\n\nGiven the real-time nature, velocity and volume of social media streams, the clus-\ntering algorithms that are applied on streaming data must be highly scalable and \nefficient. Also, the dynamic nature of data makes it difficult to know the required or \ndesirable number of clusters in advance. This renders partitioning clustering tech-\nniques (such as k-median, k-means and k-medoid) or expectation-maximization \n(EM) algorithms-based approaches unsuitable for analysing real-time social media \ndata because they require prior knowledge of clusters in advance. In addition, due \nto concept drift inherent in social media streams, scalable graph partitioning algo-\nrithms are not also suitable because of their tendency towards balanced partitioning. \nSocial media streams must be analysed dynamically in order to provide decisions at \nany given time within a limited space and time window [28–30].\n\nDensity-based clustering algorithm (such as DenStream, OpticStream, Flock-\nStream, Exclusive and Complete Clustering) unlike partitioning algorithms does not \nrequire apriori number of clusters in advance and can detect outliers [31]. However, \nthe issue with density-based clustering algorithms is that most of them except for few \nlike HDDStream, PreDeCon-Stream and PKS-Stream (which are memory intensive) \nperform less efficiently in the face of high dimensional data and as a result are not \nsuitable for analyzing social media streams [32].\n\nThreshold-based techniques, hierarchical clustering, and incremental clustering \nor online clustering are more relevant to social media analysis. Several online thresh-\nold-based stream clustering approaches or incremental clustering approaches such as \nMarkov Random Field [33, 34], Online Spherical K-means [35], and Condensed Clusters \n[36] have been adopted. Incremental approaches are suitable for continuously generated \ndata grouping by setting a maximum similarity threshold between the incoming stream \n\nTable 7 Proprietary tools and technologies for big data stream analysis\n\nTools and technology Article\n\nCodeBlue [115]\n\nAnodot [116]\n\nCloudet [117]\n\nSentiment brand monitoring [118]\n\nNumenta [119]\n\nElastic streaming processing engine [120]\n\nMicrosoft azure stream analytics [121]\n\nIBM InfoSphere streams [8, 122]\n\nGoogle MillWheel [123]\n\nArtemis [124]\n\nWSO2 analytics [125]\n\nMicrosoft StreamInsight [126]\n\nTIBCO StreamBase [127]\n\nStriim [128]\n\nKyvos insights [129]\n\nAtScale [130, 131]\n\nLambda architecture [57]\n\n\n\nPage 12 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nand the existing clusters. Much work has been done in improving the efficiency of online \nclustering algorithms, however, little research efforts have been directed to threshold \nand fragmentation issues. Incremental algorithm threshold setting should employ adap-\ntive approach instead of relying on static values [37, 38]. Some of the methods and tech-\nniques that have been employed in analysing big data streams are outlined in Table 8.\n\nTable 8 Methods and techniques for big data stream analysis\n\nMethods and techniques Article\n\nSPADE [132]\n\nLocally supervised metric learning (LSML) [133]\n\nKTS [106]\n\nMultinomial latent dirichlet allocation [106]\n\nVoltage clustering algorithm [106]\n\nLocality sensitive hashing (LSH) [134]\n\nUser profile vector update algorithm [134]\n\nTag assignment stream clustering (TASC) [134]\n\nStreamMap [117]\n\nDensity cognition [117]\n\nQRS detection algorithm [87]\n\nForward chaining rule [110]\n\nStream [135]\n\nCluStream [136, 137]\n\nHPClustering [138]\n\nDenStream [139]\n\nD-Stream [140]\n\nACluStream [141]\n\nDCStream [142]\n\nP-Stream [143]\n\nADStream [144]\n\nContinuous query processing (CQR) [145]\n\nFPSPAN-growth [146]\n\nOutlier method for cloud computing algorithm (OMCA) [147]\n\nMulti-query optimization strategy (MQOS) [148]\n\nParallel K-means clustering [72]\n\nVisibly push down automata (VPA) [73]\n\nIncremental MI outlier detection algorithm (Inc I-MLOF) [149]\n\nAdaptive windowing based online ensemble (AWOE) [74]\n\nDynamic prime-number based security verification [84]\n\nK-anonymity, I-diversity, t-closeness [90]\n\nSingular spectrum matrix completion (SS-MC) [76]\n\nTemporal fuzzy concept analysis [96]\n\nECM-sketch [77]\n\nNearest neighbour [91]\n\nMarkov chains [91]\n\nBlock-QuickSort-AdjacentJobMatch [86]\n\nBlock-QuickSort-OverlapReplicate [86]\n\nFuzzy-CSar-AFP [150]\n\nWeighted online sequential extreme learning machine with kernels (WOS-ELMK) [22]\n\nConcept-adapting very fast decision tree (CVFDT) [151]\n\n\n\nPage 13 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nMany researchers have looked at the aspect of the real-time analysis of big data \nstreams but not much attention has been directed towards social media stream pre-\nprocessing. For instance, the social media stream is characterized by incomplete, noisy, \nslang, abbreviated words. Also, contextual meaning of social media post is essential for \nimproved event detection, sentiment analysis or any other social media analytics algo-\nrithms in terms of quality and accuracy [36, 39]. There is the need to give more atten-\ntion to the preprocessing stage of social media stream analysis in the face of incomplete, \nnoisy, slang, and abbreviated words that are pertinent to social media streams. These \nchallenges create opportunities application of new semantic technology approaches, \nwhich are more suited to social media streams [40, 41].\n\nResearch Question 3: What do big data streaming tools and technologies have in common \n\nand their differences in terms of concept, purpose, and capabilities?\n\nThe features of various tools and technologies for big data stream were compared in \norder to answer this question. An overview analysis based on 10 dimensions, which are \ndatabase support, execution model, workload, fault-tolerance, latency, throughput, reli-\nability, operating system, implementation languages and application domain or areas is \npresented in Table 9.\n\nFor organisations with existing applications that have support for SQL, MySQL, SQL \nServer, Oracle Database, for instance, may consider choosing big data streaming tools \nand technologies that have support for their existing databases. There are few big data \nstreaming tools and technology that support virtually any data format. An example of \nsuch is Infochimps Cloud.\n\nThe major big data streaming tools and technologies considered are all suitable for \nstreaming execution model, however out of 19 big data tools and technology compared \nand contrasted in this section, only 10.5% is suitable for streaming, batch, and iterative \nprocessing while 47.4% can handle jobs requiring both batch and streaming processing. \nIt is safer for a job to be executed on a single platform which can accommodate all the \ndependencies required in order to avoid interoperability constraints than combining \ntwo or more platforms or frameworks. The best fit with respect to the choice of big data \nstreaming tools and technologies will depend on the state of data to process, infrastruc-\nture preference, business use case, and kind of results interested in.\n\nVirtually all the big data streaming tools and technologies are memory intensive. This \nimplies that the main performance bottleneck at higher load conditions will be due to \nlack of memory [42]. However, research has shown that the benefit of high intensive \nmemory applications outweighs the performance loss due to long memory latency [43].\n\nFrom all the big data streaming tools and technologies reviewed, only IBMInfoS-\nphere and TIBCO StreamBase support all of the three “at-most-once” “at-least-once” \nand “exactly-once” message delivery mechanisms while others support one or two of the \nthree delivery mechanisms. “At-most-once” is the cheapest with least implementation \noverhead and highest performance because it can be done in a fire-and-forget fashion \nwithout keeping the state in the transport mechanism or at the sending end. “At-least-\nonce” delivery requires multiple attempts in order to counter transport losses which \nmeans keeping the state at the sending end and having an acknowledgement mechanism \nat the receiving end. “Exactly-once” is the most expensive and has consequently worst \n\n\n\nPage 14 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nTa\nbl\n\ne \n9 \n\nCo\nm\n\npa\nri\n\nso\nn \n\nof\n b\n\nig\n d\n\nat\na \n\nst\nre\n\nam\nin\n\ng \nto\n\nol\ns \n\nan\nd \n\nte\nch\n\nno\nlo\n\ngi\nes\n\nTo\nol\n\ns \nan\n\nd \nte\n\nch\nno\n\nlo\ngy\n\nD\nat\n\nab\nas\n\ne \nsu\n\npp\nor\n\nt\nEx\n\nec\nut\n\nio\nn \n\nm\nod\n\nel\nW\n\nor\nkl\n\noa\nd\n\nFa\nul\n\nt t\nol\n\ner\nan\n\nce\nLa\n\nte\nnc\n\ny\nTh\n\nro\nug\n\nhp\nut\n\nRe\nlia\n\nbi\nlit\n\ny\nO\n\npe\nra\n\ntin\ng \n\nsy\nst\n\nem\nIm\n\npl\nem\n\nen\nta\n\ntio\nn/\n\nsu\npp\n\nor\nte\n\nd \nla\n\nng\nua\n\nge\ns\n\nA\npp\n\nlic\nat\n\nio\nn\n\nBl\noc\n\nkM\non\n\nCa\nss\n\nan\ndr\n\na,\n M\n\non\n-\n\ngo\nD\n\nB,\n X\n\nM\nL\n\nSt\nre\n\nam\nin\n\ng\nM\n\nul\nti-\n\nsl\nic\n\ne \nm\n\nem\n-\n\nor\ny \n\nal\nlo\n\nca\ntio\n\nn \nan\n\nd \nba\n\ntc\nh \n\nal\nlo\n\nca\ntio\n\nns\n\nC\nhe\n\nck\npo\n\nin\nt, \n\nro\nllb\n\nac\nk\n\nVe\nry\n\n lo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne\n\nLi\nnu\n\nx\nC\n\n +\n+\n\n11\n, P\n\nyt\nho\n\nn\nA\n\nno\nm\n\nal\ny \n\nde\nte\n\nct\nio\n\nn,\n \n\nne\ntw\n\nor\nk \n\nop\ntim\n\niz\na-\n\ntio\nn,\n\n m\nul\n\ntim\ned\n\nia\n \n\nco\nnt\n\nen\nt d\n\nel\niv\n\ner\ny,\n\n \nfin\n\nan\nci\n\nal\n m\n\nar\nke\n\nt \nan\n\nal\nys\n\nis\n, w\n\neb\n \n\nan\nal\n\nyt\nic\n\ns\n\nSp\nar\n\nk \nSt\n\nre\nam\n\nin\ng\n\nKa\nfk\n\na,\n H\n\nBa\nse\n\n, \nH\n\niv\ne \n\nFl\num\n\ne,\n \n\nH\nD\n\nF/\nS3\n\n, \nKi\n\nne\nsi\n\ns, \nTC\n\nP \nso\n\nck\net\n\ns, \nTw\n\nit-\nte\n\nr, \nSQ\n\nL\n\nBa\ntc\n\nh,\n It\n\ner\nat\n\niv\ne,\n\n \nSt\n\nre\nam\n\nin\ng\n\nC\nPU\n\n/m\nem\n\nor\ny \n\nin\nte\n\nns\niv\n\ne\nRD\n\nD\n b\n\nas\ned\n\n \nC\n\nhe\nck\n\n-p\noi\n\nnt\n-\n\nin\ng,\n\n p\nar\n\nal\nle\n\nl \nre\n\nco\nve\n\nry\n, \n\nre\npl\n\nic\nat\n\nio\nn\n\nLo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne\nW\n\nin\ndo\n\nw\ns, \n\nm\nac\n\nO\nS,\n\n L\nin\n\nux\nSc\n\nal\na,\n\n P\nyt\n\nho\nn,\n\n \nJa\n\nva\n, R\n\nEv\nen\n\nt d\net\n\nec\ntio\n\nn,\n \n\nst\nre\n\nam\nin\n\ng \nm\n\nac\nhi\n\nne\n \n\nle\nar\n\nni\nng\n\n, f\nog\n\n c\nom\n\n-\npu\n\ntin\ng,\n\n in\nte\n\nra\nct\n\niv\ne \n\nan\nal\n\nys\nis\n\n, m\nul\n\ntim\ne-\n\ndi\na \n\nan\nal\n\nys\nis\n\n, c\nlu\n\nst\ner\n\n \nan\n\nal\nys\n\nis\n, fi\n\nlte\nrin\n\ng,\n \n\nre\n-p\n\nro\nce\n\nss\nin\n\ng,\n \n\nca\nch\n\ne \nin\n\nva\nlid\n\nat\nio\n\nn\n\nA\npa\n\nch\ne \n\nSt\nor\n\nm\nSp\n\nou\nt, \n\nH\nBa\n\nse\n, \n\nH\niv\n\ne,\n S\n\nQ\nL,\n\n \nCa\n\nss\nan\n\ndr\na,\n\n \nM\n\nem\nca\n\nch\ned\n\nSt\nre\n\nam\nin\n\ng\nC\n\nPU\n/m\n\nem\nor\n\ny \nin\n\nte\nns\n\niv\ne\n\nRe\npl\n\nic\nat\n\nio\nn,\n\n \nch\n\nec\nkp\n\noi\nnt\n\n, \nda\n\nta\n re\n\nco\nve\n\nry\n, \n\nU\nps\n\ntr\nea\n\nm\n \n\nba\nck\n\nup\n, \n\nre\nco\n\nrd\n-le\n\nve\nl \n\nac\nkn\n\now\nle\n\ndg\ne-\n\nm\nen\n\nt, \nst\n\nat\nel\n\nes\ns \n\nm\nan\n\nag\nem\n\nen\nt\n\nVe\nry\n\n lo\nw\n\nLo\nw\n\nA\nt l\n\nea\nst\n\n o\nnc\n\ne\nW\n\nin\ndo\n\nw\ns, \n\nm\nac\n\nO\nS,\n\n L\nin\n\nux\nC\n\nlo\nju\n\nre\n, J\n\nav\na,\n\n S\nca\n\nla\n, \n\nC\nlo\n\nju\nre\n\n, n\non\n\n-J\nVM\n\n \nla\n\nng\nua\n\nge\ns\n\nIn\nte\n\nrn\net\n\n o\nf t\n\nhi\nng\n\ns, \nst\n\nre\nam\n\nin\ng \n\nm\nac\n\nhi\nne\n\n \nle\n\nar\nni\n\nng\n, m\n\nul\ntim\n\ne-\ndi\n\na \nan\n\nal\nys\n\nis\n\nYa\nho\n\no!\n S\n\n4\nM\n\nyS\nQ\n\nL,\n N\n\noS\nQ\n\nL,\n \n\nRi\nch\n\n D\nat\n\na \nFo\n\nrm\nat\n\nSt\nre\n\nam\nin\n\ng\nC\n\nPU\n/m\n\nem\nor\n\ny \nin\n\nte\nns\n\niv\ne\n\nRe\npl\n\nic\nat\n\nio\nn,\n\n \nch\n\nec\nkp\n\noi\nnt\n\n, \nda\n\nta\n re\n\nco\nve\n\nry\n\nLo\nw\n\nLo\nw\n\nEx\nac\n\ntly\n o\n\nnc\ne\n\nLi\nnu\n\nx\nJa\n\nva\n, P\n\nyt\nho\n\nn,\n C\n+\n+\n\n, \nPe\n\nrl\nO\n\nnl\nin\n\ne \nan\n\nal\nyt\n\nic\ns, \n\nm\non\n\nito\nrin\n\ng,\n fr\n\nau\nd \n\nde\nte\n\nct\nio\n\nn,\n fi\n\nna\nnc\n\nia\nl \n\nda\nta\n\n p\nro\n\nce\nss\n\nin\ng,\n\n \nw\n\neb\n p\n\ner\nso\n\nna\nliz\n\na-\ntio\n\nn \nan\n\nd \nse\n\nss\nio\n\nn \nm\n\nod\nel\n\nlin\ng\n\n\n\nPage 15 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nTa\nbl\n\ne \n9 \n\n(c\non\n\nti\nnu\n\ned\n)\n\nTo\nol\n\ns \nan\n\nd \nte\n\nch\nno\n\nlo\ngy\n\nD\nat\n\nab\nas\n\ne \nsu\n\npp\nor\n\nt\nEx\n\nec\nut\n\nio\nn \n\nm\nod\n\nel\nW\n\nor\nkl\n\noa\nd\n\nFa\nul\n\nt t\nol\n\ner\nan\n\nce\nLa\n\nte\nnc\n\ny\nTh\n\nro\nug\n\nhp\nut\n\nRe\nlia\n\nbi\nlit\n\ny\nO\n\npe\nra\n\ntin\ng \n\nsy\nst\n\nem\nIm\n\npl\nem\n\nen\nta\n\ntio\nn/\n\nsu\npp\n\nor\nte\n\nd \nla\n\nng\nua\n\nge\ns\n\nA\npp\n\nlic\nat\n\nio\nn\n\nA\npa\n\nch\ne \n\nSa\nm\n\nza\nKa\n\nfk\na,\n\n H\nD\n\nFS\n, \n\nKi\nne\n\nsi\ns, \n\nSt\nre\n\nam\n \n\nco\nns\n\num\ner\n\n, K\ney\n\n-\nva\n\nlu\ne \n\nst\nor\n\nes\n\nSt\nre\n\nam\nin\n\ng,\n b\n\nat\nch\n\n \npr\n\noc\nes\n\nsi\nng\n\nM\nem\n\nor\ny \n\nin\nte\n\nn-\nsi\n\nve\nC\n\nhe\nck\n\npo\nin\n\nt\nVe\n\nry\n lo\n\nw\nH\n\nig\nh\n\nA\nt l\n\nea\nst\n\n o\nnc\n\ne\nLi\n\nnu\nx,\n\n W\nin\n\ndo\nw\n\ns\nJa\n\nva\n, S\n\nca\nla\n\n, J\nVM\n\n \nla\n\nng\nua\n\nge\ns\n\nFi\nlte\n\nrin\ng,\n\n re\n-p\n\nro\n-\n\nce\nss\n\nin\ng,\n\n c\nac\n\nhe\n \n\nin\nva\n\nlid\nat\n\nio\nn\n\nA\npa\n\nch\ne \n\nFl\nin\n\nk\nKa\n\nfk\na,\n\n F\nlu\n\nm\ne,\n\n \nH\n\nD\nF/\n\nS3\n, \n\nKi\nne\n\nsi\ns, \n\nTC\nP \n\nso\nck\n\net\ns, \n\nTw\nit-\n\nte\nr, \n\nCa\nss\n\nan\ndr\n\na,\n \n\nRe\ndi\n\ns, \nM\n\non\n-\n\ngo\nD\n\nB,\n H\n\nBa\nse\n\n, \nSQ\n\nL\n\nSt\nre\n\nam\nin\n\ng,\n \n\nba\ntc\n\nh,\n it\n\ner\nat\n\niv\ne,\n\n \nin\n\nte\nra\n\nct\niv\n\ne\n\nM\nem\n\nor\ny \n\nin\nte\n\nn-\nsi\n\nve\nSt\n\nre\nam\n\n re\npl\n\nay\n \n\nan\nd \n\nm\nar\n\nke\nr-\n\nch\nec\n\nkp\noi\n\nnt\n\nVe\nry\n\n lo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne\nLi\n\nnu\nx,\n\n M\nac\n\nO\nS,\n\n \nW\n\nin\ndo\n\nw\ns\n\nJa\nva\n\n, S\nca\n\nla\n, P\n\nyt\nho\n\nn\nO\n\npt\nim\n\niz\nat\n\nio\nn \n\nof\n \n\ne-\nco\n\nm\nm\n\ner\nce\n\n \nse\n\nar\nch\n\n re\nsu\n\nlt,\n \n\nne\ntw\n\nor\nk/\n\nse\nns\n\nor\n \n\nm\non\n\nito\nrin\n\ng \nan\n\nd \ner\n\nro\nr d\n\net\nec\n\ntio\nn,\n\n \nET\n\nL \nfo\n\nr b\nus\n\nin\nes\n\ns \nin\n\nte\nlli\n\nge\nnc\n\ne \nin\n\nfra\n-\n\nst\nru\n\nct\nur\n\ne,\n m\n\nac\nhi\n\nne\n \n\nle\nar\n\nni\nng\n\nA\npa\n\nch\ne \n\nA\nur\n\nor\na\n\nH\n2,\n\n Ja\nva\n\n m\nap\n\ns, \nM\n\nyB\nat\n\nis\n, \n\nM\nyS\n\nQ\nL,\n\n P\nos\n\nt-\ngr\n\neS\nQ\n\nL\n\nSt\nre\n\nam\nin\n\ng\nM\n\nem\nor\n\ny \nan\n\nd \ndi\n\nsk\n s\n\npa\nce\n\nPe\nrio\n\ndi\nc \n\nre\nco\n\nv-\ner\n\ny \nch\n\nec\nkp\n\noi\nnt\n\n \nan\n\nd \nro\n\nllb\nac\n\nk\n\nLo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne\n\nLi\nnu\n\nx\nPy\n\nth\non\n\nM\non\n\nito\nrin\n\ng \nap\n\npl\nic\n\na-\ntio\n\nns\n s\n\nuc\nh \n\nas\n \n\nfin\nan\n\nci\nal\n\n a\nna\n\nly\nsi\n\ns \nan\n\nd \nm\n\nili\nta\n\nry\n a\n\npp\nli-\n\nca\ntio\n\nns\n\nRe\ndi\n\ns\nKe\n\ny-\nva\n\nlu\ne \n\nst\nor\n\nes\n, \n\nra\nbi\n\ntm\nq,\n\n M\non\n\n-\ngo\n\nD\nB\n\nSt\nre\n\nam\nin\n\ng\nIn\n\n-m\nem\n\nor\ny \n\nbu\nt \n\npe\nrs\n\nis\nte\n\nnt\n o\n\nn-\ndi\n\nsk\n d\n\nat\nab\n\nas\ne\n\nRe\npl\n\nic\na \n\nm\nig\n\nra\n-\n\ntio\nn,\n\n S\nen\n\ntin\nel\n\nLo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne\n\nU\nbu\n\nnt\nu,\n\n L\nin\n\nux\n, \n\nO\nSX\n\nC\n, C\n\n#,\n Ja\n\nva\n, P\n\nH\nP, \n\nPy\nth\n\non\nW\n\neb\n a\n\nna\nly\n\nsi\ns, \n\nca\nch\n\ne,\n \n\nm\nes\n\nsa\nge\n\n q\nue\n\nue\ns\n\nC\n-S\n\nPA\nRQ\n\nL\nRD\n\nF, \nSQ\n\nLJ\n, \n\nN\noS\n\nQ\nL,\n\n H\nD\n\nF\nBa\n\ntc\nh,\n\n s\ntr\n\nea\nm\n\nin\ng\n\nLo\nw\n\n m\nem\n\nor\ny \n\nus\nag\n\ne\nA\n\nda\npt\n\nat\nio\n\nn\nVe\n\nry\n lo\n\nw\nH\n\nig\nh\n\nCu\nm\n\nul\nat\n\niv\ne\n\nW\nin\n\ndo\nw\n\ns, \nLi\n\nnu\nx,\n\n M\nac\n\nO\nS,\n\n \nA\n\nnd\nro\n\nid\n\nJa\nva\n\n, A\npa\n\nch\ne \n\nJe\nna\n\n \nlib\n\nra\nrie\n\ns\nRe\n\nal\n-t\n\nim\ne \n\nre\nas\n\non\nin\n\ng \nov\n\ner\n s\n\nen\nso\n\nr d\nat\n\na,\n \n\nso\nci\n\nal\n s\n\nem\nan\n\ntic\n \n\nda\nta\n\n, u\nrb\n\nan\n c\n\nom\n-\n\npu\ntin\n\ng\n\nSA\nM\n\nO\nA\n\nH\nBa\n\nse\n, H\n\niv\ne,\n\n C\nas\n\n-\nsa\n\nnd\nra\n\nSt\nre\n\nam\nin\n\ng\nLo\n\nw\n m\n\nem\nor\n\ny \nus\n\nag\ne\n\nU\nps\n\ntr\nea\n\nm\n \n\nba\nck\n\nup\nLo\n\nw\nH\n\nig\nh\n\nEx\nac\n\ntly\n o\n\nnc\ne\n\nLi\nnu\n\nx\nJa\n\nva\nC\n\nla\nss\n\nifi\nca\n\ntio\nn,\n\n c\nlu\n\nst\ner\n\n-\nin\n\ng,\n s\n\npa\nm\n\n d\net\n\nec\n-\n\ntio\nn,\n\n re\ngr\n\nes\nsi\n\non\n, \n\nfre\nqu\n\nen\nt p\n\nat\nte\n\nrn\n \n\nm\nin\n\nin\ng\n\n\n\nPage 16 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nTa\nbl\n\ne \n9 \n\n(c\non\n\nti\nnu\n\ned\n)\n\nTo\nol\n\ns \nan\n\nd \nte\n\nch\nno\n\nlo\ngy\n\nD\nat\n\nab\nas\n\ne \nsu\n\npp\nor\n\nt\nEx\n\nec\nut\n\nio\nn \n\nm\nod\n\nel\nW\n\nor\nkl\n\noa\nd\n\nFa\nul\n\nt t\nol\n\ner\nan\n\nce\nLa\n\nte\nnc\n\ny\nTh\n\nro\nug\n\nhp\nut\n\nRe\nlia\n\nbi\nlit\n\ny\nO\n\npe\nra\n\ntin\ng \n\nsy\nst\n\nem\nIm\n\npl\nem\n\nen\nta\n\ntio\nn/\n\nsu\npp\n\nor\nte\n\nd \nla\n\nng\nua\n\nge\ns\n\nA\npp\n\nlic\nat\n\nio\nn\n\nCQ\nEL\n\nS\nRD\n\nF, \nSQ\n\nLJ\n, \n\nN\noS\n\nQ\nL,\n\n H\nD\n\nF\nBa\n\ntc\nh,\n\n s\ntr\n\nea\nm\n\nin\ng\n\nIn\n-m\n\nem\nor\n\ny\nA\n\nda\npt\n\nat\nio\n\nn\nLo\n\nw\nH\n\nig\nh\n\nCu\nm\n\nul\nat\n\niv\ne\n\nW\nin\n\ndo\nw\n\ns, \nLi\n\nnu\nx,\n\n M\nac\n\nO\nS,\n\n \nA\n\nnd\nro\n\nid\n\nJa\nva\n\nRe\nal\n\n-t\nim\n\ne \nre\n\nas\non\n\nin\ng \n\nov\ner\n\n s\nen\n\nso\nr d\n\nat\na,\n\n \nso\n\nci\nal\n\n s\nem\n\nan\ntic\n\n \nda\n\nta\n, u\n\nrb\nan\n\n c\nom\n\n-\npu\n\ntin\ng\n\nET\nA\n\nLI\nS\n\nRD\nF\n\nSt\nre\n\nam\nin\n\ng\nBi\n\nna\nriz\n\nat\nio\n\nn\nA\n\nda\npt\n\nat\nio\n\nn\nLo\n\nw\nLo\n\nw\nCu\n\nm\nul\n\nat\niv\n\ne\nW\n\nin\ndo\n\nw\ns, \n\nLi\nnu\n\nx,\n M\n\nac\nO\n\nS,\n \n\nA\nnd\n\nro\nid\n\nPr\nol\n\nog\n, J\n\nav\na,\n\n C\n, \n\nSP\nA\n\nRQ\nL,\n\n C\n#,\n\n \nET\n\nA\nLI\n\nS \nLa\n\nng\nua\n\nge\n \n\nfo\nr E\n\nve\nnt\n\ns \n(E\n\nLE\n)\n\nEv\nen\n\nt d\net\n\nec\ntio\n\nn,\n \n\nre\nas\n\non\nin\n\ng \nov\n\ner\n \n\nst\nre\n\nam\nin\n\ng \nev\n\nen\nts\n\nXS\nEQ\n\nXM\nL\n\nBa\ntc\n\nh,\n s\n\ntr\nea\n\nm\nin\n\ng\nIn\n\n-m\nem\n\nor\ny \n\nw\nith\n\n \nbu\n\nffe\nrin\n\ng\nch\n\nec\nkp\n\noi\nnt\n\nLo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne\n\nW\nin\n\ndo\nw\n\ns, \nLi\n\nnu\nx\n\nJa\nva\n\n, A\npa\n\nch\ne \n\nXe\nrc\n\nes\nBi\n\nol\nog\n\nic\nal\n\n d\nat\n\na,\n s\n\noc\nia\n\nl \nne\n\ntw\nor\n\nks\n, u\n\nse\nr \n\nbe\nha\n\nvi\nou\n\nr, \nfin\n\nan\nci\n\nal\n \n\nda\nta\n\n a\nna\n\nly\nsi\n\ns, \nfil\n\nte\nrin\n\ng\n\nIB\nM\n\n In\nfo\n\nSp\nhe\n\nre\n \n\nst\nre\n\nam\ns\n\nPi\ng,\n\n H\niv\n\ne,\n Ja\n\nql\n, \n\nH\nBa\n\nse\n F\n\nlu\nm\n\ne,\n \n\nLu\nce\n\nne\n, A\n\nvr\no,\n\n \nZo\n\noK\nee\n\npe\nr, \n\nO\noz\n\nie\n, O\n\nra\ncl\n\ne \nD\n\nat\nab\n\nas\ne,\n\n \nD\n\nB2\n, N\n\net\nez\n\nza\n, \n\nM\nyS\n\nQ\nL,\n\n A\nst\n\ner\n, \n\nIn\nfo\n\nrm\nix\n\n.\n\nSt\nre\n\nam\nin\n\ng\nCa\n\npt\nur\n\ne \nda\n\nta\nba\n\nse\n \n\nw\nor\n\nkl\noa\n\nds\n a\n\nnd\n \n\nre\npl\n\nay\n th\n\nem\n in\n\n \na \n\nte\nst\n\n d\nat\n\nab\nas\n\ne \nen\n\nvi\nro\n\nnm\nen\n\nt\n\nA\nut\n\nom\nat\n\nic\n \n\nre\nco\n\nve\nry\n\nLo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne,\n \n\nA\nt l\n\nea\nst\n\n \non\n\nce\n, A\n\nt \nm\n\nos\nt o\n\nnc\ne\n\nLi\nnu\n\nx,\n C\n\nen\ntO\n\nS\nC\n\n +\n+\n\nJa\nva\n\nSP\nL\n\nSp\nac\n\ne \nw\n\nea\nth\n\ner\n p\n\nre\n-\n\ndi\nct\n\nio\nn,\n\n p\nhy\n\nsi\nol\n\nog\ni-\n\nca\nl d\n\nat\na \n\nst\nre\n\nam\ns \n\nan\nal\n\nys\nis\n\n, t\nra\n\nffi\nc \n\nm\nan\n\nag\nem\n\nen\nt, \n\nre\nal\n\n-\ntim\n\ne \npr\n\ned\nic\n\ntio\nns\n\n, \nev\n\nen\nt d\n\net\nec\n\ntio\nn,\n\n \nvi\n\nsu\nal\n\nis\nat\n\nio\nn\n\nG\noo\n\ngl\ne \n\nM\nill\n\n-\nW\n\nhe\nel\n\nBi\ngT\n\nab\nle\n\n, S\npa\n\nn-\nne\n\nr\nSt\n\nre\nam\n\nin\ng\n\nIn\n-m\n\nem\nor\n\ny \nan\n\nd \nbl\n\noo\nm\n\n fi\nlte\n\nrin\ng\n\nU\nnc\n\noo\nrd\n\nin\nat\n\ned\n \n\npe\nrio\n\ndi\nc,\n\n \nch\n\nec\nkp\n\noi\nnt\n\n, \nup\n\nst\nre\n\nam\n \n\nba\nck\n\nup\n\nLo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne\nLi\n\nnu\nx\n\nVi\nrt\n\nua\nlly\n\n a\nny\n\n \npr\n\nog\nra\n\nm\nm\n\nin\ng \n\nla\nng\n\nua\nge\n\nA\nno\n\nm\nal\n\ny \nde\n\nte\nct\n\nio\nn,\n\n \nhe\n\nal\nth\n\n m\non\n\nito\nrin\n\ng,\n \n\nim\nag\n\ne \npr\n\noc\nes\n\nsi\nng\n\n, \nne\n\ntw\nor\n\nk \nsw\n\nitc\nh \n\nm\nan\n\nag\nem\n\nen\nt\n\n\n\nPage 17 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nTa\nbl\n\ne \n9 \n\n(c\non\n\nti\nnu\n\ned\n)\n\nTo\nol\n\ns \nan\n\nd \nte\n\nch\nno\n\nlo\ngy\n\nD\nat\n\nab\nas\n\ne \nsu\n\npp\nor\n\nt\nEx\n\nec\nut\n\nio\nn \n\nm\nod\n\nel\nW\n\nor\nkl\n\noa\nd\n\nFa\nul\n\nt t\nol\n\ner\nan\n\nce\nLa\n\nte\nnc\n\ny\nTh\n\nro\nug\n\nhp\nut\n\nRe\nlia\n\nbi\nlit\n\ny\nO\n\npe\nra\n\ntin\ng \n\nsy\nst\n\nem\nIm\n\npl\nem\n\nen\nta\n\ntio\nn/\n\nsu\npp\n\nor\nte\n\nd \nla\n\nng\nua\n\nge\ns\n\nA\npp\n\nlic\nat\n\nio\nn\n\nIn\nfo\n\nch\nim\n\nps\n \n\ncl\nou\n\nd\nSQ\n\nL,\n N\n\noS\nQ\n\nL,\n \n\nH\niv\n\ne,\n P\n\nig\n \n\nW\nuk\n\non\ng,\n\n \nH\n\nad\noo\n\np,\n \n\nRD\nBM\n\nS,\n V\n\nirt\nu-\n\nal\nly\n\n a\nny\n\n d\nat\n\na \nfo\n\nrm\nat\n\nBa\ntc\n\nh,\n s\n\ntr\nea\n\nm\nin\n\ng\nIn\n\n-m\nem\n\nor\ny\n\nU\nps\n\ntr\nea\n\nm\n \n\nba\nck\n\nup\nLo\n\nw\nH\n\nig\nh\n\nEx\nac\n\ntly\n o\n\nnc\ne\n\nLi\nnu\n\nx\nJa\n\nva\nD\n\nis\nas\n\nte\nr d\n\nis\nco\n\nve\nry\n\n, \nte\n\nxt\n a\n\nna\nly\n\nsi\ns, \n\nco\nm\n\n-\npl\n\nex\n e\n\nve\nnt\n\n p\nro\n\nce\nss\n\n-\nin\n\ng,\n v\n\nis\nua\n\nlis\nat\n\nio\nn\n\nM\nic\n\nro\nso\n\nft\n \n\nSt\nre\n\nam\nIn\n\nsi\ngh\n\nt\nSQ\n\nL \nSe\n\nrv\ner\n\nSt\nre\n\nam\nin\n\ng\nIn\n\n-m\nem\n\nor\ny\n\nRe\npl\n\nic\nat\n\nio\nn,\n\n \nch\n\nec\nkp\n\noi\nnt\n\n, \nda\n\nta\n re\n\nco\nve\n\nry\n\nVe\nry\n\n lo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne\nW\n\nin\ndo\n\nw\ns\n\n.N\nET\n\n, C\n#,\n\n L\nIN\n\nQ\n, R\n\nx\nM\n\nan\nuf\n\nac\ntu\n\nrin\ng \n\npr\noc\n\nes\ns \n\nm\non\n\nito\nr-\n\nin\ng \n\nan\nd \n\nco\nnt\n\nro\nl, \n\nfin\nan\n\nci\nal\n\n d\nat\n\na \nan\n\nal\nys\n\nis\n, o\n\npe\nra\n\n-\ntio\n\nn \nan\n\nal\nyt\n\nic\ns, \n\nw\neb\n\n \nan\n\nal\nyt\n\nic\ns, \n\nev\nen\n\nt \npa\n\ntt\ner\n\nn \nde\n\nte\nct\n\nio\nn\n\nTI\nBC\n\nO\n S\n\ntr\nea\n\nm\n-\n\nBa\nse\n\nO\nra\n\ncl\ne \n\nda\nta\n\nba\nse\n\n, \nSQ\n\nL \nSe\n\nrv\ner\n\n, \nIm\n\npa\nla\n\nBa\ntc\n\nh,\n S\n\ntr\nea\n\nm\nin\n\ng\nIn\n\n-m\nem\n\nor\ny\n\nSy\nnc\n\nhr\non\n\niz\nat\n\nio\nn,\n\n \nre\n\npl\nic\n\nat\nio\n\nn,\n \n\nro\nllb\n\nac\nk\n\nVe\nry\n\n lo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne/\n\nat\n m\n\nos\nt \n\non\nce\n\n/\nex\n\nac\ntly\n\n o\nnc\n\ne\n\nW\nin\n\ndo\nw\n\ns, \nM\n\nac\nO\n\nS,\n L\n\nin\nux\n\nR,\n Ja\n\nva\nM\n\nis\nsi\n\non\n c\n\nrit\nic\n\nal\n \n\nan\nal\n\nys\nis\n\n, I\noT\n\n a\nna\n\nly\n-\n\nsi\ns, \n\ncl\nic\n\nk-\nst\n\nre\nam\n\n \nan\n\nal\nys\n\nis\n, p\n\nre\ndi\n\nct\niv\n\ne \nan\n\nal\nyt\n\nic\ns, \n\nw\nor\n\nkfl\now\n\n \nop\n\ntim\niz\n\nat\nio\n\nn,\n ri\n\nsk\n \n\nav\noi\n\nda\nnc\n\ne\n\nLa\nm\n\nbd\na \n\nA\nrc\n\nhi\n-\n\nte\nct\n\nur\ne\n\nRD\nBM\n\nS,\n C\n\nas\nsa\n\nn-\ndr\n\na,\n K\n\naf\nka\n\n, D\nat\n\na \nW\n\nar\neh\n\nou\nse\n\ns, \nKi\n\nne\nsi\n\ns \nD\n\nat\na \n\nSt\nre\n\nam\n, H\n\nD\nFS\n\n, \nH\n\nBa\nse\n\nBa\ntc\n\nh,\n S\n\ntr\nea\n\nm\nin\n\ng\nIn\n\n-m\nem\n\nor\ny/\n\ndi\nsk\n\n \nda\n\nta\nba\n\nse\nRe\n\npl\nic\n\nat\nio\n\nn,\n \n\nch\nec\n\nkp\noi\n\nnt\nLo\n\nw\nLo\n\nw\nEx\n\nac\ntly\n\n o\nnc\n\ne\nU\n\nbu\nnt\n\nu,\n W\n\nin\n-\n\ndo\nw\n\ns, \nLi\n\nnu\nx\n\nJa\nva\n\n, C\n#,\n\n P\nyt\n\nho\nn,\n\n \nPi\n\ng \nLa\n\ntin\nIo\n\nT \nan\n\nal\nys\n\nis\n, t\n\nra\nck\n\nin\ng \n\nre\nal\n\n-t\nim\n\ne \nup\n\nda\nte\n\ns, \nfin\n\nan\nci\n\nal\n ri\n\nsk\n m\n\nan\n-\n\nag\nem\n\nen\nt, \n\ncl\nic\n\nk-\nst\n\nre\nam\n\n a\nna\n\nly\nsi\n\ns\n\n\n\nPage 18 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nperformance because, in addition to “at-least-once” delivery mechanism, it requires the \nstate to be kept at the receiving end in order to filter duplicate deliveries. In other words, \n“at-most-once” delivery mechanism implies that the message may be lost while “at-\nleast-once” delivery ensures that messages are not lost and “exactly-once” implies that \nmessage can neither be lost nor duplicated. “Exactly-once” is suitable for many critical \nsystems where duplicate messages are unacceptable.\n\nResearch Question 4: What are the limitations and strengths of big data streaming tools \n\nand technologies?\n\nObservations from the literature reveal that specific big data streaming technology may \nnot provide the full set of features that are required. It is rare to find specific big data \ntechnology that combines key features such as scalability, integration, fault-tolerance, \ntimeliness, consistency, heterogeneity and incompleteness management, and load bal-\nancing. For instance, Spark streaming [16] and Sonora [44] are excellent and efficient \nfor checkpointing but the operator space available to user codes are limited. S4 does not \nguarantee 100% fault-tolerant persistent state [45]. Storm does not guarantee the order-\ning of messages due to its “at-least-once” mechanism for record delivery [46, 47]. Strict \ntransaction ordering is required by Trident to operate [48]. While streaming SQL pro-\nvide simple and succinct solutions to many streaming problems, the complex application \nlogic (such as matrix multiplication) and intuitive state abstractions are expressed with \nthe operational flow of an imperative language rather than a declarative language such as \nSQL [49–51].\n\nMoreover, BlockMon uses batches and cache locality optimization techniques for \nmemory allocation efficiency and data speed up access. However, deadlock may occur \nif data streams are enqueued with a higher rate than that of the block consumption [52]. \nApache Samza solves batch latency processing problems but requires an added layer for \nflow control [53]. Flink is suitable for heavy stream processing and batch-oriented tasks \nalthough it has scaling limitations [46]. Redis’ in-memory data store makes it extremely \nfast although this implies that available memory size determines the size of the Redis \ndata store [54]. While C-SPARQL and CQELS are excellent for combining static and \nstreaming data, they are not suitable when scalability is required [55]. SAMOA is suit-\nable for machine learning paradigm as it focuses on speed/real-time analytics, scales \nhorizontally and is loosely coupled with its underlying distributed computation platform \n[56]. With Lambda architecture, a real-time layer can complement the batch processing \none thereby reducing maintenance overhead and risk for errors as a result of duplicate \ncode bases. In addition, Lambda architecture handles reprocessing, which is one of the \nkey challenges in stream processing. Two main problems with Lambda architecture are \ncode maintenance in two complex distributed systems that need to produce the same \nresult and high operational complexity [57, 58].\n\nSummarily, there exists various tools and technologies for implementing big data \nstreams and there seems to be no big data streaming tool and technology that offers all \nthe key features required for now. While each tool and technology may have its strengths \nand weaknesses, the choice depends on the objective of the research and data availa-\nbility. A decision in favour of the wrong technology may result in increased overhead \ncost and time. The decision should take into consideration empirical analysis along with \n\n\n\nPage 19 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nsystem requirements. In addition, research efforts should also be directed to how to \nimprove on existing big data streaming tools and technologies to provide key features \nsuch as scalability, integration, fault-tolerance, timeliness, consistency, heterogeneity \nand incompleteness management, and load balancing.\n\nResearch Question 5: What are the evaluation techniques or benchmarks that are used \n\nfor evaluating big data streaming tools and technologies?\n\nThe diversity of big data poses a challenge when it comes to developing big data bench-\nmarks that will be suitable for all workload cases. One cannot stick to one big data \nbenchmark because it has been observed that using only one benchmark on differ-\nent data sets do not give the same result. This implies that benchmark testing should \nbe application specific. Subsequently, in evaluating big data system, the identification \nof workload for an application domain is a prerequisite [59]. Most of the existing big \ndata benchmarks are designed to evaluate a specific type of systems or architectures. For \ninstance, HiBench [60] is suitable for benchmarking Hadoop, Spark and streaming work-\nloads, GridMix [61] and PigMix [62] are for MapReduce Hadoop systems. BigBench [63, \n64] is suitable for benchmarking Teradata Aster DBMS, MapReduce systems, Redshift \ndatabase, Hive, Spark and Impala. Presently, BigDataBench [65, 66] seems to be the only \nbig data benchmark that can evaluate a hybrid of different big data systems.\n\nSo far, many researchers have evaluated their work by making use of synthetic and \nreal-life data. Standard benchmark dataset for big data streaming analytics has not been \nwidely adopted. However, few of the researchers that used standardized benchmarking \nare briefly discussed below. The work of [67] was tested with two benchmarks; Word \nCount and Grep. The result showed that the proposed algorithm can effectively handle \nunstable input and the delay of the total event can be limited to an expected range.\n\nThe tool developed by [68] was tested on both car dataset and Wikinews5 dataset in \ncomparison with sequential processing. It was discovered that their tool (pipeline imple-\nmentation) performed better and faster.\n\nKrawczyk and Wozniak used several benchmark datasets which include Breast-Wis-\nconsin, Pima, Yeast3, Voting records, CYP2C19 isoform, RBF for estimating weights for \nthe new incoming data stream with their proposed method against other standard meth-\nods. They also analysed time and memory requirements. Experimental investigation \nresult proved that the proposed method can achieve better [69].\n\nA benchmark evaluation using an English movie review dataset collected from Rotten \nTomatoes website (a de facto benchmark for analysing sentiment applications) was con-\nducted by [70], the result showed that sentiment analysis engine (SAE) proposed by the \nauthors outperformed the bag of words approach.\n\nAuthors’ suite of ideas in [71] outperformed state-of-the-art searching technique \ncalled EBSM. The work of [72] used various datasets such as KDD-Cup 99, Forest Cover \ntype, Household power consumption, etc. They compared their algorithm—parallel \nK-means clustering with k-means and k-means++, the result showed that their algo-\nrithm performed better in terms of speed.\n\n5 http://en.wikin ews.org.\n\nhttp://en.wikinews.org\n\n\nPage 20 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nMozafari et al. in [73] benchmarked their system, XSeq against other general-purpose \nXML engines. The system outperformed other complex event processing engines by two \norders of magnitude improvement.\n\nAuthors in [74] evaluated their work in terms of time, accuracy and memory using \nForest cover type, Poker hand, and electricity datasets. They compared their method, \nadaptive windowing based online ensemble (AWOE) with other standard methods such \nas accuracy updated ensemble (AUE), online accuracy updated ensemble (OAUE), accu-\nracy weighted ensemble (AWE), dynamic weighted majority (DWM) and Lev Bagging \n(Lev). Their proposed approach outperformed other methods in three perspectives \nwhich include suitability in terms of different type of drifts, better resolved appropriate \nsize of block, and efficiency.\n\nThe evaluation performed by [75] using FACup and Super Tuesday datasets showed \nthat their method, which is a hybrid of topic extraction methods (i.e. a combination of \nfeature pivot and document pivot) has high efficiency and accuracy with respect to recall \nand precision.\n\nEvaluating the performance of low-rank reconstruction and prediction scheme, spe-\ncifically, singular spectrum matrix completion (SS-MC) proposed by [76], SensorScope \nGrand St-Bernard dataset6 and Intel Berkeley Research Lab dataset7 were used. The \nauthors compared their proposed method with three state-of-the-art methods; KNN-\nimputation, RegEM and ADMM version of MC and discovered that their method \noutperformed the other methods in terms of pure reconstruction as well as in the \ndemanding case of simultaneous recovery and prediction.\n\nThe authors in [77] evaluated their work using World Cup 1998 and CAIDA \nAnonymized Internet Traces 2011 datasets. When their method, ECM-Sketch (a sketch \nsynopsis that allows effective summarization of streaming data over both time-based \nand count-based sliding windows) was compared with three state-of-the-art algorithms \n(Sketch variants); ECM-RW, ECM-DW, and ECM-EH, variants using randomized waves, \ndeterministic waves and exponential histograms respectively, their method reduce \nmemory and computational requirements by at least one order of magnitude with a very \nsmall loss in accuracy.\n\nThe work of [78] centred on benchmarking real-time vehicle data streaming models \nfor a smart city using a simulator that emulates the data produced by a given amount of \nsimultaneous drivers. Experiment with the simulator shows that streaming processing \nengine such as Apache Kafka could serve as a replacement to custom-made streaming \nservers to achieve low latency and higher scalability together with cost reduction.\n\nA benchmark among Kyvos Insight, Impala and Spark conducted by [79] shows that \nKyvos Insight performed analytical queries with much lower latencies when there is a \nlarge number of concurrent users due to pre-aggregation and incremental code building \n[80].\n\nAuthors in [81] proposed that in addition to execution time and resource utilization, \nmicroarchitecture-level and energy consumption are key to fully understanding the \nbehaviour of big data frameworks.\n\n6 http://lcav.epfl.ch.page-86035 -en.html.\n7 http://db.csail .mit.edu/labda ta/labda ta.html.\n\nhttp://lcav.epfl.ch.page-86035-en.html\nhttp://db.csail.mit.edu/labdata/labdata.html\n\n\nPage 21 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nIn addition, to strengthen the confidence of big data research evaluation or result, \napplication of empirical methods (i.e. tested or evaluated concept or technology for \nevidence-based result) should be highly encouraged. The current status of empirical \nresearch in big data stream analysis is still at an infant stage. The maturity of a research \nfield is directly proportional to the number of publications with empirical result [20, 21]. \nAccording to [21] that conducted a systematic literature mapping to verify the current \nstatus of empirical research in big data, it was found out that only 151 out of 1778 stud-\nies contained empirical result. As a result, more research efforts should be directed to \nempirical research in order to raise the level of confidence of big data research outputs \nthan it is at present.\n\nMoreover, only a few big data benchmarks are suitable for different workloads at pre-\nsent. Research efforts should be geared towards advancing benchmarks that are suitable \nfor evaluating different big data systems. This would go a long way to reduce cost and \ninteroperability issue.\n\nDiscussion\nFrom the analysis, it was observed that there has been a wave of interest in big data \nstream analysis since 2013. The number of papers produced in 2012 was doubled in \n2013. In the same vein, more than double of the papers in 2013 were produced in 2014. \nThere was a relative surge in 2017 having a total of 98 paper while the year 2018 received \n156 papers (see Tables 9, 10 and Fig. 2). The percentage of papers analyzed from journals \nwas 50%; that of conferences was 41% while that of workshop/technical/symposium was \n9% as depicted in Fig. 3. Figure 4 presented the frequency of research efforts from differ-\nent geographical locations with researchers from China taking the lead.   \n\nThe selection of big data streaming tools and technologies should be based on the \nimportance of each of the factors such as the shape of the data, data access, availabil-\nity and consistent requirements, workload profile required, and latency requirement. \nCareful selection with respect to open source technology must be made especially when \nchoosing a recent technology still in production. Moreover, the problem to address, the \nunderstanding of the true costs, and benefits of both open and proprietary solutions are \nalso vital when making a selection.\n\nA lot of research efforts have been directed to big data stream analysis but social media \nstream preprocessing is still an open issue. Due to inherent characteristics of social \nmedia stream which include incomplete, noisy, slang, abbreviated words, social media \nstreams present a challenge to big data streams analytics algorithms. There is the need \nto give more attention to the preprocessing stage of social media stream analysis in the \nface of incomplete, noisy, slang, and abbreviated words that are pertinent to social media \nstreams in order to improve big data streams analytics result.\n\nOut of 19 big data streaming tools and technologies compared, 100% support stream-\ning, 47.4% can do both batch and streaming processing while only 10.5% support stream-\ning, batch and iterative processing. Depending on the state of the data to be processed, \ninfrastructure preference, business use case, and kind of results that is of interest, choos-\ning a single big data streaming technology platform that supports all the system require-\nments minimizes the effect of interoperability constraints.\n\n\n\nPage 22 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nFrom all the big data streaming tools and technologies reviewed, only IBMInfoS-\nphere and TIBCO StreamBase support all of the three “at-most-once”, “at-least-once”, \nand “exactly-once” message delivery mechanisms while others support one or two of \nthe three delivery mechanisms. Having all the three delivery mechanisms give room for \nflexibility.\n\nIt is rare to find a specific big data technology that combines key features such as scal-\nability, integration, fault-tolerance, timeliness, consistency, heterogeneity and incom-\npleteness management, and load balancing. There seems to be no big data streaming \ntool and technology that offers all the key features required for now. This calls for more \nresearch efforts that are directed to building more robust big data streaming tools and \ntechnologies.\n\nFew big data benchmarks are suitable for a hybrid of big data systems at present and \nstandard benchmark datasets for big data streaming analytics have not been widely \nadopted. Hence, research efforts should be geared towards advancing benchmarks that \nare suitable for evaluating different big data systems.\n\nLimitation of the review\nWhile authors explored Scopus, ScienceDirect and EBSCO databases which index high \nimpact journals and conference papers from IEEE, ACM, SpringerLink, and Elsevier to \nidentify all possible relevant articles, it is possible that some other relevant articles from \nother databases such as Web of Science could have been missed.\n\nThe analysis and synthesis are based on interpretation of selected articles by the \nresearch team. The authors attempted to avoid this by cross-checking papers to deal \nwith bias though that cannot completely rule out the possibility of errors. In addition, \nthe authors implemented the inclusion and exclusion criteria in the selection of articles \nand only relevant articles written in the English Language were selected. Building on the \nunderpinning of the findings of the research, while a lot of research has been done with \nrespect to tools and technologies as well as methods and techniques employed in big \ndata streaming analytics, method of evaluation or benchmarks of the technologies of \nvarious workloads for big data streaming analytics have not received much attention. As \nit could be gathered from the literature reviewed that most of the researchers evaluated \ntheir work using either synthetic or real-life datasets.\n\nConclusion and further work\nAs a result of challenges and opportunities presented by the Information Technology \nrevolution, big data streaming analytics has emerged as the new frontier of competition \nand innovation. Organisations who seize the opportunity of big data streaming analytics \nare provided with insights for robust decision making in real-time thereby making them \nto have an edge over their competitors.\n\nIn this paper, the authors have tried to present a holistic view of big data streaming \nanalytics by conducting a comprehensive literature review to understand and identify \nthe tools and technologies, methods and techniques, benchmarks or methods of evalu-\nation employed, and key issues in big data stream analysis to showcase the signpost of \nfuture research directions.\n\n\n\nPage 23 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nTa\nbl\n\ne \n10\n\n D\nis\n\ntr\nib\n\nut\nio\n\nn \nof\n\n p\nap\n\ner\ns \n\nov\ner\n\n th\ne \n\nst\nud\n\nie\nd \n\nye\nar\n\ns\n\nYe\nar\n\n20\n04\n\n20\n05\n\n20\n06\n\n20\n07\n\n20\n08\n\n20\n09\n\n20\n10\n\n20\n11\n\n20\n12\n\n20\n13\n\n20\n14\n\n20\n15\n\n20\n16\n\n20\n17\n\n20\n18\n\nTo\nta\n\nl\n\nPa\npe\n\nr\n2\n\n1\n2\n\n3\n5\n\n2\n5\n\n4\n5\n\n10\n22\n\n28\n38\n\n98\n15\n\n6\n38\n\n1\n\n\n\nPage 24 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nAlthough a lot of research efforts have been directed towards big data at rest (i.e. \nbig data batch processing), there has been increased interest in analysing big data \nin motion (i.e. big data stream processing). With respect to issues identified in this \npaper, big data streaming analytics can be considered as an emerging phenomenon \nalthough some countries and industries have seized the opportunities by making it a \n\nFig. 2 Magnitude of change in paper distribution. The figure shows the magnitude of change in paper \ndistribution over the studied years (i.e. 2004 to 2018)\n\nFig. 3 Percentage of publication type. The figure shows percentage of 381 papers from journals (50%), \nconferences (41%), and workshop/technical/symposium (9%)\n\nFig. 4 Frequency of researchers across different. The figure presented the frequency of research \ngeographical locations research efforts from different geographical locations\n\n\n\nPage 25 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\npertinent research area. Some of the key issues such as scalability, integration, fault-\ntolerance, timeliness, consistency, heterogeneity and incompleteness, load balancing, \nhigh throughput, and privacy that require further research attention were identified. \nWhile researchers have invested a lot of efforts to mitigate these issues, scalability, \nprivacy and load balancing remain a concern. In addition, researchers also need to \ngive more focus to the empirical analysis of big data streaming tools and technologies \nin order to be able to provide concrete reasons and support for choosing a tool/tech-\nnology based on empirical evidence.\n\nPresently, BigDataBench seems to be the only big data benchmark that can evaluate \na hybrid of different big data systems. Standard benchmark for a hybrid of big data sys-\ntems has not been widely adopted. It is rare to find a specific big data technology that \ncombines key features such as scalability, integration, fault-tolerance, timeliness, consist-\nency, heterogeneity and incompleteness management, and load balancing.\n\nThere is the need to give more attention to the preprocessing stage of social media \nstream analysis in the face of incomplete, noisy, slang, and abbreviated words that are \npertinent to social media streams. Many researchers have looked at the aspect of the \nreal-time analysis of big data streams but not much attention has been directed towards \nsocial media stream preprocessing.\n\nIn addition, research efforts should be geared towards developing scalable frameworks \nand algorithms that will accommodate data stream computing mode, effective resource \nallocation strategy and parallelization issues to cope with the ever-growing size and \ncomplexity of data. As regards load balancing, a distributing environment that automati-\ncally streams partial data streams to a global centre when local resources become insuf-\nficient is required. The demand for big data stream analysis is that data must be analysed \nas soon as they arrive makes privacy issue a big concern. The main challenge here is \nproposing techniques for protecting a big data stream dataset before its analysis in such \na way that the real-time analysis is still maintained. As a result, research efforts should \nbe directed to the identified areas in order to have robust solutions for big data stream-\ning analytics.\n\nAbbreviations\nAUE: accuracy updated ensemble; AWE: accuracy weighted ensemble; AWOE: adaptive windowing based online \nensemble; CEP: complex event processing; CQR: continuous query processing; DWM: dynamic weighted majority; EM: \nexpectation–maximization; GOAL: GOd’s ALgorithm; IDC: International Data Cooperation; Inc I-MLOF: Incremental MI \noutlier detection algorithm; LSH: locality sensitive hashing; LSML: locally supervised metric learning; MQOS: multi-query \noptimization strategy; OAUE: online accuracy updated ensemble; OMCA: outlier method for cloud computing algorithm; \nSAE: sentiment analysis engine; SAMOA: scalable advanced massive online analysis; SS-MC: singular spectrum matrix \ncompletion; VPA: visibly push down automata; WOS-ELMK: weighted online sequential extreme learning machine with \nkernels.\n\nAcknowledgements\nThe research was supported by Covenant University Centre for Research, Innovation, and Discovery (CUCRID); Landmark \nUniversity, Omu-Aran, Osun State, Nigeria; The World Academy of Sciences for Research and Advanced Training Fellow-\nship, FR Number: 3240301383; and the Cape Peninsula University of Technology, South Africa.\n\nAuthors’ contributions\nTK gathered all the papers from various databases that were used for the manuscript and was a major contributor in \nwriting the manuscript. OD ensured that the guideline for systematic review literature was followed, and provided direc-\ntion for the literature-based research. AA contributed to the validation of selected primary studies. All authors contrib-\nuted to the selection of papers for the systematic review. All authors read and approved the final manuscript.\n\nFunding\nNot applicable.\n\n\n\nPage 26 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nAvailability of data and materials\nAll data (papers) analysed are included in Scopus, ScienceDirect, and EBSCOhost.\n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAuthor details\n1 Department of Computer and Information Sciences, Covenant University, Ota, Nigeria. 2 Department of Computer Sci-\nence, Federal University Lokoja, Lokoja, Kogi, Nigeria. 3 Department of Information Technology, Cape Peninsula University \nof Technology, Cape Town, South Africa. 4 Department of Computer Science, Landmark University, Omu-Aran, Kwara, \nNigeria. \n\nReceived: 28 March 2019   Accepted: 28 May 2019\n\nReferences\n 1. Mavragani A, Ochoa G, Tsagarakis KP. Assessing the methods, tools, and statistical procedures in Google trends \n\nresearch: systematic review. J Med Internet Res. 2018;20(11):e270.\n 2. Sun D, Zhang G, Zheng W, Li K. Key technologies for big data stream computing. In: Li K, Jiang H, Yang LT, Guz-\n\nzocrea A, editors. Big data algorithms, analytics and applications. New York: Chapman and Hall/CRC; 2015. p. \n193–214. ISBN 978-1-4822-4055-9.\n\n 3. Qian ZP, He Y, Su CZ et al. TimeStream: Reliable stream computation in the cloud. In: Proc. 8th ACM European \nconference in computer system, EuroSys 2013. Prague: ACM Press; 2013. p. 1–4.\n\n 4. Liu R, Li Q, Li F, Mei L, Lee, J. Big data architecture for IT incident management. In: Proceedings of IEEE international \nconference on service operations and logistics, and informatics (SOLI), Qingdao, China. 2014. p. 424–9.\n\n 5. Sakr S. An introduction to Infosphere streams: A platform for analysing big data in motion. IBM. 2013. https ://\nwww.ibm.com/devel operw orks/libra ry/bd-strea msint ro/index .html. Accessed 7 Oct 2018.\n\n 6. Xhafa F, Naranjo V, Caballé S. Processing and analytics of big data stream with Yahoo!S4. In: 2015 IEEE 29th inter-\nnational conference on advanced information networking and applications, Gwangiu, South Korea, 24–27 March \n2015. 2015. https ://doi.org/10.1109/aina.2015.194.\n\n 7. Marz N. Storm: distributed and fault-tolerant real-time computation. In: Paper presented at Strata conference on \nmaking data work, Santa Clara, California, 28 Feb–1 March 2012. 2012. https ://cdn.oreil lysta tic.com/en/asset s/1/\nevent /75/Storm _%20dis tribu ted%20and %20fau lt-toler ant%20rea ltime %20com putat ion%20Pre senta tion.pdf. \nAccessed 25 Jan 2018.\n\n 8. Ballard C, Farrell DM, Lee M, Stone PD, Thibault S, Tucker S. IBM InfoSphere Streams: harnessing data in motion. IBM \nRedbooks. 2010.\n\n 9. Joseph S, Jasmin EA, Chandran S. Stream computing: opportunities and challenges in smart grid. Procedia Tech-\nnol. 2015;21:49–53.\n\n 10. IBM Research (no date) Stream computing platforms, applications and analytics. IBM. http://resea rcher .watso \nn.ibm.com/resea rcher /view_grp.php?id=2531 Accessed 5 Mar 2019.\n\n 11. Gantz J, Reinsel D. The digital universe in 2020: big data, bigger digital shadows, and biggest growth in the Far \nEast. New York: IDC iView: IDC Analyse future; 2012.\n\n 12. Cortes R, Bonnaire X, Marin O, Sens P. Stream processing of healthcare sensor data: studying user traces to identify \nchallenges from a big data perspective. The 4th international workshop on body area sensor networks (BAS-\nNet-2015). Procedia Comput Sci. 2015;52:1004–9.\n\n 13. Chung D, Shi H. Big data analytics: a literature review. J Manag Anal. 2015;2(3):175–201.\n 14. Lu J, Li D. Bias correction in a small sample from big data. IEEE Trans Knowl Data Eng. 2013;25(11):2658–63.\n 15. Garzo A, Benczur AA, Sidlo CI, Tahara D, Ywatt EF. Real-time streaming mobility analytics. In: Proc. 2013 IEEE interna-\n\ntional conference on big data, big data, Santa Clara, CA, United States, IEEE Press. 2013. p 697–702.\n 16. Zaharia M, Das T, Li H, Hunter T, Shenker S, Stoica I. Discretized streams: fault-tolerant streaming computation \n\nat scale. In: Proc. the 24th ACM symposium on operating system principles, SOSP 2013, Farmington, PA, United \nStates. New York: ACM Press; 2013. p. 423–38.\n\n 17. Fan J, Liu H. Statistical analysis of big data on pharmacogenomics. Adv Drug Deliv Rev. 2013;65(7):987–1000.\n 18. Bifet A, Holmes G, Kirkby R, Pfahringer B. Moa: massive online analysis. J Mach Learn Res. 2010;11:1601–4.\n 19. Akter S, Fosso WS. Big data analytics in e-commerce: a systematic review and agenda for future research. Electr \n\nMarkets. 2016;26:173–94.\n 20. Sivarajah U, Kamal MM, Irani Z, Weerakkody V. Critical analysis of big data challenges and analytical methods. J Bus \n\nRes. 2016;70:263–86.\n 21. Wienhofen LW, Mathisen BM, Roman D. Empirical big data research: a systematic literature mapping. CoRR, \n\nabs/1509.03045. 2015.\n 22. Habeeb RAA, Nasaruddin F, Gani A, Hashem IAT, Ahmed E, Imran M. Real-time big data processing for anomaly \n\ndetection: a survey. Int J Inform Manage. 2018;45:289–307. https ://doi.org/10.1016/j.ijinf omgt.2018.08.006.\n 23. Mehta N, Pandit A. Concurrence of big data analytics in healthcare: a systematic review. Int J Med Inform. \n\n2018;114:57–65.\n 24. Kitchenham BA, Charters S. Guidelines for performing systematic literature review in software engineering. Techni-\n\ncal report 2(3), EBSE-2007-01, Keele University and University of Durham. 2007.\n 25. Host M, Orucevic-Alagic A. A systematic review of research on open source software in commercial software prod-\n\nuct development. 2013. http://www.bcs.org/uploa d/pdf/ewic_ea10_sessi on5pa per2.pdf. Accessed 2 Mar 2018.\n\nhttps://www.ibm.com/developerworks/library/bd-streamsintro/index.html\nhttps://www.ibm.com/developerworks/library/bd-streamsintro/index.html\nhttps://doi.org/10.1109/aina.2015.194\nhttps://cdn.oreillystatic.com/en/assets/1/event/75/Storm_%20distributed%20and%20fault-tolerant%20realtime%20computation%20Presentation.pdf\nhttps://cdn.oreillystatic.com/en/assets/1/event/75/Storm_%20distributed%20and%20fault-tolerant%20realtime%20computation%20Presentation.pdf\nhttp://researcher.watson.ibm.com/researcher/view_grp.php%3fid%3d2531\nhttp://researcher.watson.ibm.com/researcher/view_grp.php%3fid%3d2531\nhttps://doi.org/10.1016/j.ijinfomgt.2018.08.006\nhttp://www.bcs.org/upload/pdf/ewic_ea10_session5paper2.pdf\n\n\nPage 27 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\n 26. Millman N. Analytics for business. Computerworld. 2014. https ://www.compu terwo rld.com/artic le/24758 40/big-\ndata/8-consi derat ions-when-selec ting-big-data-techn ology .html. Accessed 7 Oct 2018.\n\n 27. Oussous A, Benjelloun F, Lachen AA, Belfkih S. Big data technologies: a survey. J King Saud Univ Comput Inform \nSci. 2018;30:431–48.\n\n 28. Becker H, Naaman M, Gravano L. Learning similarity metrics for event identification in social media. In: Proceed-\nings of the third ACM international conference on web search and data mining (WSDM’10), ACM New York, NY, \nUSA, 4–6 Feb 2010. 2010. p. 291–300.\n\n 29. Aggarwal CC, Zhai C. A survey of text clustering algorithms. In: Aggarwal CC, Zhai C, editors. Mining text data. New \nYork: Springer; 2012. p. 77–128.\n\n 30. Panagiotou N, Katakis I, Gunopulos D. Detecting events in online social networks: Definitions, trends and chal-\nlenges. In: Michaelis S, et al., editors. Solving large scale learning tasks: challenges and algorithms. Lecture Notes in \nComputer Science, vol. 9850. Cham: Springer; 2016. p. 42–84. https ://doi.org/10.1007/978-3-319-41706 -6_2.\n\n 31. Deepa MS, Sujatha N. Comparative study of various clustering techniques and its characteristics. Int J Adv Netw \nAppl. 2014;5(6):2104–16.\n\n 32. Reddy KSS, Bindu CS. A review of density-based clustering algorithms for big data analysis. In: International confer-\nence on I-SMAC (IoT in Social, Mobile, Analytic, and Cloud), Palladam, India 10–11 February 2017, IEEE. 2017. https \n://doi.org/10.1109/i-smac.2017.80583 22.\n\n 33. Pelkowitz L. A continuous relaxation labelling algorithm for Markov random fields. IEEE Trans Syst Man Cybern. \n1990;20:709–15.\n\n 34. Li SZ. Markov random field modelling in image analysis. New York: Springer; 2001.\n 35. Zhong S. Efficient streaming text clustering. Neural Netw. 2005;18:5–6.\n 36. Aggarwal CC, Yu PS. A framework for clustering massive text and categorical data streams. In: Proceedings of \n\nthe sixth SIAM international conference on data mining, Bethesda, MD, USA, 20–22 Apr 2016. 2006. https ://doi.\norg/10.1137/1.97816 11972 764.44.\n\n 37. Li H, Jiang X, Xiong L, Liu J. Differentially private histogram publication for dynamic datasets: an adaptive sampling \napproach. Proc ACM Int Conf Knowl Manag. 2015. p. 1001–10. https ://doi.org/10.1145/28064 16.28064 41.\n\n 38. Deng JD. Outline detection energy data streams using incremental and kernel PCA algorithms. 2016 IEEE 16th \ninternational conference on data mining workshops. 2016. p. 390–7. https ://doi.org/10.1109/icdmw .2016.158.\n\n 39. Limsopatham N, Collier N. Adapting phrase-based machine translation to normalise medical terms in social media \nmessages. In: Proceedings of the 2015 conference on empirical methods in natural language processing, EMNLP \n2015, Lisbon. 2015. p.ρ 1675–80.\n\n 40. Kaushik R, Apoorva CS, Mallya D, Chaitanya JNVK, Kamath SS. Sociopedia: an interactive system for event detec-\ntion and trend analysis for Twitter data. In: Nagar A, Mohapatra D, Chaki N (eds) Smart innovation, systems and \ntechnologies, proceedings of 3rd international conference on advanced computing, networking and informatics. \nNew Delhi: Springer; 2016.\n\n 41. Carter S, Weerkamp W, Tsagkias E. Microblog language identification: overcoming the limitations of short, \nunedited and idiomatic text. Lang Resour Eval J. 2013;47(1):195–215.\n\n 42. Pooja P, Pandey A. Impact of memory intensive applications on performance of cloud virtual machine. In: Pro-\nceedings of 2014 recent advances in engineering and computational sciences (RAECS), UIET Panjab University \nChandigarh, 6–8 March 2014. 2014. p. 1–6. https ://doi.org/10.1109/raecs .2014.67996 29.\n\n 43. Chang M, Choi IS, Niu D, Zheng H. Performance impact of emerging memory technologies on big data applica-\ntions: a latency-programmable system emulation approach. In: Proceedings of 2018 on great lake symposium \non VLSI (GLSVLSI’18), Chicago, IL, USA, ACM New York, NY, USA, 23–25 May 2018. 2018. p. 439–42. https ://doi.\norg/10.1145/31945 54.31946 33.\n\n 44. Yang W, Da Silva A, Picard ML. Computing data quality indicators on big data streams using a CEP. In: International \nworkshop on computational intelligence for multimedia understanding IWCIM, Prague, Czech Republic, 29–30 \nOctober 2015. 2015.\n\n 45. Neumeyer L, Robbins B, Nair A, Kesari A. S4: Distribute stream computing platform. In: Proceedings of the 2010 \nIEEE international conference on data mining workshops. 2010. p. 170–7. https ://doi.org/10.1109/icdmw .2010.172.\n\n 46. Inoubli W, Aridhi S, Mezni H, Maddouri M, Nguifo E. A comparative study on streaming frameworks for big data. \nIn: 44th international conference on very large databases: workshop LADaS—Latin American Data Science, Aug \n2018, Rio de Janeiro, Brazil. 2018. p. 1–8.\n\n 47. Peng D, Dabek F Large-scale incremental processing using distributed transactions and notifications. In: Proc 9th \nUSENIX conf oper sys. des implement, Vancouver, BC, Canada, 4–6 Oct 2010. 2010. p. 1–15.\n\n 48. Marz N. Trident. 2012. https ://githu b.com/natha nmarz /storm /wiki/Tride nt-tutor ial. Accessed 8 Mar 2018.\n 49. Babcock B, Babu S, Datar M, Motwani R, Widom J. Models and issues in data stream systems. In: Proc of the 21st \n\nACM SIGACT-SIGMOD-SIGART symposium on principles of database systems (PODS), Madison, Wisconsin, 3–5 \nJune 2002. 2002. p. 1–16.\n\n 50. Chandrasekaran S, Cooper O, Deshpande A, Franklin MJ, Hellerstein JM, Hong W, Krishnamurthy S, Madden SR, \nReiss F, Shah MA. TelegraphCQ: Continuous dataflow processing. In: Proceedings of the 2003 ACM SIGMOD inter-\nnational conference on management of data, San Diego, California, 9–12 Jun 2003. 2003. p. 668.\n\n 51. Abadi DJ, Ahmad Y, Balazinska M, Cherniack M, Hwang JH, Lindner W, Maskey AS, Rasin E, Ryvkina E, Tatbul N, Xing \nY, Zdonik S. The design of the borealis stream processing engine. Second biennial conference on innovative data \nsystems research (CIDR 2005). CA: Asilomar; 2005. p. 277–89.\n\n 52. Groleat T. High-performance traffic monitoring for network security and management. Human–computer interac-\ntion [cs.HC]. Télécom Bretagne; Université de Bretagne Occidentale; 2014.\n\n 53. Kamburugamuve S, Fox G, Leake D, Qiu J. Survey of distributed stream processing for large stream sources. Grids \nUCS Indiana Educ. 2013. https ://doi.org/10.13140 /rg.2.1.3856.2968.\n\n 54. Murthy S. What are the disadvantages of Redis? 2016. https ://www.quora .com/What-are-the-disad vanta ges-of-\nRedis . Accessed 8 Mar 2018.\n\nhttps://www.computerworld.com/article/2475840/big-data/8-considerations-when-selecting-big-data-technology.html\nhttps://www.computerworld.com/article/2475840/big-data/8-considerations-when-selecting-big-data-technology.html\nhttps://doi.org/10.1007/978-3-319-41706-6_2\nhttps://doi.org/10.1109/i-smac.2017.8058322\nhttps://doi.org/10.1109/i-smac.2017.8058322\nhttps://doi.org/10.1137/1.9781611972764.44\nhttps://doi.org/10.1137/1.9781611972764.44\nhttps://doi.org/10.1145/2806416.2806441\nhttps://doi.org/10.1109/icdmw.2016.158\nhttps://doi.org/10.1109/raecs.2014.6799629\nhttps://doi.org/10.1145/3194554.3194633\nhttps://doi.org/10.1145/3194554.3194633\nhttps://doi.org/10.1109/icdmw.2010.172\nhttps://github.com/nathanmarz/storm/wiki/Trident-tutorial\nhttps://doi.org/10.13140/rg.2.1.3856.2968\nhttps://www.quora.com/What-are-the-disadvantages-of-Redis\nhttps://www.quora.com/What-are-the-disadvantages-of-Redis\n\n\nPage 28 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\n 55. Su X, Gilman E, Wetz P, Riekki J, Zuo Y, Leppanen T. Stream reasoning for the internet of things: challenges and gap \nanalysis. WIMS ‘16 proceedings of the 6th international conference on web intelligence, mining and semantics, \nNîmes, France—June 13–15, New York: ACM. Article no 1. 2016. https ://doi.org/10.1145/29128 45.29128 53.\n\n 56. Morales GDF, Bifet A. SAMOA: scalable advanced massive online analysis. J Mach Learn Res. 2015;16(1):149–53.\n 57. Amazon Web Services. Lambda architecture for batch and stream processing. 2018. https ://d1.awsst atic.com/\n\nwhite paper s/lambd a-archi tecur e-on-for-batch -aws.pdf Accessed 2 May 2019.\n 58. Kreps J. Questioning the Lambda architecture. 2014. https ://www.oreil ly.com/ideas /quest ionin g-the-lambd a-archi \n\ntectu re. Accessed 2 May 2019.\n 59. Tay Y. Data generation for application-specific benchmarking. Proc VLDB Endowment. 2011;4(12):1470–3.\n 60. HiBench big data benchmark suite. https ://githu b.com/intel -hadoo p/HiBen ch. Accessed 21 Dec 2018.\n 61. Hadoop 1.2.1 Documentation. GridMix. https ://hadoo p.apach e.org/docs/r1.2.1/gridm ix.html. Accessed 8 Mar \n\n2018.\n 62. Ouaknine K, Carey M, KirkPatrick S. The PigMix benchmark on Pig, MapReduce, and HPCC systems. 2015 IEEE \n\ninternational conference on big data, New York, NY, USA, 27 June–2 July 2015. p. 643–8. https ://doi.org/10.1109/\nbigda tacon gress .2015.99.\n\n 63. Ghazal A, Rabl T, Hu M, Raab F, Poess M, Crolotte A, Jacobson H. BigBench: towards an industry standard bench-\nmark for big data analytics. In: Proceedings of the 2013 ACM SIGMOID international conference on management \nof data, New York, NY, USA, 22–27 Jun 2013. p. 1197–203.\n\n 64. Bergamaschi S, Gagliardelli L, Simonini G, Zhu S. BigBench workload executed by using apache flink. Procedia \nManuf. 2017;11:695–702. https ://doi.org/10.1016/j.promf g.2017.07.169.\n\n 65. Wang L, Zhan J, Luo C, Zhu Y, Yang Q, He Y, et al. BigDataBench: a big data benchmark suite from internet services. \nIn: 2014 IEEE 20th international symposium on high performance architecture (HPCA), Orlando, FL, USA: IEEE, \n15–19 February 2014. 2014. https ://doi.org/10.1109/hpca.2014.68359 58.\n\n 66. Gao W, Zhan J, Wang L, Luo C, Zheng D, Wen X, et al. BigDataBench: A scalable and unified big data and AI bench-\nmark suite. 2018. arXiv.org > cs > arXiv :1802.08254 v2. https ://arxiv .org/abs/1802.08254 v2.\n\n 67. Liao X, Gao Z, Ji W, Wang Y. An enforcement of real-time scheduling in Spark Streaming. 6th international green \nand sustainable computing conference, IEEE. 2016. https ://doi.org/10.1109/igcc.2015.73937 30. p. 1–6.\n\n 68. Agerri R, Artola X, Beloki Z, Rigau G, Soroa A. Big data for natural language processing: a streaming approach. \nKnowledge‐based systems. 2015;79:36–42 ISSN 0950-7051.\n\n 69. Krawczyk B, Woźniak M. Incremental weighted one-class classifier for mining stationary data streams. J Comput \nSci. 2015;9:19–25.\n\n 70. Chan SWK, Chong MWC. Sentiment analysis in financial texts. Decis Support Syst. 2017;94:53–64.\n 71. Rakthanmanon T, Campana B, Mueen A, Batista G, Westover B, Zhu Q, Zakaria J, Keogh E. Addressing big data time \n\nseries: mining trillions of time series subsequences under dynamic time warping. ACM Trans Knowl Discov Data. \n2013;7(3):31. https ://doi.org/10.1145/25004 89.\n\n 72. Hadian A, Shahrivari S. High-performance parallel k-means clustering for disk-resident datasets on multi-core \nCPUs. J Supercomput. 2014;69(2):845–63.\n\n 73. Mozafari B, Zeng K, D’Antoni L, Zaniolo C. High-performance complex event processing over hierarchical data. \nACM Trans Datab Syst. 2013;38(4):39. https ://doi.org/10.1145/25367 79.\n\n 74. Sun Y, Wang Z, Liu H, Du C, Yuan J. Online ensemble using adaptive windowing for data streams with concept \ndrift. Int J Distrib Sens Netw. Article ID 4218973, 9 pages. 2016. http://dx.doi.org/10.1155/2016/42189 73.\n\n 75. Nguyen DT, Jung JJ. Real-time event detection on social data stream. Mobile Netw Appl. 2014;20(4):475–86.\n 76. Tsagkatakis G, Beferull-Lozano B, Tsakalides P. Singular spectrum-based matrix completion for time series recovery \n\nand prediction. EURASIP J Adv Signal Proces. 2016;2016:66. https ://doi.org/10.1186/s1363 4-016-0360-0.\n 77. Papapetrou O, Garofalakis M, Deligiannakis A. Sketching distributed sliding-window data streams. VLDB J. \n\n2015;24:345–68. https ://doi.org/10.1007/s0077 8-015-0380-7.\n 78. Elkhoukhi H, NaitMalek Y, Berouine A, Bakhouya M, Elouadghiri D, Essaaidi M. Towards a real-time occupancy \n\ndetection approach for smart buildings. Procedia Comput Sci. 2018;134:114–20.\n 79. Chakrabarti C. Delivering interactive access to data at massive scale at Barclays. Austin. 2016.\n 80. Kovacevc I, Mekterovic I. Novel BI data architectures. MIPRO 2018, Opatija, Croatia. 2018. p. 1191–6.\n 81. Veiga J, Enes J, Exposito RR, Tourino J. BDEv 3.0: energy efficiency and microarchitectural characterization of big \n\ndata processing frameworks. Fut Generat Comput Syst. 2018;86:565–81.\n 82. Tozzi, C. Dummy’s guide to batch vs. streaming. Trillium Software. 2017. http://blog.syncs ort.com/2017/07/big-\n\ndata/big-data-101-batch -strea m-proce ssing /. Accessed 2 Mar 2018.\n 83. Dusi M, D’Heureuse N, Huici F, Trammell B, Niccolini S. Blockmon: flexible and high performance big data stream \n\nanalytics platform and its use cases. NEC Tech J. 2012;7:102–6.\n 84. Puthal D, Nepal S, Ranjan R, Chen J. A dynamic prime number based efficient security mechanism for big sensing \n\ndata streams. J Comput Syst Sci. 2017;83:22–42.\n 85. Vanathi R, and Khadir ASA. A robust architectural framework for big data stream computing in personal healthcare \n\nreal-time analytics. World Congress on Computing and Communication Technologies. 2017. p. 97–104. https ://doi.\norg/10.1109/wccct .2016.32.\n\n 86. Ma K, Yang B. Stream-based live entity resolution approach with adaptive duplicate count strategy. Int J Web Grid \nServ. 2017;13(3):351–73.\n\n 87. Murphy BM, O’Driscoll C, Boylan GB, Lightbody G, Marnane WP. Stream computing for biomedical signal process-\ning: A QRS complex detection case study. In: Conf proc IEEE eng med biol soc. 2015. https ://doi.org/10.1109/\nembc.2015.73197 41. p. 5928–31.\n\n 88. Apache Spark Streaming—Spark 2.1.0 Documentation. http://spark .apach e.org/strea ming.\n 89. Sun H, Birke R, Bjorkqvist M, Chen LY. AccStream: accuracy-aware overload management for stream processing \n\nsystems. In: IEEE conference on autonomic computing. New York: Elsevier; 2017. p. 39–48.\n 90. Canbay Y, Sağıroğlu S. Big data anonymization with spark (UBMK’17). In: 2nd IEEE international conference \n\non computer science and engineering. 2017. p. 833–8.\n\nhttps://doi.org/10.1145/2912845.2912853\nhttps://d1.awsstatic.com/whitepapers/lambda-architecure-on-for-batch-aws.pdf\nhttps://d1.awsstatic.com/whitepapers/lambda-architecure-on-for-batch-aws.pdf\nhttps://www.oreilly.com/ideas/questioning-the-lambda-architecture\nhttps://www.oreilly.com/ideas/questioning-the-lambda-architecture\nhttps://github.com/intel-hadoop/HiBench\nhttps://hadoop.apache.org/docs/r1.2.1/gridmix.html\nhttps://doi.org/10.1109/bigdatacongress.2015.99\nhttps://doi.org/10.1109/bigdatacongress.2015.99\nhttps://doi.org/10.1016/j.promfg.2017.07.169\nhttps://doi.org/10.1109/hpca.2014.6835958\nhttp://arxiv.org/abs/1802.08254v2\nhttps://arxiv.org/abs/1802.08254v2\nhttps://doi.org/10.1109/igcc.2015.7393730\nhttps://doi.org/10.1145/2500489\nhttps://doi.org/10.1145/2536779\nhttp://dx.doi.org/10.1155/2016/4218973\nhttps://doi.org/10.1186/s13634-016-0360-0\nhttps://doi.org/10.1007/s00778-015-0380-7\nhttp://blog.syncsort.com/2017/07/big-data/big-data-101-batch-stream-processing/\nhttp://blog.syncsort.com/2017/07/big-data/big-data-101-batch-stream-processing/\nhttps://doi.org/10.1109/wccct.2016.32\nhttps://doi.org/10.1109/wccct.2016.32\nhttps://doi.org/10.1109/embc.2015.7319741\nhttps://doi.org/10.1109/embc.2015.7319741\nhttp://spark.apache.org/streaming\n\n\nPage 29 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\n 91. Sahana RG, Babu BS. Converting an E-commerce prospect into a customer using streaming analytics. In: 2nd \ninternational conference on applied and theoretical computing and communication technology (iCATccT) IEEE. \n2016. p. 312–7. https ://doi.org/10.1109/icatc ct.2016.79120 14.\n\n 92. Troiano L, Vaccaro A, Vitelli MC. On-line smart grids optimization by case-based reasoning on big data. In: 2016 \nIEEE workshop on environmental, energy, and structural monitoring systems (EESMS), Bari, Italy, 13–14 Jun 2016.\n\n 93. Joseph S, Jasmin EA. Stream computing framework for outage detection in smart grid. In: Proceedings of 2015 \nIEEE international conference on power, instrumentation, control and computing (PICC), Thrissur, India, 9–11 Dec \n2015. 2015. https ://doi.org/10.1109/picc.2015.74557 44.\n\n 94. Apache. Apache Storm. 2016. http://storm .apach e.org. Accessed 10 Oct 2018.\n 95. Gokalp MO, Kocyigit A, Eren PE. A visual programming framework for distributed Internet of Things centric com-\n\nplex event processing. Comput Elect Eng. 2018;74:581–604.\n 96. Maio CD, Fenza G, Loia E, Orciuoli F. Distributed online temporal fuzzy concept analysis for stream processing in \n\nsmart cities. J Parallel Distrib Comput. 2017;110:31–41.\n 97. Val PB, Garcia NF, Sanchez-Fernandez L, Arias-Fisteus J. Patterns for distributed real-time stream processing. IEEE \n\nTrans Parallel Distrib Syst. 2017;2(11):3243–57. https ://doi.org/10.1109/TPDS.2017.27169 29.\n 98. Fernandez-Rodrigues JY, Alvarez-Garcia JA, Fisteus JA, Luaces MR, Magana VC. Benchmarking real-time vehicle \n\ndata streaming models for a smart city. Inform Syst. 2017;72:62–76.\n 99. Bifet A. Mining big data in real time. Informatica (Slovenia). 2013;37:15–20.\n 100. Apache. Apache Samza-What is Samza? 2016. http://samza .apach e.org. Accessed 8 Oct 2018.\n 101. Ananthanarayanan R, Basker V, Das S, Gupta A, Jiang H, Qiu T, Reznichenko A, Ryabkov D, Singh M, Venkataraman \n\nS. Photon: fault-tolerant and scalable joining of continuous data streams. In: Proceedings of 2013 ACM SIGMOD \ninternational conference on management of data, New York, New York, USA, 22–27 June 2013. 2013. p. 577–88.\n\n 102. Apache Apache Aurora. 2016. http://auror a.apach e.org. Accessed 7 Oct 2018.\n 103. Jiang Q, Adaikkalavan R, Chakravarthy S. MavEStream: synergistic integration of stream and event processing. In: \n\n2007 second international conference on digital telecommunications (ICDT’07) San Jose, CA, USA. 2007. p 29–361. \nhttps ://doi.org/10.1109/icdt.2007.21 IEEE Xplore.\n\n 104. Yang W, Da Silva A, Picard ML. Computing data quality indicators on big data streams using a CEP. In: 2015 Inter-\nnational workshop on computational intelligence for multimedia understanding (IWCIM), Prague, Czech Republic, \n29–30 Oct 2015. 2015.\n\n 105. EsperTech. http://www.esper tech.com. Accessed 8 Oct 2018.\n 106. Song M, Kim MC.  RT2M: real-time twitter trend mining system. In: Proceedings of international conference on \n\nsocial intelligence and technology (SOCIETY), State College, PA, USA, 8–10 May 2013. 2013. p. 64–71.\n 107. Barbieri DF, Braga D, Ceri S. Querying RDF streams with C-SPARQL. ACM Sigmoid. 2010;39(1):20–36. https ://doi.\n\norg/10.1145/18607 02.18607 05.\n 108. Ren X, Khrouf H, Kazi-Aoul Z, ChabChoub Y, Cure O. On measuring performances of C-SPARQL and CQELS. CoRR, \n\nabs/1611.08269. 2016.\n 109. Morales GF. SAMOA: A platform for mining big data streams. WWW 2013 Companions, Rio de Janeiro, Brazil, 13–17 \n\nMay 2013. 2013.\n 110. Keeney J, Fallon L, Tai W, O’Sullivan D. Towards composite semantic reasoning for real-time network management \n\ndata enrichment. In: Proceedings of the 11th international conference on network and service management \n(CNSM), Barcelona, Spain, 9–13 Nov 2013. 2015. p. 182–6.\n\n 111. Le-Phuoc D, Dao-Tran M, Parreira JX, Hauswirth M. A native and adaptive approach for unified processing of linked \nstreams and linked data. In: International semantic web conference, Koblenz, Germany, 23–27 October 2011. \n2011. p. 370–88.\n\n 112. Anicic D, Rudolph S, Fodor P, Stojanovic N. Stream reasoning and complex event processing in ETALIS. Sem Web \nLinked Spatiotemp Data Geo-Ontolo. 2012;3(4):397–407.\n\n 113. Apache Kylin. Kylin cube from streaming (Kafka). 2015. http://kylin .apach e.org/docs1 5/tutor ial/cube_strea ming.\nhtml. Accessed 2 Oct 2018.\n\n 114. Splunk. Splunk Stream. 2017. https ://splun kbase .splun k.com/app/1809/. Accessed 2 Oct 2018.\n 115. Shnayder V, Chen B, Lorincz K, Fulford-Jones TRF, Welsh M. Sensor networks for medical care. Technical report \n\nTR-08-05, Division of Engineering and Applied Sciences, Harvard University. 2005. https ://www.eecs.harva \nrd.edu/~shnay der/paper s/codeb lue-techr ept05 .pdf. Accessed 8 Oct 2018.\n\n 116. Dror Y. Practical elastic search anomaly detection made powerful with anodot. 2017. https ://www.anodo t.com/\nblog/pract ical-elast icsea rch-anoma lydet ectio n-made-owerf ul-with-anodo t/. Accessed 8 Mar 2019.\n\n 117. Baciu G, Li C, Wang Y, Zhang X. Cloudets: Cloud-based cognition for large streaming data. In: Ge N, Lu J, Wang Y, \nHoward N, Chen P, Tao X, Zhang B, Zadeh LA (eds) Proceedings of IEEE 14th international conference on cognitive \ninformatics and cognitive computing (ICCI*CC’15), Tsinghua, Univ., Beijing, China, 6–8 Jul 2015. 2015. p. 333–8.\n\n 118. Tedeschi A, Benedetto F. A cloud-based big data sentiment analysis application for enterprises’ brand monitor-\ning in social media streams. In: 2015 IEEE 1st international forum on research and technologies for society and \nindustry leveraging a better tomorrow (RTSI), Turing, Italy, 16–18 Sept 2015. 2015. p 186–91.\n\n 119. Lavin A, Ahmad S. Evaluating real-time anomaly detection algorithms–the Numenta anomaly benchmark. In: 2015 \nIEEE 14th international conference on machine learning and applications (ICMLA), Miami, FL, USA, 9–11 Dec 2015. \n2015. https ://doi.org/10.1109/icmla .2015.141.\n\n 120. Chen X, Chen H, Zhang N, Huang J, Zhang W. Large-scale real-time semantic processing framework for Internet of \nThings. Int J Distrib Sens Netw. 2015;365372:11. https ://doi.org/10.1155/2015/36537 2.\n\n 121. Branscombe M. How Microsoft’s fast track Azure will help businesses conquer IoT. 2015. http://www.techr adar.\ncom/news/inter net/cloud -servi ces/howmi croso ft-s-fast-track -azure -will-help-busin esses -conqu er-iot-12910 25. \nAccessed 8 Mar 2018.\n\n 122. Biem A, Bouillet E, Feng H, Ranganathan A, Riabov A, Verscheure O, Koutsopoulos H, Moran C. IBM InfoSphere \nstreams for scalable, real-time, intelligent transportation services. SIGMOID’10 Indianapolis, Indiana, USA, 6–11 Jun \n2010. 2010. p. 1093–100.\n\nhttps://doi.org/10.1109/icatcct.2016.7912014\nhttps://doi.org/10.1109/picc.2015.7455744\nhttp://storm.apache.org\nhttps://doi.org/10.1109/TPDS.2017.2716929\nhttp://samza.apache.org\nhttp://aurora.apache.org\nhttps://doi.org/10.1109/icdt.2007.21\nhttp://www.espertech.com\nhttps://doi.org/10.1145/1860702.1860705\nhttps://doi.org/10.1145/1860702.1860705\nhttp://kylin.apache.org/docs15/tutorial/cube_streaming.html\nhttp://kylin.apache.org/docs15/tutorial/cube_streaming.html\nhttps://splunkbase.splunk.com/app/1809/\nhttps://www.eecs.harvard.edu/%7eshnayder/papers/codeblue-techrept05.pdf\nhttps://www.eecs.harvard.edu/%7eshnayder/papers/codeblue-techrept05.pdf\nhttps://www.anodot.com/blog/practical-elasticsearch-anomalydetection-made-owerful-with-anodot/\nhttps://www.anodot.com/blog/practical-elasticsearch-anomalydetection-made-owerful-with-anodot/\nhttps://doi.org/10.1109/icmla.2015.141\nhttps://doi.org/10.1155/2015/365372\nhttp://www.techradar.com/news/internet/cloud-services/howmicrosoft-s-fast-track-azure-will-help-businesses-conquer-iot-1291025\nhttp://www.techradar.com/news/internet/cloud-services/howmicrosoft-s-fast-track-azure-will-help-businesses-conquer-iot-1291025\n\n\nPage 30 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\n 123. Akidau T, Balikov A, Bekiroglu K, Chernyak S, Haberman J, Lax R, McVeety S, Mills D, Nordstrom P, Whittle S. Mill-\nWheel: fault-tolerant stream processing at internet scale. Proc VLDB Endowment. 2013;6(11):1033–44.\n\n 124. Blount M, Ebling MR, Eklund JM, James AG, McGregor C, Percival N, Smith KP, Sow D. Real-time analysis for inten-\nsive care: development and deployment of the artemis analytic system. IEEE Eng Med Biol Mag. 2010;29(2):110–8. \nhttps ://doi.org/10.1109/MEMB.2010.93645 4.\n\n 125. Introducing WSO2 Data Analytics Server. 2015. https ://docs.wso2.com/displ ay/DAS30 0/Intro ducin g+DAS. \nAccessed 8 Mar 2019.\n\n 126. Ali M, Chandramouli B, Goldstein J, Schindlauer R. The extensibility framework in Microsoft StreamInsight. In: Pro-\nceedings of the 2011 IEEE 27th international conference on data engineering (ICDE), Washington, DC, USA, 11–16 \nApr 2011. 2011. p. 1242–53.\n\n 127. TIBCO StreamBase Documentation. https ://docs.tibco .com. Accessed 8 Mar 2018.\n 128. Wilkes S. Making in-memory computing enterprise-grade—overview–Striim. 2016. http://www.strii m.com/\n\nblog/2016/06/makin g-in-memor ycomp uting -enter prise -grade -overv iew/ Accessed 8 Mar 2019.\n 129. Kyvos Insights. Kyvos insights 2018. 2018. https ://www.kyvos insig hts.com/. Accessed 1 Feb 2018.\n 130. AtScale. AtScale overview (version 4.1). 2017. http://info.atsca le.com/atsca le-overv iew. Accessed 2 Feb 2018.\n 131. AtScale. AtScale. 2018. http://atsca le.com/produ ct/. Accessed 2 Feb 2018.\n 132. Gedik B, Andrade H, Wu K, Yu PS, Doo M. Spade: the S declarative stream processing engine. In: 2008 ACM SIG-\n\nMOID international conference on management of data, Vancouver, Canada, 9–12 Jun 2008. 2008. p. 1123–34.\n 133. Mimic, II. http://physi onet.org/physi obank /datab ase/mimic 2db/. Accessed 4 Nov 2016.\n 134. Wu Z, Zou M. An incremental community detection method for social tagging systems using locality sensitive \n\nhashing. Neural Netw. 2014;58:14–28. https ://doi.org/10.1016/j.neune t.2014.05.019.\n 135. O’Callaghan L, Mishra N, Meyerson A, Guha S, Motwani R. Streaming-data algorithms for high-quality clustering. \n\nIn: Proceedings of IEEE international conference on data engineering, San Jose, CA, USA, 26 Feb–1 Mar 2002. 2002. \np. 685–94.\n\n 136. Aggarwal CC, Han JW, Wang JY. A framework for clustering evolving data streams. In: Proceedings of the 29th \nVLDB conference, vol. 29, Berlin, Germany, 9–12 Sep 2003. 2003. p. 81–92.\n\n 137. Backhoff O, Ntoutsi E. Scalable online-offline stream clustering in apache spark. In: 2016 IEEE 16th international \nconference on data mining workshops (ICDMW), Barcelona, Spain, 12–15 Dec 2016. 2016. p. 37–44. https ://doi.\norg/10.1109/icdmw .2016.0014.\n\n 138. Aggarwal CC, Han J, Wang J, Yu PS. A framework for projected clustering of high dimensional data streams. In: \nProceedings of the 30th international conference on very large data bases, 30, Toronto, Canada, 31 Aug–3 Sep \n2004. 2004. p. 852–63.\n\n 139. Cao F, Ester M, Qian W, Zhou A. Density-based clustering over an evolving data stream with noise. In: 2006 SIAM \nconference on data mining. 2006. p. 328–39.\n\n 140. Chen Y, Tu L. Density-based clustering for real-time stream data. In: Proceedings of the 13th ACM SIGKDD interna-\ntional conference on knowledge discovery and data mining, San Jose, CA, USA, 12–15 Aug 2007. 2007. p. 133–42.\n\n 141. Zhu WH, Yin J, Xie YH. Arbitrary shape cluster algorithm for clustering data stream. J Softw. 2006;17(3):379–87.\n 142. Khalilian M, Mustapha N, Sulaiman N. Data stream clustering by divide and conquer approach based on vector \n\nmodel. J Big Data. 2016;3:1. https ://doi.org/10.1186/s4053 7-015-0036-x.\n 143. Dai DB, Zhao G, Sun SL. Effective clustering algorithm for probabilistic data stream. J Softw. 2009;20(5):1313–28.\n 144. Ding S, Zhang J, Jia H, Qian J. An adaptive density data stream clustering algorithm. Cogn Comput. 2016;8(1):1–9. \n\nhttps ://doi.org/10.1007/s1255 9-015-9342-z.\n 145. Choi D, Song S, Kim B, Bae I. Processing moving objects and traffic events based on spark streaming. In: Proceed-\n\nings of the 8th international conference on disaster recovery and business continuity (DRBC), Jeju, South Korea, \n25–28 Nov 2015. 2015. p. 4–7.\n\n 146. Chen XJ, Ke J. Fast processing of conversion time data flow in cloud computing via weighted FPtree mining \nalgorithms. In: 2015 IEEE 12th intl conf on ubiquitous intelligence and computing and 2015 IEEE 12th intl conf on \nautonomic and trusted computing and 2015 IEEE 15th intl conf on scalable computing and communications and \nits associated workshops (UIC-ATC-ScalCom), Beijing, China, 10–14 Aug 2015. 2015.\n\n 147. Li T, Wang L. Key technology of online auditing data stream processing. In: 2015 IEEE 12th intl conf on ubiquitous \nintelligence and computing and 2015 IEEE 12th intl conf on autonomic and trusted computing and 2015 IEEE \n15th intl conf on scalable computing and communications and its associated workshops (UIC-ATC-ScalCom), \nBeijing, China, 10–14 Aug 2015. 2015.\n\n 148. Xiao F, Aritsugi M, Wang Q, Zhang R. Efficient processing of multiple nested event pattern queries over multi-\ndimensional event streams based on a triaxial hierarchical model. Artif Intell Med. 2016;72(1):56–71. https ://doi.\norg/10.1016/j.artme d.2016.08.002.\n\n 149. Wang Z, Zhao Z, Weng S, Zhang C. Incremental multiple instance outlier detection. Neural Comput Appl. \n2015;26:957–68. https ://doi.org/10.1007/s0052 1-014-1750-6.\n\n 150. Ruiz E, Casillas J. Adaptive fuzzy partitions for evolving association rules in big data stream. Int J Approx Reasoning. \n2018;93:463–86.\n\n 151. Jadhav SA, Kosbatwar SP. Concept-adapting very fast decision tree with misclassification error. Int J Adv Res Com-\nput Eng Technol (IJARCET). 2016;5(6):1763–7.\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nhttps://doi.org/10.1109/MEMB.2010.936454\nhttps://docs.wso2.com/display/DAS300/Introducing%2bDAS\nhttps://docs.tibco.com\nhttp://www.striim.com/blog/2016/06/making-in-memorycomputing-enterprise-grade-overview/\nhttp://www.striim.com/blog/2016/06/making-in-memorycomputing-enterprise-grade-overview/\nhttps://www.kyvosinsights.com/\nhttp://info.atscale.com/atscale-overview\nhttp://atscale.com/product/\nhttp://physionet.org/physiobank/database/mimic2db/\nhttps://doi.org/10.1016/j.neunet.2014.05.019\nhttps://doi.org/10.1109/icdmw.2016.0014\nhttps://doi.org/10.1109/icdmw.2016.0014\nhttps://doi.org/10.1186/s40537-015-0036-x\nhttps://doi.org/10.1007/s12559-015-9342-z\nhttps://doi.org/10.1016/j.artmed.2016.08.002\nhttps://doi.org/10.1016/j.artmed.2016.08.002\nhttps://doi.org/10.1007/s00521-014-1750-6\n\n\tBig data stream analysis: a systematic literature review\n\tAbstract \n\tIntroduction\n\tBackground and related work\n\tStream computing\n\tBig data stream analysis\n\tKey issues in big data stream analysis\n\tScalability\n\tIntegration\n\tFault-tolerance\n\tTimeliness\n\tConsistency\n\tHeterogeneity and incompleteness\n\tLoad balancing\n\tHigh throughput\n\tPrivacy\n\tAccuracy\n\n\tRelated work\n\n\tResearch method\n\tResearch question\n\tSearch string\n\tData sources\n\tData retrieval\n\tInclusion criteria\n\tExclusion criteria\n\n\n\tResult\n\tResearch Question 1: What are the tools and technologies employed for big data stream analysis?\n\tShape of the data\n\tData access\n\tAvailability and consistency requirement\n\tWorkload profile required\n\tLatency requirement\n\n\tResearch Question 2: What methods and techniques are used in analysing big data streams?\n\tResearch Question 3: What do big data streaming tools and technologies have in common and their differences in terms of concept, purpose, and capabilities?\n\tResearch Question 4: What are the limitations and strengths of big data streaming tools and technologies?\n\tResearch Question 5: What are the evaluation techniques or benchmarks that are used for evaluating big data streaming tools and technologies?\n\n\tDiscussion\n\tLimitation of the review\n\tConclusion and further work\n\tAcknowledgements\n\tReferences\n\n\n\n\n",
      "metadata_author": "Taiwo Kolajo ",
      "metadata_title": "Big data stream analysis: a systematic literature review",
      "metadata_creation_date": "2019-06-04T14:40:29Z"
    }
  ]
}

query 2 - search=devops&$select=description,rating_average,level,role,title,source&$filter=level eq 'intermediate'
result
{
  "@odata.context": "https://test-cogno.search.windows.net/indexes('azureblob-index')/$metadata#docs(*)",
  "value": [
    {
      "@search.score": 4.44421,
      "source": "MS Learn",
      "title": "Implement CI/CD with Azure DevOps",
      "description": "Implement CI/CD with Azure DevOps",
      "level": "intermediate",
      "role": "data-engineer",
      "rating_average": 4.74
    },
    {
      "@search.score": 3.5846813,
      "source": "Company Moodle",
      "title": "DevOps for Dev",
      "description": "For developers, this course will teach you how to hook your dev work into our existing CI/CD pipelines.",
      "level": "intermediate",
      "role": "developer",
      "rating_average": 3.8
    },
    {
      "@search.score": 2.4684517,
      "source": "Company Moodle",
      "title": "DevOps for Ops",
      "description": "For administrators, this course will teach you how our CI/CD pipelines work from an operations perspective",
      "level": "intermediate",
      "role": "admin",
      "rating_average": 4.9
    },
    {
      "@search.score": 2.0757747,
      "source": "MS Learn",
      "title": "Deploy a cloud-native ASP.NET Core microservice with GitHub Actions",
      "description": "Implement GitHub Actions to build a container image and deploy to Azure Kubernetes Service.",
      "level": "intermediate",
      "role": "devops-engineer",
      "rating_average": 4.79
    },
    {
      "@search.score": 2.0152462,
      "source": "MS Learn",
      "title": "Deploy a cloud-native ASP.NET Core microservice with GitHub Actions",
      "description": "Implement GitHub Actions to build a container image and deploy to Azure Kubernetes Service.",
      "level": "intermediate",
      "role": "devops-engineer",
      "rating_average": 4.79
    },
    {
      "@search.score": 1.9439561,
      "source": "MS Learn",
      "title": "Deploy ASP.NET web apps with Azure Pipelines",
      "description": "Azure Pipelines help automate building, deploying, and maintaining your applications. While they support a wide range of platforms and programming languages, in this module you’ll focus on using them to implement ASP.NET apps on Azure App Service Web Apps with Azure SQL Database as their data store.",
      "level": "intermediate",
      "role": "devops-engineer",
      "rating_average": 4.85
    },
    {
      "@search.score": 1.7994078,
      "source": "MS Learn",
      "title": "Deploy a cloud-native ASP.NET Core microservice with GitHub Actions",
      "description": "Implement GitHub Actions to build a container image and deploy to Azure Kubernetes Service.",
      "level": "intermediate",
      "role": "devops-engineer",
      "rating_average": 4.79
    },
    {
      "@search.score": 1.7994078,
      "source": "MS Learn",
      "title": "Manage release cadence in Azure Pipelines by using deployment patterns",
      "description": "Choose and implement a deployment pattern that helps you smoothly roll out new application features to your users.",
      "level": "intermediate",
      "role": "devops-engineer",
      "rating_average": 4.72
    },
    {
      "@search.score": 1.744621,
      "source": "MS Learn",
      "title": "Provision databases in Azure Pipelines",
      "description": "Provision databases in Azure Pipelines",
      "level": "intermediate",
      "role": "devops-engineer",
      "rating_average": 0
    },
    {
      "@search.score": 1.744621,
      "source": "MS Learn",
      "title": "Deploy a cloud-native ASP.NET Core microservice with GitHub Actions",
      "description": "Implement GitHub Actions to build a container image and deploy to Azure Kubernetes Service.",
      "level": "intermediate",
      "role": "devops-engineer",
      "rating_average": 4.79
    },
    {
      "@search.score": 1.7401998,
      "source": "MS Learn",
      "title": "Manage database changes in Azure Pipelines",
      "description": "Use a release approval in Azure Pipelines to help coordinate database schema changes between developers and database administrators.",
      "level": "intermediate",
      "role": "devops-engineer",
      "rating_average": 4.65
    },
    {
      "@search.score": 1.7401998,
      "source": "MS Learn",
      "title": "Manage database changes in Azure Pipelines",
      "description": "Use a release approval in Azure Pipelines to help coordinate database schema changes between developers and database administrators.",
      "level": "intermediate",
      "role": "devops-engineer",
      "rating_average": 4.65
    },
    {
      "@search.score": 1.7401998,
      "source": "MS Learn",
      "title": "Manage database changes in Azure Pipelines",
      "description": "Use a release approval in Azure Pipelines to help coordinate database schema changes between developers and database administrators.",
      "level": "intermediate",
      "role": "devops-engineer",
      "rating_average": 4.65
    },
    {
      "@search.score": 1.5762777,
      "source": "MS Learn",
      "title": "Provision databases in Azure Pipelines",
      "description": "Provision databases in Azure Pipelines",
      "level": "intermediate",
      "role": "devops-engineer",
      "rating_average": 0
    },
    {
      "@search.score": 1.5762777,
      "source": "MS Learn",
      "title": "Deploy a cloud-native ASP.NET Core microservice with GitHub Actions",
      "description": "Implement GitHub Actions to build a container image and deploy to Azure Kubernetes Service.",
      "level": "intermediate",
      "role": "devops-engineer",
      "rating_average": 4.79
    },
    {
      "@search.score": 1.5762777,
      "source": "MS Learn",
      "title": "Manage release cadence in Azure Pipelines by using deployment patterns",
      "description": "Choose and implement a deployment pattern that helps you smoothly roll out new application features to your users.",
      "level": "intermediate",
      "role": "devops-engineer",
      "rating_average": 4.72
    },
    {
      "@search.score": 1.5095558,
      "source": "MS Learn",
      "title": "Introduction to GitHub's Products",
      "description": "Overview GitHub's products, associated features, and licensing of per-use and metered features.",
      "level": "intermediate",
      "role": "devops-engineer",
      "rating_average": 4.77
    },
    {
      "@search.score": 1.5095558,
      "source": "MS Learn",
      "title": "Leverage GitHub Actions to publish to GitHub Packages",
      "description": "Publish automatically and securely your code libraries or Docker images with GitHub Packages.",
      "level": "intermediate",
      "role": "devops-engineer",
      "rating_average": 4.71
    },
    {
      "@search.score": 1.5095558,
      "source": "MS Learn",
      "title": "Host your own build agent in Azure Pipelines",
      "description": "Work with guidance from the Space Game web team to set up your build agent running on-premises or on an Azure virtual machine running in the cloud.",
      "level": "intermediate",
      "role": "devops-engineer",
      "rating_average": 4.72
    },
    {
      "@search.score": 1.5095558,
      "source": "MS Learn",
      "title": "Deploy a cloud-native ASP.NET Core microservice with GitHub Actions",
      "description": "Implement GitHub Actions to build a container image and deploy to Azure Kubernetes Service.",
      "level": "intermediate",
      "role": "devops-engineer",
      "rating_average": 4.79
    },
    {
      "@search.score": 1.5095558,
      "source": "MS Learn",
      "title": "Introduction to GitHub administration",
      "description": "Understand the security and control measures available to GitHub administrators within an organization or enterprise.",
      "level": "intermediate",
      "role": "devops-engineer",
      "rating_average": 4.69
    },
    {
      "@search.score": 1.5095558,
      "source": "MS Learn",
      "title": "Scan open source components for vulnerabilities and license ratings in Azure Pipelines",
      "description": "Scan open-source components for security vulnerabilities and assess their license ratings when your application builds in Azure Pipelines.",
      "level": "intermediate",
      "role": "devops-engineer",
      "rating_average": 4.72
    },
    {
      "@search.score": 1.5095558,
      "source": "MS Learn",
      "title": "Provision infrastructure in Azure Pipelines",
      "description": "Learn how infrastructure as code enables you to describe and automatically provision the infrastructure that you need for your application.",
      "level": "intermediate",
      "role": "devops-engineer",
      "rating_average": 4.69
    },
    {
      "@search.score": 1.4867325,
      "source": "MS Learn",
      "title": "Deploy a cloud-native ASP.NET Core microservice with GitHub Actions",
      "description": "Implement GitHub Actions to build a container image and deploy to Azure Kubernetes Service.",
      "level": "intermediate",
      "role": "devops-engineer",
      "rating_average": 4.79
    },
    {
      "@search.score": 1.4867325,
      "source": "MS Learn",
      "title": "Build and deploy applications to Azure by using GitHub Actions",
      "description": "Create two deployment workflows using GitHub Actions and Microsoft Azure.",
      "level": "intermediate",
      "role": "devops-engineer",
      "rating_average": 4.74
    },
    {
      "@search.score": 1.4867325,
      "source": "MS Learn",
      "title": "Provision infrastructure in Azure Pipelines",
      "description": "Learn how infrastructure as code enables you to describe and automatically provision the infrastructure that you need for your application.",
      "level": "intermediate",
      "role": "devops-engineer",
      "rating_average": 4.69
    },
    {
      "@search.score": 1.4424613,
      "source": "MS Learn",
      "title": "Host your own build agent in Azure Pipelines",
      "description": "Work with guidance from the Space Game web team to set up your build agent running on-premises or on an Azure virtual machine running in the cloud.",
      "level": "intermediate",
      "role": "devops-engineer",
      "rating_average": 4.72
    },
    {
      "@search.score": 1.4424613,
      "source": "MS Learn",
      "title": "Build and deploy applications to Azure by using GitHub Actions",
      "description": "Create two deployment workflows using GitHub Actions and Microsoft Azure.",
      "level": "intermediate",
      "role": "devops-engineer",
      "rating_average": 4.74
    }
  ]
}